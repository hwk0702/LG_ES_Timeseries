{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1477ead4",
   "metadata": {},
   "source": [
    "# [ LG에너지 솔루션_DX_Intensive_Course ] 시계열 데이터 분석을 위한 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ff0db",
   "metadata": {},
   "source": [
    "## 트랜스포머 기반의 시계열 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee81dd",
   "metadata": {},
   "source": [
    "## 강의 복습\n",
    "강의자료 : 시계열 분석을 위한 딥러닝, 02 트랜스포머 기반의 시계열 데이터 분석\n",
    "\n",
    "- Transformer는 Sequence를 입력으로 받아 sequenc를 출력하는 구조이므로, 시계열 과업에도 적용 가능\n",
    "- Representation learning : 시계열의 의미 있는 정보를 더 쉽게 추출할 수 있도록 고차원의 raw data를 저 차원 공간에 mapping하는 것을 목표, 비지도 학습 방식으로 해당 데이터의 representation을 학습하고, 이를 downstreamtask에서 활용하는 방식\n",
    "- 시계열 회귀 : 시계열로 구성된 예측변수 x를 통해 다른 시계열 종속변수 y를 예측하는 과업 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d933bbc",
   "metadata": {},
   "source": [
    "<img src=\"./image/TST05.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4c126",
   "metadata": {},
   "source": [
    "## 실습 요약\n",
    "\n",
    "1. 본 실습에서는 Transformer를 활용한 representation learning 모델인 TST를 활용하여 시계열 회귀를 수행합니다.\n",
    "2. Input에 대해 일부 masking을 한 뒤 masking 된 부분을 예측하는 방식으로 pre-training을 진행합니다.\n",
    "3. 회귀 과업을 수행하도록 fine-tuning을 진행합니다.\n",
    "4. 해당 모델에 대한 전체적인 구조는 강의자료 47 page에서 확인하실 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739ca6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T07:25:51.767588Z",
     "start_time": "2023-06-21T07:25:51.759328Z"
    }
   },
   "source": [
    "[Zerveas, George, et al. \"A transformer-based framework for multivariate time series representation learning.\" Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2021.](https://arxiv.org/pdf/2010.02803.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42150185",
   "metadata": {},
   "source": [
    "<img src=\"./image/TST01.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff07a84",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff69a0",
   "metadata": {},
   "source": [
    "### SETP 0. 환경 구축하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4300b8",
   "metadata": {},
   "source": [
    "- TST 모델에 맞는 환경을 구축하기 위하여 필요한 패키지를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22faf9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:30:55.213258Z",
     "start_time": "2023-06-22T14:30:55.205066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/dsba/external_lecture/LG_ES_Timeseries/mvts_transformer\n"
     ]
    }
   ],
   "source": [
    "# github에서 데이터 불러오기\n",
    "# !git clone https://github.com/hwk0702/LG_ES_Timeseries.git\n",
    "#%cd LG_ES_Timeseries/mvts_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e32035d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T15:49:08.359449Z",
     "start_time": "2023-06-21T15:49:04.905454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipdb in /usr/local/lib/python3.6/dist-packages (from -r ./mvts_transformer/requirements.txt (line 1)) (0.13.13)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r ./mvts_transformer/requirements.txt (line 2)) (4.62.3)\n",
      "Requirement already satisfied: xlrd in /usr/local/lib/python3.6/dist-packages (from -r ./mvts_transformer/requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r ./mvts_transformer/requirements.txt (line 4)) (1.1.5)\n",
      "Requirement already satisfied: xlutils in /usr/local/lib/python3.6/dist-packages (from -r ./mvts_transformer/requirements.txt (line 5)) (2.0.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from -r ./mvts_transformer/requirements.txt (line 6)) (0.8.9)\n",
      "Requirement already satisfied: xlwt in /usr/local/lib/python3.6/dist-packages (from -r ./mvts_transformer/requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r ./mvts_transformer/requirements.txt (line 8)) (1.7.1)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r ./mvts_transformer/requirements.txt (line 9)) (2.6.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r ./mvts_transformer/requirements.txt (line 10)) (3.3.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r ./mvts_transformer/requirements.txt (line 11)) (1.19.3)\n",
      "Requirement already satisfied: scikit_learn in /usr/local/lib/python3.6/dist-packages (from -r ./mvts_transformer/requirements.txt (line 12)) (0.24.2)\n",
      "Requirement already satisfied: sktime==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r ./mvts_transformer/requirements.txt (line 13)) (0.4.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from sktime==0.4.1->-r ./mvts_transformer/requirements.txt (line 13)) (0.37.1)\n",
      "Requirement already satisfied: statsmodels>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from sktime==0.4.1->-r ./mvts_transformer/requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipdb->-r ./mvts_transformer/requirements.txt (line 1)) (4.4.2)\n",
      "Requirement already satisfied: tomli in /usr/local/lib/python3.6/dist-packages (from ipdb->-r ./mvts_transformer/requirements.txt (line 1)) (1.2.3)\n",
      "Requirement already satisfied: ipython<7.17.0,>=7.16.3 in /usr/local/lib/python3.6/dist-packages (from ipdb->-r ./mvts_transformer/requirements.txt (line 1)) (7.16.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r ./mvts_transformer/requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r ./mvts_transformer/requirements.txt (line 4)) (2021.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->-r ./mvts_transformer/requirements.txt (line 8)) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->-r ./mvts_transformer/requirements.txt (line 8)) (3.7.4.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (0.15.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (2.27.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (59.6.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (1.43.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (3.3.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (3.19.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r ./mvts_transformer/requirements.txt (line 10)) (8.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r ./mvts_transformer/requirements.txt (line 10)) (3.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r ./mvts_transformer/requirements.txt (line 10)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r ./mvts_transformer/requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit_learn->-r ./mvts_transformer/requirements.txt (line 12)) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit_learn->-r ./mvts_transformer/requirements.txt (line 12)) (3.0.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit_learn->-r ./mvts_transformer/requirements.txt (line 12)) (1.5.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.4->tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (1.15.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: jedi<=0.17.2,>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython<7.17.0,>=7.16.3->ipdb->-r ./mvts_transformer/requirements.txt (line 1)) (0.17.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ipython<7.17.0,>=7.16.3->ipdb->-r ./mvts_transformer/requirements.txt (line 1)) (3.0.24)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython<7.17.0,>=7.16.3->ipdb->-r ./mvts_transformer/requirements.txt (line 1)) (2.11.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython<7.17.0,>=7.16.3->ipdb->-r ./mvts_transformer/requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython<7.17.0,>=7.16.3->ipdb->-r ./mvts_transformer/requirements.txt (line 1)) (4.3.3)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython<7.17.0,>=7.16.3->ipdb->-r ./mvts_transformer/requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.6/dist-packages (from ipython<7.17.0,>=7.16.3->ipdb->-r ./mvts_transformer/requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (4.8.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (2021.10.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.11.0->sktime==0.4.1->-r ./mvts_transformer/requirements.txt (line 13)) (0.5.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (3.6.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from jedi<=0.17.2,>=0.10->ipython<7.17.0,>=7.16.3->ipdb->-r ./mvts_transformer/requirements.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<7.17.0,>=7.16.3->ipdb->-r ./mvts_transformer/requirements.txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./mvts_transformer/requirements.txt (line 9)) (3.1.1)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython<7.17.0,>=7.16.3->ipdb->-r ./mvts_transformer/requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect->ipython<7.17.0,>=7.16.3->ipdb->-r ./mvts_transformer/requirements.txt (line 1)) (0.7.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !pip install torch==1.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815c293",
   "metadata": {},
   "source": [
    "- 필요한 library들을 import 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e83505a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:31:09.188302Z",
     "start_time": "2023-06-22T14:31:07.811158Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:31:07,826 | INFO : Loading packages ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:[3.6.9 (default, Dec  8 2021, 21:08:43) \n",
      "[GCC 8.4.0]].\n",
      "PyTorch version:[1.7.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Loading packages ...\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "from easydict import EasyDict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.options import Options\n",
    "from src.running import setup, pipeline_factory, validate, check_progress, NEG_METRICS\n",
    "from src.utils import utils\n",
    "from src.datasets import utils as dutils\n",
    "from src.datasets.data import data_factory, Normalizer\n",
    "from src.datasets.datasplit import split_dataset\n",
    "from src.models.ts_transformer import model_factory\n",
    "from src.models.loss import get_loss_module\n",
    "from src.optimizers import get_optimizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#check torch version & device\n",
    "print (\"Python version:[%s].\"%(sys.version))\n",
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device)) # device에 cuda:0가 프린트 된다면 GPU를 사용하는 상태입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6704d987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:31:15.592732Z",
     "start_time": "2023-06-22T14:31:15.580653Z"
    }
   },
   "outputs": [],
   "source": [
    "# set random seed \n",
    "\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "random_seed = 42\n",
    "set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15eff85",
   "metadata": {},
   "source": [
    "모델에 필요한 argument들을 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26905fe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:31:16.897694Z",
     "start_time": "2023-06-22T14:31:16.881099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_filepath': None,\n",
       " 'output_dir': './output',\n",
       " 'data_dir': './data',\n",
       " 'load_model': None,\n",
       " 'resume': False,\n",
       " 'change_output': False,\n",
       " 'save_all': False,\n",
       " 'experiment_name': '',\n",
       " 'comment': '',\n",
       " 'no_timestamp': False,\n",
       " 'records_file': './records.xls',\n",
       " 'console': False,\n",
       " 'print_interval': 1,\n",
       " 'gpu': '0',\n",
       " 'n_proc': -1,\n",
       " 'num_workers': 0,\n",
       " 'seed': None,\n",
       " 'limit_size': None,\n",
       " 'test_only': None,\n",
       " 'data_class': 'weld',\n",
       " 'labels': None,\n",
       " 'test_from': None,\n",
       " 'test_ratio': 0,\n",
       " 'val_ratio': 0.2,\n",
       " 'pattern': None,\n",
       " 'val_pattern': None,\n",
       " 'test_pattern': None,\n",
       " 'normalization': 'standardization',\n",
       " 'norm_from': None,\n",
       " 'subsample_factor': None,\n",
       " 'task': 'imputation',\n",
       " 'masking_ratio': 0.15,\n",
       " 'mean_mask_length': 3,\n",
       " 'mask_mode': 'separate',\n",
       " 'mask_distribution': 'geometric',\n",
       " 'exclude_feats': None,\n",
       " 'mask_feats': '0, 1',\n",
       " 'start_hint': 0.0,\n",
       " 'end_hint': 0.0,\n",
       " 'harden': False,\n",
       " 'epochs': 400,\n",
       " 'val_interval': 2,\n",
       " 'optimizer': 'Adam',\n",
       " 'lr': 0.001,\n",
       " 'lr_step': '1000000',\n",
       " 'lr_factor': '0.1',\n",
       " 'batch_size': 64,\n",
       " 'l2_reg': 0,\n",
       " 'global_reg': False,\n",
       " 'key_metric': 'loss',\n",
       " 'freeze': False,\n",
       " 'model': 'transformer',\n",
       " 'max_seq_len': None,\n",
       " 'data_window_len': None,\n",
       " 'd_model': 64,\n",
       " 'dim_feedforward': 256,\n",
       " 'num_heads': 8,\n",
       " 'num_layers': 3,\n",
       " 'dropout': 0.1,\n",
       " 'pos_encoding': 'fixed',\n",
       " 'activation': 'gelu',\n",
       " 'normalization_layer': 'BatchNorm'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Offical code에서 default로 세팅한 argument들을 불러옵니다.\n",
    "args = Options()\n",
    "args = args.parser.parse_args([])\n",
    "args.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1488e76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:31:18.296687Z",
     "start_time": "2023-06-22T14:31:18.288517Z"
    }
   },
   "outputs": [],
   "source": [
    "# 실습을 위해 수정해야 할 argumetn들을 설정해줍니다.\n",
    "args_change = EasyDict({\n",
    "    # for dataloader \n",
    "    'output_dir': './output',    # 학습 및 테스트 과정에서 생성되는 output들을 저장하는 폴더 위치\n",
    "    'data_dir': './data/BeijingPM25Quality',    # 실습 데이터가 저장되어 있는 폴더 위치\n",
    "    'name': 'pretrained',    # 실행 이름 지정\n",
    "    'records_file': 'Imputation_records.xls',    # 최종 metric 결과를 저장하기 위한 파일 지정 (실습에서는 사용하지 않습니다.)\n",
    "    \n",
    "    # Dataset\n",
    "    'limit_size': None,\n",
    "    'data_class': 'tsra',    # 데이터 종류 지정 (tsra -> TSRegressionArchive)\n",
    "    'pattern': 'TRAIN',    # TRAIN or TEST\n",
    "    'val_ratio': 0.2,    # Train, validation split 비율\n",
    "    'epochs': 400,\n",
    "    'lr': 0.001,\n",
    "    'optimizer': 'RAdam',\n",
    "    'batch_size': 32,\n",
    "    'pos_encoding': 'learnable',\n",
    "    'd_model': 128\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48defe00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:31:20.742615Z",
     "start_time": "2023-06-22T14:31:20.731688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_filepath': None,\n",
       " 'output_dir': './output',\n",
       " 'data_dir': './data/BeijingPM25Quality',\n",
       " 'load_model': None,\n",
       " 'resume': False,\n",
       " 'change_output': False,\n",
       " 'save_all': False,\n",
       " 'experiment_name': '',\n",
       " 'comment': '',\n",
       " 'no_timestamp': False,\n",
       " 'records_file': 'Imputation_records.xls',\n",
       " 'console': False,\n",
       " 'print_interval': 1,\n",
       " 'gpu': '0',\n",
       " 'n_proc': -1,\n",
       " 'num_workers': 0,\n",
       " 'seed': None,\n",
       " 'limit_size': None,\n",
       " 'test_only': None,\n",
       " 'data_class': 'tsra',\n",
       " 'labels': None,\n",
       " 'test_from': None,\n",
       " 'test_ratio': 0,\n",
       " 'val_ratio': 0.2,\n",
       " 'pattern': 'TRAIN',\n",
       " 'val_pattern': None,\n",
       " 'test_pattern': None,\n",
       " 'normalization': 'standardization',\n",
       " 'norm_from': None,\n",
       " 'subsample_factor': None,\n",
       " 'task': 'imputation',\n",
       " 'masking_ratio': 0.15,\n",
       " 'mean_mask_length': 3,\n",
       " 'mask_mode': 'separate',\n",
       " 'mask_distribution': 'geometric',\n",
       " 'exclude_feats': None,\n",
       " 'mask_feats': '0, 1',\n",
       " 'start_hint': 0.0,\n",
       " 'end_hint': 0.0,\n",
       " 'harden': False,\n",
       " 'epochs': 400,\n",
       " 'val_interval': 2,\n",
       " 'optimizer': 'RAdam',\n",
       " 'lr': 0.001,\n",
       " 'lr_step': '1000000',\n",
       " 'lr_factor': '0.1',\n",
       " 'batch_size': 32,\n",
       " 'l2_reg': 0,\n",
       " 'global_reg': False,\n",
       " 'key_metric': 'loss',\n",
       " 'freeze': False,\n",
       " 'model': 'transformer',\n",
       " 'max_seq_len': None,\n",
       " 'data_window_len': None,\n",
       " 'd_model': 128,\n",
       " 'dim_feedforward': 256,\n",
       " 'num_heads': 8,\n",
       " 'num_layers': 3,\n",
       " 'dropout': 0.1,\n",
       " 'pos_encoding': 'learnable',\n",
       " 'activation': 'gelu',\n",
       " 'normalization_layer': 'BatchNorm',\n",
       " 'name': 'pretrained'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 수정할 argument들을 업데이트 해줍니다.\n",
    "args.__dict__.update(args_change)\n",
    "args.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2aa8105",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:31:22.546613Z",
     "start_time": "2023-06-22T14:31:22.539490Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:31:22,543 | INFO : Stored configuration file in './output/_2023-06-22_14-31-22_Nbr'\n"
     ]
    }
   ],
   "source": [
    "# 세팅한 argument들로 configuration diretory를 생성합니다. (+ config 저장 및 불러오기)\n",
    "config = setup(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d4d82",
   "metadata": {},
   "source": [
    "### SETP 1. 데이터 준비하기 (이 부분만 데이터를 포맷에 맞게 코드 변경 해주시면 됩니다!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00def37",
   "metadata": {},
   "source": [
    "금일 실습에서는 BeijingPM25 데이터를 활용하여 시계열 회귀를 진행합니다.\n",
    "* 해당 데이터는 중국 베이징에서 수집한 대기질 데이터를 포함\n",
    "* 베이징의 미세먼지(PM2.5) 농도에 대한 시간별 데이터를 주로 포함하며, 추가적으로 다양한 기상 정보(온도, 압력, 풍향, 풍속 등)와 누적된 시간별 미세먼지 농도를 포함\n",
    "* 데이터셋 출처\n",
    "    * https://zenodo.org/record/3902651#.YB5P0OpOm3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e64787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:31:25.084697Z",
     "start_time": "2023-06-22T14:31:25.076685Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = './data/BeijingPM25Quality'\n",
    "data_paths = glob.glob(os.path.join(data_dir, '*'))\n",
    "train_path, test_path = [p for p in data_paths if os.path.isfile(p) and p.endswith('.ts')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed6c540",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:32:07.961227Z",
     "start_time": "2023-06-22T14:31:26.922033Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11942it [00:39, 299.06it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df, train_labels = dutils.load_from_tsfile_to_dataframe(train_path,\n",
    "                                                              return_separate_X_and_y=True,\n",
    "                                                              replace_missing_vals_with='NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283c39c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:33:18.004963Z",
     "start_time": "2023-06-22T14:33:17.872909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-03-01 00:00:00     4.0\n",
       "2013-03-01 01:00:0...</td>\n",
       "      <td>2013-03-01 00:00:00     7.0\n",
       "2013-03-01 01:00:0...</td>\n",
       "      <td>2013-03-01 00:00:00    300.0\n",
       "2013-03-01 01:00:...</td>\n",
       "      <td>2013-03-01 00:00:00    77.0\n",
       "2013-03-01 01:00:0...</td>\n",
       "      <td>2013-03-01 00:00:00   -0.7\n",
       "2013-03-01 01:00:00...</td>\n",
       "      <td>2013-03-01 00:00:00    1023.0\n",
       "2013-03-01 01:00...</td>\n",
       "      <td>2013-03-01 00:00:00   -18.8\n",
       "2013-03-01 01:00:0...</td>\n",
       "      <td>2013-03-01 00:00:00    0.0\n",
       "2013-03-01 01:00:00...</td>\n",
       "      <td>2013-03-01 00:00:00    4.4\n",
       "2013-03-01 01:00:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-03-02 00:00:00    24.0\n",
       "2013-03-02 01:00:0...</td>\n",
       "      <td>2013-03-02 00:00:00     44.0\n",
       "2013-03-02 01:00:...</td>\n",
       "      <td>2013-03-02 00:00:00     500.0\n",
       "2013-03-02 01:00...</td>\n",
       "      <td>2013-03-02 00:00:00    44.0\n",
       "2013-03-02 01:00:0...</td>\n",
       "      <td>2013-03-02 00:00:00   -0.4\n",
       "2013-03-02 01:00:00...</td>\n",
       "      <td>2013-03-02 00:00:00    1031.0\n",
       "2013-03-02 01:00...</td>\n",
       "      <td>2013-03-02 00:00:00   -17.6\n",
       "2013-03-02 01:00:0...</td>\n",
       "      <td>2013-03-02 00:00:00    0.0\n",
       "2013-03-02 01:00:00...</td>\n",
       "      <td>2013-03-02 00:00:00    1.4\n",
       "2013-03-02 01:00:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-03-03 00:00:00     73.0\n",
       "2013-03-03 01:00:...</td>\n",
       "      <td>2013-03-03 00:00:00    100.0\n",
       "2013-03-03 01:00:...</td>\n",
       "      <td>2013-03-03 00:00:00    1899.0\n",
       "2013-03-03 01:00...</td>\n",
       "      <td>2013-03-03 00:00:00     2.0\n",
       "2013-03-03 01:00:0...</td>\n",
       "      <td>2013-03-03 00:00:00    -1.4\n",
       "2013-03-03 01:00:0...</td>\n",
       "      <td>2013-03-03 00:00:00    1020.4\n",
       "2013-03-03 01:00...</td>\n",
       "      <td>2013-03-03 00:00:00   -13.0\n",
       "2013-03-03 01:00:0...</td>\n",
       "      <td>2013-03-03 00:00:00    0.0\n",
       "2013-03-03 01:00:00...</td>\n",
       "      <td>2013-03-03 00:00:00    1.2\n",
       "2013-03-03 01:00:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-03-04 00:00:00    51.0\n",
       "2013-03-04 01:00:0...</td>\n",
       "      <td>2013-03-04 00:00:00    86.0\n",
       "2013-03-04 01:00:0...</td>\n",
       "      <td>2013-03-04 00:00:00    1300.0\n",
       "2013-03-04 01:00...</td>\n",
       "      <td>2013-03-04 00:00:00     4.0\n",
       "2013-03-04 01:00:0...</td>\n",
       "      <td>2013-03-04 00:00:00     7.7\n",
       "2013-03-04 01:00:0...</td>\n",
       "      <td>2013-03-04 00:00:00    1015.7\n",
       "2013-03-04 01:00...</td>\n",
       "      <td>2013-03-04 00:00:00   -11.1\n",
       "2013-03-04 01:00:0...</td>\n",
       "      <td>2013-03-04 00:00:00    0.0\n",
       "2013-03-04 01:00:00...</td>\n",
       "      <td>2013-03-04 00:00:00    2.6\n",
       "2013-03-04 01:00:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-03-05 00:00:00     66.0\n",
       "2013-03-05 01:00:...</td>\n",
       "      <td>2013-03-05 00:00:00     78.0\n",
       "2013-03-05 01:00:...</td>\n",
       "      <td>2013-03-05 00:00:00    1100.0\n",
       "2013-03-05 01:00...</td>\n",
       "      <td>2013-03-05 00:00:00    84.0\n",
       "2013-03-05 01:00:0...</td>\n",
       "      <td>2013-03-05 00:00:00     4.7\n",
       "2013-03-05 01:00:0...</td>\n",
       "      <td>2013-03-05 00:00:00    1015.2\n",
       "2013-03-05 01:00...</td>\n",
       "      <td>2013-03-05 00:00:00   -9.1\n",
       "2013-03-05 01:00:00...</td>\n",
       "      <td>2013-03-05 00:00:00    0.0\n",
       "2013-03-05 01:00:00...</td>\n",
       "      <td>2013-03-05 00:00:00    1.6\n",
       "2013-03-05 01:00:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11913</th>\n",
       "      <td>2015-12-27 00:00:00    16.0\n",
       "2015-12-27 01:00:0...</td>\n",
       "      <td>2015-12-27 00:00:00    39.0\n",
       "2015-12-27 01:00:0...</td>\n",
       "      <td>2015-12-27 00:00:00     800.0\n",
       "2015-12-27 01:00...</td>\n",
       "      <td>2015-12-27 00:00:00    25.0\n",
       "2015-12-27 01:00:0...</td>\n",
       "      <td>2015-12-27 00:00:00   -4.1\n",
       "2015-12-27 01:00:00...</td>\n",
       "      <td>2015-12-27 00:00:00    1033.5\n",
       "2015-12-27 01:00...</td>\n",
       "      <td>2015-12-27 00:00:00   -10.7\n",
       "2015-12-27 01:00:0...</td>\n",
       "      <td>2015-12-27 00:00:00    0.0\n",
       "2015-12-27 01:00:00...</td>\n",
       "      <td>2015-12-27 00:00:00    2.4\n",
       "2015-12-27 01:00:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11914</th>\n",
       "      <td>2015-12-28 00:00:00    26.0\n",
       "2015-12-28 01:00:0...</td>\n",
       "      <td>2015-12-28 00:00:00     66.0\n",
       "2015-12-28 01:00:...</td>\n",
       "      <td>2015-12-28 00:00:00    2600.0\n",
       "2015-12-28 01:00...</td>\n",
       "      <td>2015-12-28 00:00:00    11.0\n",
       "2015-12-28 01:00:0...</td>\n",
       "      <td>2015-12-28 00:00:00   -7.6\n",
       "2015-12-28 01:00:00...</td>\n",
       "      <td>2015-12-28 00:00:00    1033.2\n",
       "2015-12-28 01:00...</td>\n",
       "      <td>2015-12-28 00:00:00   -11.4\n",
       "2015-12-28 01:00:0...</td>\n",
       "      <td>2015-12-28 00:00:00    0.0\n",
       "2015-12-28 01:00:00...</td>\n",
       "      <td>2015-12-28 00:00:00    0.9\n",
       "2015-12-28 01:00:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11915</th>\n",
       "      <td>2015-12-29 00:00:00    33.0\n",
       "2015-12-29 01:00:0...</td>\n",
       "      <td>2015-12-29 00:00:00    105.0\n",
       "2015-12-29 01:00:...</td>\n",
       "      <td>2015-12-29 00:00:00    3500.0\n",
       "2015-12-29 01:00...</td>\n",
       "      <td>2015-12-29 00:00:00     9.0\n",
       "2015-12-29 01:00:0...</td>\n",
       "      <td>2015-12-29 00:00:00   -3.2\n",
       "2015-12-29 01:00:00...</td>\n",
       "      <td>2015-12-29 00:00:00    1028.2\n",
       "2015-12-29 01:00...</td>\n",
       "      <td>2015-12-29 00:00:00   -6.5\n",
       "2015-12-29 01:00:00...</td>\n",
       "      <td>2015-12-29 00:00:00    0.0\n",
       "2015-12-29 01:00:00...</td>\n",
       "      <td>2015-12-29 00:00:00    1.0\n",
       "2015-12-29 01:00:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11916</th>\n",
       "      <td>2015-12-30 00:00:00    38.0\n",
       "2015-12-30 01:00:0...</td>\n",
       "      <td>2015-12-30 00:00:00    148.0\n",
       "2015-12-30 01:00:...</td>\n",
       "      <td>2015-12-30 00:00:00    6500.0\n",
       "2015-12-30 01:00...</td>\n",
       "      <td>2015-12-30 00:00:00    10.0\n",
       "2015-12-30 01:00:0...</td>\n",
       "      <td>2015-12-30 00:00:00   -2.9\n",
       "2015-12-30 01:00:00...</td>\n",
       "      <td>2015-12-30 00:00:00    1023.9\n",
       "2015-12-30 01:00...</td>\n",
       "      <td>2015-12-30 00:00:00    -4.3\n",
       "2015-12-30 01:00:0...</td>\n",
       "      <td>2015-12-30 00:00:00    0.0\n",
       "2015-12-30 01:00:00...</td>\n",
       "      <td>2015-12-30 00:00:00    1.4\n",
       "2015-12-30 01:00:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>2015-12-31 00:00:00     7.0\n",
       "2015-12-31 01:00:0...</td>\n",
       "      <td>2015-12-31 00:00:00     59.0\n",
       "2015-12-31 01:00:...</td>\n",
       "      <td>2015-12-31 00:00:00     800.0\n",
       "2015-12-31 01:00...</td>\n",
       "      <td>2015-12-31 00:00:00    14.0\n",
       "2015-12-31 01:00:0...</td>\n",
       "      <td>2015-12-31 00:00:00   -1.4\n",
       "2015-12-31 01:00:00...</td>\n",
       "      <td>2015-12-31 00:00:00    1030.2\n",
       "2015-12-31 01:00...</td>\n",
       "      <td>2015-12-31 00:00:00   -11.3\n",
       "2015-12-31 01:00:0...</td>\n",
       "      <td>2015-12-31 00:00:00    0.0\n",
       "2015-12-31 01:00:00...</td>\n",
       "      <td>2015-12-31 00:00:00    0.9\n",
       "2015-12-31 01:00:00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11918 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   dim_0  \\\n",
       "0      2013-03-01 00:00:00     4.0\n",
       "2013-03-01 01:00:0...   \n",
       "1      2013-03-02 00:00:00    24.0\n",
       "2013-03-02 01:00:0...   \n",
       "2      2013-03-03 00:00:00     73.0\n",
       "2013-03-03 01:00:...   \n",
       "3      2013-03-04 00:00:00    51.0\n",
       "2013-03-04 01:00:0...   \n",
       "4      2013-03-05 00:00:00     66.0\n",
       "2013-03-05 01:00:...   \n",
       "...                                                  ...   \n",
       "11913  2015-12-27 00:00:00    16.0\n",
       "2015-12-27 01:00:0...   \n",
       "11914  2015-12-28 00:00:00    26.0\n",
       "2015-12-28 01:00:0...   \n",
       "11915  2015-12-29 00:00:00    33.0\n",
       "2015-12-29 01:00:0...   \n",
       "11916  2015-12-30 00:00:00    38.0\n",
       "2015-12-30 01:00:0...   \n",
       "11917  2015-12-31 00:00:00     7.0\n",
       "2015-12-31 01:00:0...   \n",
       "\n",
       "                                                   dim_1  \\\n",
       "0      2013-03-01 00:00:00     7.0\n",
       "2013-03-01 01:00:0...   \n",
       "1      2013-03-02 00:00:00     44.0\n",
       "2013-03-02 01:00:...   \n",
       "2      2013-03-03 00:00:00    100.0\n",
       "2013-03-03 01:00:...   \n",
       "3      2013-03-04 00:00:00    86.0\n",
       "2013-03-04 01:00:0...   \n",
       "4      2013-03-05 00:00:00     78.0\n",
       "2013-03-05 01:00:...   \n",
       "...                                                  ...   \n",
       "11913  2015-12-27 00:00:00    39.0\n",
       "2015-12-27 01:00:0...   \n",
       "11914  2015-12-28 00:00:00     66.0\n",
       "2015-12-28 01:00:...   \n",
       "11915  2015-12-29 00:00:00    105.0\n",
       "2015-12-29 01:00:...   \n",
       "11916  2015-12-30 00:00:00    148.0\n",
       "2015-12-30 01:00:...   \n",
       "11917  2015-12-31 00:00:00     59.0\n",
       "2015-12-31 01:00:...   \n",
       "\n",
       "                                                   dim_2  \\\n",
       "0      2013-03-01 00:00:00    300.0\n",
       "2013-03-01 01:00:...   \n",
       "1      2013-03-02 00:00:00     500.0\n",
       "2013-03-02 01:00...   \n",
       "2      2013-03-03 00:00:00    1899.0\n",
       "2013-03-03 01:00...   \n",
       "3      2013-03-04 00:00:00    1300.0\n",
       "2013-03-04 01:00...   \n",
       "4      2013-03-05 00:00:00    1100.0\n",
       "2013-03-05 01:00...   \n",
       "...                                                  ...   \n",
       "11913  2015-12-27 00:00:00     800.0\n",
       "2015-12-27 01:00...   \n",
       "11914  2015-12-28 00:00:00    2600.0\n",
       "2015-12-28 01:00...   \n",
       "11915  2015-12-29 00:00:00    3500.0\n",
       "2015-12-29 01:00...   \n",
       "11916  2015-12-30 00:00:00    6500.0\n",
       "2015-12-30 01:00...   \n",
       "11917  2015-12-31 00:00:00     800.0\n",
       "2015-12-31 01:00...   \n",
       "\n",
       "                                                   dim_3  \\\n",
       "0      2013-03-01 00:00:00    77.0\n",
       "2013-03-01 01:00:0...   \n",
       "1      2013-03-02 00:00:00    44.0\n",
       "2013-03-02 01:00:0...   \n",
       "2      2013-03-03 00:00:00     2.0\n",
       "2013-03-03 01:00:0...   \n",
       "3      2013-03-04 00:00:00     4.0\n",
       "2013-03-04 01:00:0...   \n",
       "4      2013-03-05 00:00:00    84.0\n",
       "2013-03-05 01:00:0...   \n",
       "...                                                  ...   \n",
       "11913  2015-12-27 00:00:00    25.0\n",
       "2015-12-27 01:00:0...   \n",
       "11914  2015-12-28 00:00:00    11.0\n",
       "2015-12-28 01:00:0...   \n",
       "11915  2015-12-29 00:00:00     9.0\n",
       "2015-12-29 01:00:0...   \n",
       "11916  2015-12-30 00:00:00    10.0\n",
       "2015-12-30 01:00:0...   \n",
       "11917  2015-12-31 00:00:00    14.0\n",
       "2015-12-31 01:00:0...   \n",
       "\n",
       "                                                   dim_4  \\\n",
       "0      2013-03-01 00:00:00   -0.7\n",
       "2013-03-01 01:00:00...   \n",
       "1      2013-03-02 00:00:00   -0.4\n",
       "2013-03-02 01:00:00...   \n",
       "2      2013-03-03 00:00:00    -1.4\n",
       "2013-03-03 01:00:0...   \n",
       "3      2013-03-04 00:00:00     7.7\n",
       "2013-03-04 01:00:0...   \n",
       "4      2013-03-05 00:00:00     4.7\n",
       "2013-03-05 01:00:0...   \n",
       "...                                                  ...   \n",
       "11913  2015-12-27 00:00:00   -4.1\n",
       "2015-12-27 01:00:00...   \n",
       "11914  2015-12-28 00:00:00   -7.6\n",
       "2015-12-28 01:00:00...   \n",
       "11915  2015-12-29 00:00:00   -3.2\n",
       "2015-12-29 01:00:00...   \n",
       "11916  2015-12-30 00:00:00   -2.9\n",
       "2015-12-30 01:00:00...   \n",
       "11917  2015-12-31 00:00:00   -1.4\n",
       "2015-12-31 01:00:00...   \n",
       "\n",
       "                                                   dim_5  \\\n",
       "0      2013-03-01 00:00:00    1023.0\n",
       "2013-03-01 01:00...   \n",
       "1      2013-03-02 00:00:00    1031.0\n",
       "2013-03-02 01:00...   \n",
       "2      2013-03-03 00:00:00    1020.4\n",
       "2013-03-03 01:00...   \n",
       "3      2013-03-04 00:00:00    1015.7\n",
       "2013-03-04 01:00...   \n",
       "4      2013-03-05 00:00:00    1015.2\n",
       "2013-03-05 01:00...   \n",
       "...                                                  ...   \n",
       "11913  2015-12-27 00:00:00    1033.5\n",
       "2015-12-27 01:00...   \n",
       "11914  2015-12-28 00:00:00    1033.2\n",
       "2015-12-28 01:00...   \n",
       "11915  2015-12-29 00:00:00    1028.2\n",
       "2015-12-29 01:00...   \n",
       "11916  2015-12-30 00:00:00    1023.9\n",
       "2015-12-30 01:00...   \n",
       "11917  2015-12-31 00:00:00    1030.2\n",
       "2015-12-31 01:00...   \n",
       "\n",
       "                                                   dim_6  \\\n",
       "0      2013-03-01 00:00:00   -18.8\n",
       "2013-03-01 01:00:0...   \n",
       "1      2013-03-02 00:00:00   -17.6\n",
       "2013-03-02 01:00:0...   \n",
       "2      2013-03-03 00:00:00   -13.0\n",
       "2013-03-03 01:00:0...   \n",
       "3      2013-03-04 00:00:00   -11.1\n",
       "2013-03-04 01:00:0...   \n",
       "4      2013-03-05 00:00:00   -9.1\n",
       "2013-03-05 01:00:00...   \n",
       "...                                                  ...   \n",
       "11913  2015-12-27 00:00:00   -10.7\n",
       "2015-12-27 01:00:0...   \n",
       "11914  2015-12-28 00:00:00   -11.4\n",
       "2015-12-28 01:00:0...   \n",
       "11915  2015-12-29 00:00:00   -6.5\n",
       "2015-12-29 01:00:00...   \n",
       "11916  2015-12-30 00:00:00    -4.3\n",
       "2015-12-30 01:00:0...   \n",
       "11917  2015-12-31 00:00:00   -11.3\n",
       "2015-12-31 01:00:0...   \n",
       "\n",
       "                                                   dim_7  \\\n",
       "0      2013-03-01 00:00:00    0.0\n",
       "2013-03-01 01:00:00...   \n",
       "1      2013-03-02 00:00:00    0.0\n",
       "2013-03-02 01:00:00...   \n",
       "2      2013-03-03 00:00:00    0.0\n",
       "2013-03-03 01:00:00...   \n",
       "3      2013-03-04 00:00:00    0.0\n",
       "2013-03-04 01:00:00...   \n",
       "4      2013-03-05 00:00:00    0.0\n",
       "2013-03-05 01:00:00...   \n",
       "...                                                  ...   \n",
       "11913  2015-12-27 00:00:00    0.0\n",
       "2015-12-27 01:00:00...   \n",
       "11914  2015-12-28 00:00:00    0.0\n",
       "2015-12-28 01:00:00...   \n",
       "11915  2015-12-29 00:00:00    0.0\n",
       "2015-12-29 01:00:00...   \n",
       "11916  2015-12-30 00:00:00    0.0\n",
       "2015-12-30 01:00:00...   \n",
       "11917  2015-12-31 00:00:00    0.0\n",
       "2015-12-31 01:00:00...   \n",
       "\n",
       "                                                   dim_8  \n",
       "0      2013-03-01 00:00:00    4.4\n",
       "2013-03-01 01:00:00...  \n",
       "1      2013-03-02 00:00:00    1.4\n",
       "2013-03-02 01:00:00...  \n",
       "2      2013-03-03 00:00:00    1.2\n",
       "2013-03-03 01:00:00...  \n",
       "3      2013-03-04 00:00:00    2.6\n",
       "2013-03-04 01:00:00...  \n",
       "4      2013-03-05 00:00:00    1.6\n",
       "2013-03-05 01:00:00...  \n",
       "...                                                  ...  \n",
       "11913  2015-12-27 00:00:00    2.4\n",
       "2015-12-27 01:00:00...  \n",
       "11914  2015-12-28 00:00:00    0.9\n",
       "2015-12-28 01:00:00...  \n",
       "11915  2015-12-29 00:00:00    1.0\n",
       "2015-12-29 01:00:00...  \n",
       "11916  2015-12-30 00:00:00    1.4\n",
       "2015-12-30 01:00:00...  \n",
       "11917  2015-12-31 00:00:00    0.9\n",
       "2015-12-31 01:00:00...  \n",
       "\n",
       "[11918 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1be923a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:33:20.694746Z",
     "start_time": "2023-06-22T14:33:20.676156Z"
    }
   },
   "outputs": [],
   "source": [
    "# 결측치를 보간법을 통해 채워주는 함수\n",
    "def interpolate_missing(y):\n",
    "    \"\"\"\n",
    "    Replaces NaN values in pd.Series `y` using linear interpolation\n",
    "    \"\"\"\n",
    "    if y.isna().any():\n",
    "        y = y.interpolate(method='linear', limit_direction='both')\n",
    "    return y\n",
    "\n",
    "class load_dataframe:\n",
    "    def __init__(self, root_dir):\n",
    "    \n",
    "        self.all_df, self.labels_df = self.load_all(root_dir)\n",
    "        self.all_IDs = self.all_df.index.unique()\n",
    "        \n",
    "        # use all features\n",
    "        self.feature_names = self.all_df.columns\n",
    "        self.feature_df = self.all_df\n",
    "    \n",
    "    \n",
    "    def load_all(self, root_dir):\n",
    "        \n",
    "        df, labels = dutils.load_from_tsfile_to_dataframe(root_dir,\n",
    "                                                          return_separate_X_and_y=True,\n",
    "                                                          replace_missing_vals_with='NaN')\n",
    "        \n",
    "        labels_df = pd.DataFrame(labels, dtype=np.float32)\n",
    "        \n",
    "        lengths = df.applymap(lambda x: len(x)).values\n",
    "        self.max_seq_len = lengths[0, 0]\n",
    "        \n",
    "        df = pd.concat((pd.DataFrame({col: df.loc[row, col] for col in df.columns}).reset_index(drop=True).set_index(\n",
    "            pd.Series(lengths[row, 0]*[row])) for row in range(df.shape[0])), axis=0)\n",
    "        grp = df.groupby(by=df.index)\n",
    "        df = grp.transform(interpolate_missing)\n",
    "        \n",
    "        return df, labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b985159",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:34:47.170866Z",
     "start_time": "2023-06-22T14:33:21.757265Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11942it [00:39, 301.08it/s]\n"
     ]
    }
   ],
   "source": [
    "my_data = load_dataframe(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "445614f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:04.801645Z",
     "start_time": "2023-06-22T14:35:04.769089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>-18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.2</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.5</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>1024.5</td>\n",
       "      <td>-19.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1025.2</td>\n",
       "      <td>-19.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>27.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>1026.3</td>\n",
       "      <td>-8.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>34.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>1026.2</td>\n",
       "      <td>-8.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>31.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>1025.8</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>40.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>1025.5</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>43.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>1025.2</td>\n",
       "      <td>-7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286032 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dim_0  dim_1   dim_2  dim_3  dim_4   dim_5  dim_6  dim_7  dim_8\n",
       "0        4.0    7.0   300.0   77.0   -0.7  1023.0  -18.8    0.0    4.4\n",
       "0        4.0    7.0   300.0   77.0   -1.1  1023.2  -18.2    0.0    4.7\n",
       "0        5.0   10.0   300.0   73.0   -1.1  1023.5  -18.2    0.0    5.6\n",
       "0       11.0   11.0   300.0   72.0   -1.4  1024.5  -19.4    0.0    3.1\n",
       "0       12.0   12.0   300.0   72.0   -2.0  1025.2  -19.5    0.0    2.0\n",
       "...      ...    ...     ...    ...    ...     ...    ...    ...    ...\n",
       "11917   27.0   96.0  3300.0    9.0   -1.4  1026.3   -8.6    0.0    1.0\n",
       "11917   34.0   99.0  3700.0    9.0   -2.5  1026.2   -8.4    0.0    1.3\n",
       "11917   31.0   95.0  3100.0    9.0   -2.7  1025.8   -8.0    0.0    0.9\n",
       "11917   40.0   99.0  4200.0   13.0   -3.5  1025.5   -7.6    0.0    0.4\n",
       "11917   43.0  104.0  4800.0   16.0   -3.4  1025.2   -7.5    0.0    1.8\n",
       "\n",
       "[286032 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18634518",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:05.970955Z",
     "start_time": "2023-06-22T14:35:05.953319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11913</th>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11914</th>\n",
       "      <td>281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11915</th>\n",
       "      <td>543.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11916</th>\n",
       "      <td>505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>227.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11918 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0       24.0\n",
       "1       93.0\n",
       "2      117.0\n",
       "3       58.0\n",
       "4      226.0\n",
       "...      ...\n",
       "11913   89.0\n",
       "11914  281.0\n",
       "11915  543.0\n",
       "11916  505.0\n",
       "11917  227.0\n",
       "\n",
       "[11918 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b414f32",
   "metadata": {},
   "source": [
    "### STEP 2. 데이터 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f862a",
   "metadata": {},
   "source": [
    "train 데이터셋의 일부를 validation 데이터셋으로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf38e50c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:13.742894Z",
     "start_time": "2023-06-22T14:35:13.733527Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_method = 'ShuffleSplit'    # 데이터를 나누는 방법을 명시합니다. ShuffleSplit or StratifiedShuffleSplit(for classification)\n",
    "labels = None    # classification task에서 데이터 class를 고려하여 나눌 때 사용합니다.\n",
    "\n",
    "test_data = my_data\n",
    "test_indices = None\n",
    "val_data = my_data\n",
    "val_indices = []\n",
    "\n",
    "train_indices, val_indices, test_indices = split_dataset(data_indices=my_data.all_IDs,\n",
    "                                                            validation_method=validation_method,\n",
    "                                                            n_splits=1,\n",
    "                                                            validation_ratio=config['val_ratio'],\n",
    "                                                            random_seed=1337)\n",
    "train_indices = train_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "val_indices = val_indices[0]  # `split_dataset` returns a list of indices *per fold/split*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b6cc7f",
   "metadata": {},
   "source": [
    "### STEP 3. 데이터 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f30e7f03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:15.629973Z",
     "start_time": "2023-06-22T14:35:15.340766Z"
    }
   },
   "outputs": [],
   "source": [
    "normalizer = None\n",
    "\n",
    "normalizer = Normalizer(config['normalization'])\n",
    "my_data.feature_df.loc[train_indices] = normalizer.normalize(my_data.feature_df.loc[train_indices])\n",
    "val_data.feature_df.loc[val_indices] = normalizer.normalize(val_data.feature_df.loc[val_indices])\n",
    "test_data.feature_df.loc[test_indices] = normalizer.normalize(test_data.feature_df.loc[test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc04e8e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:17.936113Z",
     "start_time": "2023-06-22T14:35:17.825571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.372892</td>\n",
       "      <td>0.305150</td>\n",
       "      <td>-0.952829</td>\n",
       "      <td>-0.852954</td>\n",
       "      <td>0.993705</td>\n",
       "      <td>-0.054040</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.704167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>0.305150</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.871063</td>\n",
       "      <td>0.993705</td>\n",
       "      <td>-0.069076</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.543053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.147589</td>\n",
       "      <td>0.218324</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.871063</td>\n",
       "      <td>1.003529</td>\n",
       "      <td>-0.069076</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.381940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.006774</td>\n",
       "      <td>0.131499</td>\n",
       "      <td>-0.952829</td>\n",
       "      <td>-0.880117</td>\n",
       "      <td>0.983881</td>\n",
       "      <td>-0.076595</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.704167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>-0.049551</td>\n",
       "      <td>0.218324</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.880117</td>\n",
       "      <td>0.983881</td>\n",
       "      <td>-0.061558</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.543053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>0.308048</td>\n",
       "      <td>0.175752</td>\n",
       "      <td>-0.389454</td>\n",
       "      <td>-0.658387</td>\n",
       "      <td>-1.024983</td>\n",
       "      <td>1.612613</td>\n",
       "      <td>-1.527624</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>1.068081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>0.434365</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>-0.302629</td>\n",
       "      <td>-0.744988</td>\n",
       "      <td>-1.097416</td>\n",
       "      <td>1.691205</td>\n",
       "      <td>-1.753172</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.220827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>-0.070905</td>\n",
       "      <td>-0.190366</td>\n",
       "      <td>-0.563106</td>\n",
       "      <td>-0.450546</td>\n",
       "      <td>-1.187958</td>\n",
       "      <td>1.799268</td>\n",
       "      <td>-1.723099</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.381940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>-0.239329</td>\n",
       "      <td>-0.471994</td>\n",
       "      <td>-0.563106</td>\n",
       "      <td>-0.277344</td>\n",
       "      <td>-1.242282</td>\n",
       "      <td>1.789444</td>\n",
       "      <td>-1.858428</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>0.343070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>-0.534070</td>\n",
       "      <td>-0.894437</td>\n",
       "      <td>-0.736757</td>\n",
       "      <td>-0.086823</td>\n",
       "      <td>-1.405257</td>\n",
       "      <td>1.779620</td>\n",
       "      <td>-1.753172</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>0.262514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228816 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         dim_0     dim_1     dim_2     dim_3     dim_4     dim_5     dim_6  \\\n",
       "1967 -0.660388  0.372892  0.305150 -0.952829 -0.852954  0.993705 -0.054040   \n",
       "1967 -0.660388  0.316566  0.305150 -0.970150 -0.871063  0.993705 -0.069076   \n",
       "1967 -0.660388  0.147589  0.218324 -0.970150 -0.871063  1.003529 -0.069076   \n",
       "1967 -0.660388  0.006774  0.131499 -0.952829 -0.880117  0.983881 -0.076595   \n",
       "1967 -0.660388 -0.049551  0.218324 -0.970150 -0.880117  0.983881 -0.061558   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3223  0.308048  0.175752 -0.389454 -0.658387 -1.024983  1.612613 -1.527624   \n",
       "3223  0.434365  0.316566 -0.302629 -0.744988 -1.097416  1.691205 -1.753172   \n",
       "3223 -0.070905 -0.190366 -0.563106 -0.450546 -1.187958  1.799268 -1.723099   \n",
       "3223 -0.239329 -0.471994 -0.563106 -0.277344 -1.242282  1.789444 -1.858428   \n",
       "3223 -0.534070 -0.894437 -0.736757 -0.086823 -1.405257  1.779620 -1.753172   \n",
       "\n",
       "         dim_7     dim_8  \n",
       "1967 -0.078872 -0.704167  \n",
       "1967 -0.078872 -0.543053  \n",
       "1967 -0.078872 -0.381940  \n",
       "1967 -0.078872 -0.704167  \n",
       "1967 -0.078872 -0.543053  \n",
       "...        ...       ...  \n",
       "3223 -0.078872  1.068081  \n",
       "3223 -0.078872 -0.220827  \n",
       "3223 -0.078872 -0.381940  \n",
       "3223 -0.078872  0.343070  \n",
       "3223 -0.078872  0.262514  \n",
       "\n",
       "[228816 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.feature_df.loc[train_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500e058",
   "metadata": {},
   "source": [
    "### STEP 4. 데이터 마스킹 및 데이터로더 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8a80c",
   "metadata": {},
   "source": [
    "<img src=\"./image/TST02.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14e6e5b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:19.906169Z",
     "start_time": "2023-06-22T14:35:19.883008Z"
    }
   },
   "outputs": [],
   "source": [
    "def noise_mask(X, masking_ratio, lm=3, mode='separate', distribution='geometric', exclude_feats=None):\n",
    "    \"\"\"\n",
    "    feature를 마스킹해야 하는 위치에 0을 사용하여 X와 동일한 모양의 random boolean mask를 만듭니다.\n",
    "    Args:\n",
    "        X: (seq_length, feat_dim) 단일 샘플에 해당하는 numpy array of features\n",
    "        masking_ratio: 마스킹할 seq_length의 비율. 각 timestamp에서 평균적으로 마스킹할 feat_dim의 비율.\n",
    "        lm: 마스킹 시퀀스의 평균 길이. Used only when `distribution` is 'geometric'.\n",
    "        mode: 각 변수를 개별적으로 마스킹할지('별도'), 특정 위치의 모든 변수를 동시에 마스킹할지('동시') 결정\n",
    "        distribution: 각 마스크 시퀀스 요소가 무작위로 독립적으로 샘플링되는지 또는 샘플링이 마코프 체인을 따르는지, \n",
    "        원하는 평균 길이 `lm`의 마스크된 시퀀스의 기하학적 분포를 가져오는지 여부\n",
    "        exclude_feats: 마스킹에서 제외할 feature에 해당하는 인덱스 (i.e. to remain all 1s)\n",
    "\n",
    "    Returns:\n",
    "        feature를 마스킹해야 하는 위치에 0이 있는 X와 같은 모양의 boolean numpy array\n",
    "    \"\"\"\n",
    "    if exclude_feats is not None:\n",
    "        exclude_feats = set(exclude_feats)\n",
    "\n",
    "    if distribution == 'geometric':  # stateful (Markov chain)\n",
    "        if mode == 'separate':  # each variable (feature) is independent\n",
    "            mask = np.ones(X.shape, dtype=bool)\n",
    "            for m in range(X.shape[1]):  # feature dimension\n",
    "                if exclude_feats is None or m not in exclude_feats:\n",
    "                    mask[:, m] = geom_noise_mask_single(X.shape[0], lm, masking_ratio)  # time dimension\n",
    "        else:  # replicate across feature dimension (mask all variables at the same positions concurrently)\n",
    "            mask = np.tile(np.expand_dims(geom_noise_mask_single(X.shape[0], lm, masking_ratio), 1), X.shape[1])\n",
    "    else:  # each position is independent Bernoulli with p = 1 - masking_ratio\n",
    "        if mode == 'separate':\n",
    "            mask = np.random.choice(np.array([True, False]), size=X.shape, replace=True,\n",
    "                                    p=(1 - masking_ratio, masking_ratio))\n",
    "        else:\n",
    "            mask = np.tile(np.random.choice(np.array([True, False]), size=(X.shape[0], 1), replace=True,\n",
    "                                            p=(1 - masking_ratio, masking_ratio)), X.shape[1])\n",
    "\n",
    "    return mask\n",
    "\n",
    "def geom_noise_mask_single(L, lm, masking_ratio):\n",
    "    \"\"\"\n",
    "    masking_ratio에 맞게 평균 길이 lm의 시퀀스로 구성된 길이 'L'의 boolean mask를 무작위로 생성\n",
    "    마스킹 시퀀스의 길이와 간격은 geometric distribution를 따릅니다.\n",
    "    Args:\n",
    "        L: 마스크의 길이 및 마스크할 시퀀스\n",
    "        lm: 마스킹 시퀀스의 평균 길이\n",
    "        masking_ratio: 마스킹할 L의 비율\n",
    "\n",
    "    Returns:\n",
    "        (L,) boolean numpy array intended to mask ('drop') with 0s a sequence of length L\n",
    "    \"\"\"\n",
    "    keep_mask = np.ones(L, dtype=bool)\n",
    "    p_m = 1 / lm  # probability of each masking sequence stopping. parameter of geometric distribution.\n",
    "    p_u = p_m * masking_ratio / (1 - masking_ratio)  # probability of each unmasked sequence stopping. parameter of geometric distribution.\n",
    "    p = [p_m, p_u]\n",
    "\n",
    "    # Start in state 0 with masking_ratio probability\n",
    "    state = int(np.random.rand() > masking_ratio)  # state 0 means masking, 1 means not masking\n",
    "    for i in range(L):\n",
    "        keep_mask[i] = state  # here it happens that state and masking value corresponding to state are identical\n",
    "        if np.random.rand() < p[state]:\n",
    "            state = 1 - state\n",
    "\n",
    "    return keep_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bae0e7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:20.985021Z",
     "start_time": "2023-06-22T14:35:20.970911Z"
    }
   },
   "outputs": [],
   "source": [
    "mask = noise_mask(my_data.feature_df.loc[train_indices[0]], \n",
    "                  masking_ratio=0.15, \n",
    "                  lm=3, \n",
    "                  mode='separate', \n",
    "                  distribution='geometric', \n",
    "                  exclude_feats=None)  # (seq_length, feat_dim) boolean array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e75c728e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:21.615101Z",
     "start_time": "2023-06-22T14:35:21.607005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True, False,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True, False,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True, False,  True,  True,  True],\n",
       "       [ True, False,  True,  True,  True, False,  True,  True,  True],\n",
       "       [ True, False,  True,  True,  True, False,  True,  True, False],\n",
       "       [False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [False,  True, False,  True,  True,  True,  True,  True, False],\n",
       "       [False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True, False,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True, False,  True,  True,  True,  True, False,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True, False,  True,  True],\n",
       "       [ True,  True,  True,  True,  True, False, False,  True,  True],\n",
       "       [ True,  True,  True,  True,  True, False,  True,  True,  True],\n",
       "       [ True,  True, False,  True,  True, False,  True,  True,  True],\n",
       "       [ True,  True, False, False,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaef66e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:22.801530Z",
     "start_time": "2023-06-22T14:35:22.774502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.372892</td>\n",
       "      <td>0.305150</td>\n",
       "      <td>-0.952829</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.993705</td>\n",
       "      <td>-0.054040</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.704167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>0.305150</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.993705</td>\n",
       "      <td>-0.069076</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.543053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.147589</td>\n",
       "      <td>0.218324</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.871063</td>\n",
       "      <td>1.003529</td>\n",
       "      <td>-0.069076</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.381940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.006774</td>\n",
       "      <td>0.131499</td>\n",
       "      <td>-0.952829</td>\n",
       "      <td>-0.880117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.076595</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.704167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.218324</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.880117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.061558</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.543053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305150</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.880117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.054040</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.175752</td>\n",
       "      <td>0.305150</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.880117</td>\n",
       "      <td>1.003529</td>\n",
       "      <td>-0.016448</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.865280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.305150</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.898225</td>\n",
       "      <td>1.033001</td>\n",
       "      <td>-0.016448</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.784724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.260240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.880117</td>\n",
       "      <td>1.101768</td>\n",
       "      <td>0.006107</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.119426</td>\n",
       "      <td>0.218324</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.871063</td>\n",
       "      <td>1.131240</td>\n",
       "      <td>0.006107</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.623610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.119426</td>\n",
       "      <td>0.218324</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.852954</td>\n",
       "      <td>1.101768</td>\n",
       "      <td>0.021143</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.462497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.147589</td>\n",
       "      <td>0.218324</td>\n",
       "      <td>-0.935509</td>\n",
       "      <td>-0.852954</td>\n",
       "      <td>1.082120</td>\n",
       "      <td>0.021143</td>\n",
       "      <td>0.042134</td>\n",
       "      <td>-0.220827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.147589</td>\n",
       "      <td>0.305150</td>\n",
       "      <td>-0.935509</td>\n",
       "      <td>-0.816738</td>\n",
       "      <td>0.993705</td>\n",
       "      <td>0.021143</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.381940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.232077</td>\n",
       "      <td>0.305150</td>\n",
       "      <td>-0.935509</td>\n",
       "      <td>-0.780521</td>\n",
       "      <td>0.944585</td>\n",
       "      <td>0.028661</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.462497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.260240</td>\n",
       "      <td>0.218324</td>\n",
       "      <td>-0.935509</td>\n",
       "      <td>-0.789575</td>\n",
       "      <td>0.915113</td>\n",
       "      <td>0.043698</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.462497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.260240</td>\n",
       "      <td>0.305150</td>\n",
       "      <td>-0.952829</td>\n",
       "      <td>-0.816738</td>\n",
       "      <td>0.895465</td>\n",
       "      <td>0.051216</td>\n",
       "      <td>0.042134</td>\n",
       "      <td>-0.381940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.288403</td>\n",
       "      <td>0.305150</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.807684</td>\n",
       "      <td>0.905289</td>\n",
       "      <td>0.028661</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.784724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305150</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.816738</td>\n",
       "      <td>0.915113</td>\n",
       "      <td>0.021143</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.059713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305150</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.807684</td>\n",
       "      <td>0.954409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.704167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.260240</td>\n",
       "      <td>0.478801</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.807684</td>\n",
       "      <td>1.003529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042134</td>\n",
       "      <td>-0.543053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.260240</td>\n",
       "      <td>0.478801</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.789575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042134</td>\n",
       "      <td>-0.301383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.618282</td>\n",
       "      <td>0.232077</td>\n",
       "      <td>0.391975</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.798629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073771</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.623610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.175752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.970150</td>\n",
       "      <td>-0.798629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066253</td>\n",
       "      <td>0.163141</td>\n",
       "      <td>-0.865280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>-0.660388</td>\n",
       "      <td>0.175752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.789575</td>\n",
       "      <td>1.101768</td>\n",
       "      <td>0.081290</td>\n",
       "      <td>-0.078872</td>\n",
       "      <td>-0.784724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dim_0     dim_1     dim_2     dim_3     dim_4     dim_5     dim_6  \\\n",
       "1967 -0.660388  0.372892  0.305150 -0.952829 -0.000000  0.993705 -0.054040   \n",
       "1967 -0.660388  0.316566  0.305150 -0.970150 -0.000000  0.993705 -0.069076   \n",
       "1967 -0.660388  0.147589  0.218324 -0.970150 -0.871063  1.003529 -0.069076   \n",
       "1967 -0.660388  0.006774  0.131499 -0.952829 -0.880117  0.000000 -0.076595   \n",
       "1967 -0.660388 -0.000000  0.218324 -0.970150 -0.880117  0.000000 -0.061558   \n",
       "1967 -0.660388  0.000000  0.305150 -0.970150 -0.880117  0.000000 -0.054040   \n",
       "1967 -0.000000  0.175752  0.305150 -0.970150 -0.880117  1.003529 -0.016448   \n",
       "1967 -0.000000  0.063100  0.305150 -0.970150 -0.898225  1.033001 -0.016448   \n",
       "1967 -0.000000  0.260240  0.000000 -0.970150 -0.880117  1.101768  0.006107   \n",
       "1967 -0.000000  0.119426  0.218324 -0.970150 -0.871063  1.131240  0.006107   \n",
       "1967 -0.660388  0.119426  0.218324 -0.970150 -0.852954  1.101768  0.021143   \n",
       "1967 -0.660388  0.147589  0.218324 -0.935509 -0.852954  1.082120  0.021143   \n",
       "1967 -0.660388  0.147589  0.305150 -0.935509 -0.816738  0.993705  0.021143   \n",
       "1967 -0.660388  0.232077  0.305150 -0.935509 -0.780521  0.944585  0.028661   \n",
       "1967 -0.660388  0.260240  0.218324 -0.935509 -0.789575  0.915113  0.043698   \n",
       "1967 -0.660388  0.260240  0.305150 -0.952829 -0.816738  0.895465  0.051216   \n",
       "1967 -0.660388  0.288403  0.305150 -0.970150 -0.807684  0.905289  0.028661   \n",
       "1967 -0.660388  0.000000  0.305150 -0.970150 -0.816738  0.915113  0.021143   \n",
       "1967 -0.660388  0.000000  0.305150 -0.970150 -0.807684  0.954409  0.000000   \n",
       "1967 -0.660388  0.260240  0.478801 -0.970150 -0.807684  1.003529  0.000000   \n",
       "1967 -0.660388  0.260240  0.478801 -0.970150 -0.789575  0.000000  0.000000   \n",
       "1967 -0.618282  0.232077  0.391975 -0.970150 -0.798629  0.000000  0.073771   \n",
       "1967 -0.660388  0.175752  0.000000 -0.970150 -0.798629  0.000000  0.066253   \n",
       "1967 -0.660388  0.175752  0.000000 -0.000000 -0.789575  1.101768  0.081290   \n",
       "\n",
       "         dim_7     dim_8  \n",
       "1967 -0.078872 -0.704167  \n",
       "1967 -0.078872 -0.543053  \n",
       "1967 -0.078872 -0.381940  \n",
       "1967 -0.078872 -0.704167  \n",
       "1967 -0.078872 -0.543053  \n",
       "1967 -0.078872 -0.000000  \n",
       "1967 -0.078872 -0.865280  \n",
       "1967 -0.078872 -0.784724  \n",
       "1967 -0.078872 -0.000000  \n",
       "1967 -0.078872 -0.623610  \n",
       "1967 -0.078872 -0.462497  \n",
       "1967  0.042134 -0.220827  \n",
       "1967 -0.078872 -0.381940  \n",
       "1967 -0.078872 -0.462497  \n",
       "1967 -0.078872 -0.462497  \n",
       "1967  0.042134 -0.381940  \n",
       "1967 -0.078872 -0.784724  \n",
       "1967 -0.078872 -0.059713  \n",
       "1967 -0.078872 -0.704167  \n",
       "1967  0.042134 -0.543053  \n",
       "1967  0.042134 -0.301383  \n",
       "1967 -0.078872 -0.623610  \n",
       "1967  0.163141 -0.865280  \n",
       "1967 -0.078872 -0.784724  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.feature_df.loc[train_indices[0]] * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edde9d3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:32.069655Z",
     "start_time": "2023-06-22T14:35:32.049485Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImputationDataset(Dataset):\n",
    "    \"\"\"각 샘플에 대한 missingness (noise) mask를 동적으로 계산\"\"\"\n",
    "\n",
    "    def __init__(self, data, indices, mean_mask_length=3, masking_ratio=0.15,\n",
    "                 mode='separate', distribution='geometric', exclude_feats=None):\n",
    "        super(ImputationDataset, self).__init__()\n",
    "\n",
    "        self.data = data  # this is a subclass of the BaseData class in data.py\n",
    "        self.IDs = indices  # list of data IDs, but also mapping between integer index and ID\n",
    "        self.feature_df = self.data.feature_df.loc[self.IDs]\n",
    "\n",
    "        self.masking_ratio = masking_ratio\n",
    "        self.mean_mask_length = mean_mask_length\n",
    "        self.mode = mode\n",
    "        self.distribution = distribution\n",
    "        self.exclude_feats = exclude_feats\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        \"\"\"\n",
    "        주어진 정수 인덱스에 대해 해당 (seq_length, feat_dim) 배열과 동일한 모양의 noise mask를 반환합니다.\n",
    "        Args:\n",
    "            ind: integer index of sample in dataset\n",
    "        Returns:\n",
    "            X: (seq_length, feat_dim) tensor of the multivariate time series corresponding to a sample\n",
    "            mask: (seq_length, feat_dim) boolean tensor: 0s mask and predict, 1s: unaffected input\n",
    "            ID: ID of sample\n",
    "        \"\"\"\n",
    "\n",
    "        X = self.feature_df.loc[self.IDs[ind]].values  # (seq_length, feat_dim) array\n",
    "        mask = noise_mask(X, self.masking_ratio, self.mean_mask_length, self.mode, self.distribution,\n",
    "                          self.exclude_feats)  # (seq_length, feat_dim) boolean array\n",
    "\n",
    "        return torch.from_numpy(X), torch.from_numpy(mask), self.IDs[ind]\n",
    "\n",
    "    def update(self):\n",
    "        self.mean_mask_length = min(20, self.mean_mask_length + 1)\n",
    "        self.masking_ratio = min(1, self.masking_ratio + 0.05)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b4feb",
   "metadata": {},
   "source": [
    "해당 dataset generator를 통해서 input X, noise mask, index를 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e407b4eb",
   "metadata": {},
   "source": [
    "Official code에서는 data generator, collate function, runner가 한번에 반환되는 pipeline이 구축되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dd14bb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:35.721541Z",
     "start_time": "2023-06-22T14:35:35.582563Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize data generators\n",
    "dataset_class, collate_fn, runner_class = pipeline_factory(config)    \n",
    "\"\"\"\n",
    "dataset_class : (X, mask, index)를 도출하는 데이터셋 생성하는 class\n",
    "collate_fn : DataLoader에서 각각의 데이터 샘플을 어떻게 배치로 결합할지 결정하는 함수\n",
    "runner_class : 학습 및 테스트 과정에 대한 class\n",
    "\"\"\"\n",
    "val_dataset = dataset_class(val_data, val_indices)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=config['batch_size'],\n",
    "                        shuffle=False,\n",
    "                        num_workers=config['num_workers'],\n",
    "                        pin_memory=True,\n",
    "                        collate_fn=lambda x: collate_fn(x, max_len=config['max_seq_len']))\n",
    "\n",
    "train_dataset = dataset_class(my_data, train_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                            batch_size=config['batch_size'],\n",
    "                            shuffle=True,\n",
    "                            num_workers=config['num_workers'],\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=lambda x: collate_fn(x, max_len=config['max_seq_len']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82c827a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:39.270286Z",
     "start_time": "2023-06-22T14:35:37.016321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 24, 9]) torch.Size([32, 24, 9]) torch.Size([32, 24, 9]) torch.Size([32, 24])\n"
     ]
    }
   ],
   "source": [
    "X, targets, target_masks, padding_masks, IDs = next(iter(train_loader))\n",
    "print(X.shape, targets.shape, target_masks.shape, padding_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9a714b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:39.548363Z",
     "start_time": "2023-06-22T14:35:39.536920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  True, False, False, False,  True, False],\n",
       "        [False, False, False,  True, False, False, False,  True, False],\n",
       "        [False, False, False,  True, False, False,  True,  True, False],\n",
       "        [False, False, False,  True, False, False,  True,  True, False],\n",
       "        [False, False, False,  True, False, False, False, False, False],\n",
       "        [False, False, False,  True, False, False, False, False, False],\n",
       "        [False, False, False,  True, False,  True, False, False, False],\n",
       "        [False, False, False, False,  True,  True, False,  True, False],\n",
       "        [False, False, False, False, False,  True, False,  True, False],\n",
       "        [False, False, False, False,  True,  True, False, False, False],\n",
       "        [False, False,  True, False, False,  True, False, False, False],\n",
       "        [False, False,  True, False, False,  True, False, False, False],\n",
       "        [ True, False,  True,  True, False,  True, False, False, False],\n",
       "        [ True, False, False,  True, False,  True, False, False, False],\n",
       "        [False, False, False,  True, False,  True, False, False, False],\n",
       "        [False, False,  True,  True, False,  True, False, False, False],\n",
       "        [False, False, False,  True, False,  True, False, False, False],\n",
       "        [False, False, False, False, False,  True, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False],\n",
       "        [ True, False, False, False, False, False, False, False, False],\n",
       "        [ True, False, False, False, False, False, False, False, False],\n",
       "        [ True, False, False, False, False,  True, False, False, False],\n",
       "        [ True, False, False, False,  True, False, False, False, False],\n",
       "        [ True, False, False, False,  True, False, False, False, False]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d40eaf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:39.788218Z",
     "start_time": "2023-06-22T14:35:39.783089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1081,  0.7953,  2.5626, -0.0000, -1.7584,  1.6028, -0.9262, -0.0000,\n",
       "         -0.8653],\n",
       "        [ 0.6870,  0.3166,  1.4339, -0.0000, -1.8036,  1.5733, -0.9487, -0.0000,\n",
       "         -1.2681],\n",
       "        [ 1.4870, -0.1622,  0.9129, -0.0000, -1.8489,  1.6028, -0.0000, -0.0000,\n",
       "         -0.8653],\n",
       "        [-0.1130, -0.0777,  1.0432, -0.0000, -1.8489,  1.5733, -0.0000, -0.0000,\n",
       "         -1.1070],\n",
       "        [-0.5341,  0.1758,  1.1734, -0.0000, -1.8308,  1.5144, -0.9562, -0.0789,\n",
       "         -0.1403],\n",
       "        [-0.4078,  0.6264,  1.3471, -0.0000, -1.8217,  1.5242, -0.9638, -0.0789,\n",
       "         -0.3014],\n",
       "        [ 0.1817,  0.4574,  1.7812, -0.0000, -1.8127,  0.0000, -0.9938, -0.0789,\n",
       "         -0.7042],\n",
       "        [ 0.4344,  0.3729,  1.6075, -0.7450, -0.0000,  0.0000, -1.0089, -0.0000,\n",
       "          0.0208],\n",
       "        [ 0.6028,  0.3447,  1.5207, -0.7623, -1.8127,  0.0000, -1.0089, -0.0000,\n",
       "         -0.3819],\n",
       "        [ 0.6028,  0.1476,  1.0866, -0.7796, -0.0000,  0.0000, -0.9412, -0.0789,\n",
       "         -0.3014],\n",
       "        [ 0.7712,  0.3447,  0.0000, -0.7277, -1.5954,  0.0000, -0.9036, -0.0789,\n",
       "         -0.3014],\n",
       "        [ 0.2659,  0.9925,  0.0000, -0.6757, -1.5049,  0.0000, -0.7833, -0.0789,\n",
       "         -0.1403],\n",
       "        [ 0.0000,  1.6402,  0.0000, -0.0000, -1.4596,  0.0000, -0.6555, -0.0789,\n",
       "         -0.1403],\n",
       "        [ 0.0000,  1.7529,  2.4758, -0.0000, -1.3690,  0.0000, -0.6781, -0.0789,\n",
       "          0.0208],\n",
       "        [ 0.5607,  1.6121,  1.9548, -0.0000, -1.2966,  0.0000, -0.6931, -0.0789,\n",
       "         -0.2208],\n",
       "        [ 1.3607,  1.6965,  0.0000, -0.0000, -1.2061,  0.0000, -0.7006, -0.0789,\n",
       "         -0.3014],\n",
       "        [ 2.3712,  2.3161,  1.8680, -0.0000, -1.2061,  0.0000, -0.6856, -0.0789,\n",
       "          0.1820],\n",
       "        [ 2.2870,  2.3161,  2.1285, -0.6930, -1.2966,  0.0000, -0.8209, -0.0789,\n",
       "         -0.3819],\n",
       "        [ 1.7818,  2.2598,  2.4758, -0.7103, -1.2966,  1.4849, -0.7683, -0.0789,\n",
       "         -0.3014],\n",
       "        [ 0.0000,  2.7667,  3.7782, -0.6411, -1.2966,  0.9937, -0.6781, -0.0789,\n",
       "         -0.3014],\n",
       "        [ 0.0000,  2.7386,  3.9518, -0.6584, -1.4777,  1.4849, -0.6856, -0.0789,\n",
       "         -1.3486],\n",
       "        [-0.0000,  2.4569,  4.1255, -0.6411, -1.6588,  0.0000, -0.7307, -0.0789,\n",
       "         -0.7847],\n",
       "        [-0.0000,  2.3161,  4.2991, -0.6411, -0.0000,  0.9937, -0.7081, -0.0789,\n",
       "         -1.3486],\n",
       "        [-0.0000,  2.1472,  4.1255, -0.6584, -0.0000,  1.2884, -0.7984, -0.0789,\n",
       "         -0.8653]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56e467ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:40.039490Z",
     "start_time": "2023-06-22T14:35:40.027941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1081,  0.7953,  2.5626, -0.6411, -1.7584,  1.6028, -0.9262, -0.0789,\n",
       "         -0.8653],\n",
       "        [ 0.6870,  0.3166,  1.4339, -0.7969, -1.8036,  1.5733, -0.9487, -0.0789,\n",
       "         -1.2681],\n",
       "        [ 1.4870, -0.1622,  0.9129, -0.9355, -1.8489,  1.6028, -1.0239, -0.0789,\n",
       "         -0.8653],\n",
       "        [-0.1130, -0.0777,  1.0432, -0.7103, -1.8489,  1.5733, -0.9713, -0.0789,\n",
       "         -1.1070],\n",
       "        [-0.5341,  0.1758,  1.1734, -0.7796, -1.8308,  1.5144, -0.9562, -0.0789,\n",
       "         -0.1403],\n",
       "        [-0.4078,  0.6264,  1.3471, -0.8143, -1.8217,  1.5242, -0.9638, -0.0789,\n",
       "         -0.3014],\n",
       "        [ 0.1817,  0.4574,  1.7812, -0.7277, -1.8127,  1.5537, -0.9938, -0.0789,\n",
       "         -0.7042],\n",
       "        [ 0.4344,  0.3729,  1.6075, -0.7450, -1.8036,  1.5831, -1.0089, -0.0789,\n",
       "          0.0208],\n",
       "        [ 0.6028,  0.3447,  1.5207, -0.7623, -1.8127,  1.6028, -1.0089, -0.0789,\n",
       "         -0.3819],\n",
       "        [ 0.6028,  0.1476,  1.0866, -0.7796, -1.6950,  1.6421, -0.9412, -0.0789,\n",
       "         -0.3014],\n",
       "        [ 0.7712,  0.3447,  1.6944, -0.7277, -1.5954,  1.6617, -0.9036, -0.0789,\n",
       "         -0.3014],\n",
       "        [ 0.2659,  0.9925,  2.9967, -0.6757, -1.5049,  1.6323, -0.7833, -0.0789,\n",
       "         -0.1403],\n",
       "        [ 0.2238,  1.6402,  2.6494, -0.7103, -1.4596,  1.5438, -0.6555, -0.0789,\n",
       "         -0.1403],\n",
       "        [ 0.4765,  1.7529,  2.4758, -0.7103, -1.3690,  1.4063, -0.6781, -0.0789,\n",
       "          0.0208],\n",
       "        [ 0.5607,  1.6121,  1.9548, -0.7277, -1.2966,  0.9937, -0.6931, -0.0789,\n",
       "         -0.2208],\n",
       "        [ 1.3607,  1.6965,  1.6075, -0.7277, -1.2061,  1.4849, -0.7006, -0.0789,\n",
       "         -0.3014],\n",
       "        [ 2.3712,  2.3161,  1.8680, -0.6930, -1.2061,  1.1902, -0.6856, -0.0789,\n",
       "          0.1820],\n",
       "        [ 2.2870,  2.3161,  2.1285, -0.6930, -1.2966,  0.9937, -0.8209, -0.0789,\n",
       "         -0.3819],\n",
       "        [ 1.7818,  2.2598,  2.4758, -0.7103, -1.2966,  1.4849, -0.7683, -0.0789,\n",
       "         -0.3014],\n",
       "        [ 1.5291,  2.7667,  3.7782, -0.6411, -1.2966,  0.9937, -0.6781, -0.0789,\n",
       "         -0.3014],\n",
       "        [ 0.4344,  2.7386,  3.9518, -0.6584, -1.4777,  1.4849, -0.6856, -0.0789,\n",
       "         -1.3486],\n",
       "        [-0.3235,  2.4569,  4.1255, -0.6411, -1.6588,  1.1902, -0.7307, -0.0789,\n",
       "         -0.7847],\n",
       "        [-0.4920,  2.3161,  4.2991, -0.6411, -1.6588,  0.9937, -0.7081, -0.0789,\n",
       "         -1.3486],\n",
       "        [-0.6183,  2.1472,  4.1255, -0.6584, -1.7493,  1.2884, -0.7984, -0.0789,\n",
       "         -0.8653]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d6f238",
   "metadata": {},
   "source": [
    "### STEP 5. 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d339b56",
   "metadata": {},
   "source": [
    "<img src=\"./image/TST03.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24efc18b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:42.941791Z",
     "start_time": "2023-06-22T14:35:42.922155Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model_factory(config, my_data)\n",
    "\n",
    "# Initialize optimizer\n",
    "# L2 regularization\n",
    "if config['global_reg']:\n",
    "    weight_decay = config['l2_reg']\n",
    "    output_reg = None\n",
    "else:\n",
    "    weight_decay = 0\n",
    "    output_reg = config['l2_reg']\n",
    "\n",
    "optim_class = get_optimizer(config['optimizer'])\n",
    "optimizer = optim_class(model.parameters(), lr=config['lr'], weight_decay=weight_decay)\n",
    "\n",
    "lr_step = 0  # current step index of `lr_step`\n",
    "lr = config['lr']  # current learning step\n",
    "\n",
    "# 학습된 weight을 불러올 때 사용됩니다.\n",
    "if args.load_model:\n",
    "    model, optimizer, start_epoch = utils.load_model(model, config['load_model'], optimizer, config['resume'],\n",
    "                                                    config['change_output'],\n",
    "                                                    config['lr'],\n",
    "                                                    config['lr_step'],\n",
    "                                                    config['lr_factor'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_module = get_loss_module(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b0cf41",
   "metadata": {},
   "source": [
    "pre-training에 맞게 마지막 Linear layer의 output이 9개가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30f4ce42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:45.011330Z",
     "start_time": "2023-06-22T14:35:45.003104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TSTransformerEncoder(\n",
       "  (project_inp): Linear(in_features=9, out_features=128, bias=True)\n",
       "  (pos_enc): LearnablePositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerBatchNormEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerBatchNormEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerBatchNormEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=128, out_features=9, bias=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48da840",
   "metadata": {},
   "source": [
    "### STEP 6. Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6185391b",
   "metadata": {},
   "source": [
    "Trainer 이해하기\n",
    "- 학습하는 모든 과정은 어떤 모델, 어떤 구조를 사용하더라도 항상 반복됩니다\n",
    "- 반복되는 과정은 하나의 파이프라인으로 구성할 수 있으며, 대부분의 개발자와 연구자들은 아래와 같이 Trainer를 만들어 학습을 진행합니다\n",
    "- 모든 딥러닝 모델은 데이터를 생성하고 - 모델을 정의하고 - 반복학습(epoch)을 통해 여러번 학습되므로 모든 과정은 구조화 되어있음을 알 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "226143b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T15:52:54.587913Z",
     "start_time": "2023-06-21T15:52:54.455886Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseRunner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-1fe57be72c9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mUnsupervisedRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseRunner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \"\"\"\n\u001b[1;32m      5\u001b[0m         \u001b[0mepoch마다\u001b[0m \u001b[0m수행되는\u001b[0m \u001b[0m학습\u001b[0m \u001b[0m과정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseRunner' is not defined"
     ]
    }
   ],
   "source": [
    "class UnsupervisedRunner(BaseRunner):\n",
    "\n",
    "    def train_epoch(self, epoch_num=None):\n",
    "        \"\"\"\n",
    "        epoch마다 수행되는 학습 과정\n",
    "        Args:\n",
    "            epoch_num: epoch 번호\n",
    "\n",
    "        Returns:\n",
    "            해당 epoch의 metric 결과\n",
    "        \"\"\"\n",
    "        \n",
    "        '''\n",
    "            STEP 1) 초기 세팅\n",
    "        '''\n",
    "        self.model = self.model.train()\n",
    "\n",
    "        epoch_loss = 0  # epoch의 총 loss\n",
    "        total_active_elements = 0  # epoch의 총 unmasked elements 수\n",
    "        \n",
    "        '''\n",
    "            STEP 2) 한 epoch에서 수행되는 학습 과정\n",
    "        '''\n",
    "        for i, batch in enumerate(self.dataloader):\n",
    "            '''\n",
    "            STEP 3) 하나의 batch에서 수행되는 학습 과정\n",
    "            '''\n",
    "            X, targets, target_masks, padding_masks, IDs = batch\n",
    "            targets = targets.to(self.device)\n",
    "            target_masks = target_masks.to(self.device)\n",
    "            padding_masks = padding_masks.to(self.device) \n",
    "\n",
    "            predictions = self.model(X.to(self.device), padding_masks)  # (batch_size, padded_length, feat_dim)\n",
    "\n",
    "            # Cascade noise masks (batch_size, padded_length, feat_dim) and padding masks (batch_size, padded_length)\n",
    "            target_masks = target_masks * padding_masks.unsqueeze(-1)\n",
    "            loss = self.loss_module(predictions, targets, target_masks)  # (num_active,) individual loss (square error per element) for each active value in batch\n",
    "            batch_loss = torch.sum(loss)\n",
    "            mean_loss = batch_loss / len(loss)  # mean loss (over active elements) used for optimization\n",
    "\n",
    "            if self.l2_reg:\n",
    "                total_loss = mean_loss + self.l2_reg * l2_reg_loss(self.model)\n",
    "            else:\n",
    "                total_loss = mean_loss\n",
    "\n",
    "            # gradients를 제로화하고, backward pass를 수행하고, 가중치를 업데이트합니다.\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=4.0)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            metrics = {\"loss\": mean_loss.item()}\n",
    "            if i % self.print_interval == 0:\n",
    "                ending = \"\" if epoch_num is None else 'Epoch {} '.format(epoch_num)\n",
    "                self.print_callback(i, metrics, prefix='Training ' + ending)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                total_active_elements += len(loss)\n",
    "                epoch_loss += batch_loss.item()  # add total loss of batch\n",
    "\n",
    "        epoch_loss = epoch_loss / total_active_elements  # average loss per element for whole epoch\n",
    "        self.epoch_metrics['epoch'] = epoch_num\n",
    "        self.epoch_metrics['loss'] = epoch_loss\n",
    "        return self.epoch_metrics\n",
    "\n",
    "    def evaluate(self, epoch_num=None, keep_all=True):\n",
    "        \"\"\"\n",
    "        학습된 모델의 평가 함수\n",
    "        Args:\n",
    "            epoch_num: epoch 번호\n",
    "            keep_all: 매 평가마다 배치 정보를 남길 것인지에 대한 변수\n",
    "\n",
    "        Returns:\n",
    "            해당 epoch의 metric 결과\n",
    "        \"\"\"\n",
    "\n",
    "        '''\n",
    "            STEP 1) 초기 세팅\n",
    "        '''\n",
    "        self.model = self.model.eval()\n",
    "\n",
    "        epoch_loss = 0  # total loss of epoch\n",
    "        total_active_elements = 0  # total unmasked elements in epoch\n",
    "\n",
    "        if keep_all:\n",
    "            per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': []}\n",
    "            \n",
    "        '''\n",
    "            STEP 2) 한 epoch에서 수행되는 평가 과정\n",
    "        '''\n",
    "        for i, batch in enumerate(self.dataloader):\n",
    "            '''\n",
    "            STEP 3) 하나의 batch에서 수행되는 학습 과정\n",
    "            '''\n",
    "            X, targets, target_masks, padding_masks, IDs = batch\n",
    "            targets = targets.to(self.device)\n",
    "            target_masks = target_masks.to(self.device)  # 1s: mask and predict, 0s: unaffected input (ignore)\n",
    "            padding_masks = padding_masks.to(self.device)  # 0s: ignore\n",
    "\n",
    "            predictions = self.model(X.to(self.device), padding_masks)  # (batch_size, padded_length, feat_dim)\n",
    "\n",
    "            # Cascade noise masks (batch_size, padded_length, feat_dim) and padding masks (batch_size, padded_length)\n",
    "            target_masks = target_masks * padding_masks.unsqueeze(-1)\n",
    "            loss = self.loss_module(predictions, targets, target_masks)  # (num_active,) individual loss (square error per element) for each active value in batch\n",
    "            batch_loss = torch.sum(loss).cpu().item()\n",
    "            mean_loss = batch_loss / len(loss)  # mean loss (over active elements) used for optimization the batch\n",
    "\n",
    "            if keep_all:\n",
    "                per_batch['target_masks'].append(target_masks.cpu().numpy())\n",
    "                per_batch['targets'].append(targets.cpu().numpy())\n",
    "                per_batch['predictions'].append(predictions.cpu().numpy())\n",
    "                per_batch['metrics'].append([loss.cpu().numpy()])\n",
    "                per_batch['IDs'].append(IDs)\n",
    "\n",
    "            metrics = {\"loss\": mean_loss}\n",
    "            if i % self.print_interval == 0:\n",
    "                ending = \"\" if epoch_num is None else 'Epoch {} '.format(epoch_num)\n",
    "                self.print_callback(i, metrics, prefix='Evaluating ' + ending)\n",
    "\n",
    "            total_active_elements += len(loss)\n",
    "            epoch_loss += batch_loss  # add total loss of batch\n",
    "\n",
    "        epoch_loss = epoch_loss / total_active_elements  # average loss per element for whole epoch\n",
    "        self.epoch_metrics['epoch'] = epoch_num\n",
    "        self.epoch_metrics['loss'] = epoch_loss\n",
    "\n",
    "        if keep_all:\n",
    "            return self.epoch_metrics, per_batch\n",
    "        else:\n",
    "            return self.epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31117a77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:35:57.981376Z",
     "start_time": "2023-06-22T14:35:55.835892Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:35:56,776 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 0   0.0% | batch:         0 of        75\t|\tloss: 40.0271\n",
      "Evaluating Epoch 0   1.3% | batch:         1 of        75\t|\tloss: 57.4144\n",
      "Evaluating Epoch 0   2.7% | batch:         2 of        75\t|\tloss: 18.5814\n",
      "Evaluating Epoch 0   4.0% | batch:         3 of        75\t|\tloss: 32.4551\n",
      "Evaluating Epoch 0   5.3% | batch:         4 of        75\t|\tloss: 54.7981\n",
      "Evaluating Epoch 0   6.7% | batch:         5 of        75\t|\tloss: 77.6586\n",
      "Evaluating Epoch 0   8.0% | batch:         6 of        75\t|\tloss: 22.9577\n",
      "Evaluating Epoch 0   9.3% | batch:         7 of        75\t|\tloss: 16.8651\n",
      "Evaluating Epoch 0  10.7% | batch:         8 of        75\t|\tloss: 22.5102\n",
      "Evaluating Epoch 0  12.0% | batch:         9 of        75\t|\tloss: 29.3689\n",
      "Evaluating Epoch 0  13.3% | batch:        10 of        75\t|\tloss: 27.5272\n",
      "Evaluating Epoch 0  14.7% | batch:        11 of        75\t|\tloss: 14.1647\n",
      "Evaluating Epoch 0  16.0% | batch:        12 of        75\t|\tloss: 33.1974\n",
      "Evaluating Epoch 0  17.3% | batch:        13 of        75\t|\tloss: 28.9797\n",
      "Evaluating Epoch 0  18.7% | batch:        14 of        75\t|\tloss: 15.6806\n",
      "Evaluating Epoch 0  20.0% | batch:        15 of        75\t|\tloss: 65.2778\n",
      "Evaluating Epoch 0  21.3% | batch:        16 of        75\t|\tloss: 22.3002\n",
      "Evaluating Epoch 0  22.7% | batch:        17 of        75\t|\tloss: 70.9251\n",
      "Evaluating Epoch 0  24.0% | batch:        18 of        75\t|\tloss: 17.2443\n",
      "Evaluating Epoch 0  25.3% | batch:        19 of        75\t|\tloss: 28.0279\n",
      "Evaluating Epoch 0  26.7% | batch:        20 of        75\t|\tloss: 22.4216\n",
      "Evaluating Epoch 0  28.0% | batch:        21 of        75\t|\tloss: 14.7466\n",
      "Evaluating Epoch 0  29.3% | batch:        22 of        75\t|\tloss: 36.9305\n",
      "Evaluating Epoch 0  30.7% | batch:        23 of        75\t|\tloss: 76.3947\n",
      "Evaluating Epoch 0  32.0% | batch:        24 of        75\t|\tloss: 37.9525\n",
      "Evaluating Epoch 0  33.3% | batch:        25 of        75\t|\tloss: 20.7361\n",
      "Evaluating Epoch 0  34.7% | batch:        26 of        75\t|\tloss: 17.4067\n",
      "Evaluating Epoch 0  36.0% | batch:        27 of        75\t|\tloss: 22.467\n",
      "Evaluating Epoch 0  37.3% | batch:        28 of        75\t|\tloss: 22.2162\n",
      "Evaluating Epoch 0  38.7% | batch:        29 of        75\t|\tloss: 22.9431\n",
      "Evaluating Epoch 0  40.0% | batch:        30 of        75\t|\tloss: 28.1132\n",
      "Evaluating Epoch 0  41.3% | batch:        31 of        75\t|\tloss: 19.5267\n",
      "Evaluating Epoch 0  42.7% | batch:        32 of        75\t|\tloss: 51.1333\n",
      "Evaluating Epoch 0  44.0% | batch:        33 of        75\t|\tloss: 23.2203\n",
      "Evaluating Epoch 0  45.3% | batch:        34 of        75\t|\tloss: 17.2117\n",
      "Evaluating Epoch 0  46.7% | batch:        35 of        75\t|\tloss: 42.7659\n",
      "Evaluating Epoch 0  48.0% | batch:        36 of        75\t|\tloss: 18.6499\n",
      "Evaluating Epoch 0  49.3% | batch:        37 of        75\t|\tloss: 42.2948\n",
      "Evaluating Epoch 0  50.7% | batch:        38 of        75\t|\tloss: 120.141\n",
      "Evaluating Epoch 0  52.0% | batch:        39 of        75\t|\tloss: 42.3141\n",
      "Evaluating Epoch 0  53.3% | batch:        40 of        75\t|\tloss: 43.9027\n",
      "Evaluating Epoch 0  54.7% | batch:        41 of        75\t|\tloss: 15.9965\n",
      "Evaluating Epoch 0  56.0% | batch:        42 of        75\t|\tloss: 47.8574\n",
      "Evaluating Epoch 0  57.3% | batch:        43 of        75\t|\tloss: 18.8359\n",
      "Evaluating Epoch 0  58.7% | batch:        44 of        75\t|\tloss: 32.2864\n",
      "Evaluating Epoch 0  60.0% | batch:        45 of        75\t|\tloss: 19.1837\n",
      "Evaluating Epoch 0  61.3% | batch:        46 of        75\t|\tloss: 21.7326\n",
      "Evaluating Epoch 0  62.7% | batch:        47 of        75\t|\tloss: 60.9428\n",
      "Evaluating Epoch 0  64.0% | batch:        48 of        75\t|\tloss: 20.05\n",
      "Evaluating Epoch 0  65.3% | batch:        49 of        75\t|\tloss: 16.9068\n",
      "Evaluating Epoch 0  66.7% | batch:        50 of        75\t|\tloss: 29.0313\n",
      "Evaluating Epoch 0  68.0% | batch:        51 of        75\t|\tloss: 19.0894\n",
      "Evaluating Epoch 0  69.3% | batch:        52 of        75\t|\tloss: 38.7034\n",
      "Evaluating Epoch 0  70.7% | batch:        53 of        75\t|\tloss: 24.4266\n",
      "Evaluating Epoch 0  72.0% | batch:        54 of        75\t|\tloss: 26.0798\n",
      "Evaluating Epoch 0  73.3% | batch:        55 of        75\t|\tloss: 25.3931\n",
      "Evaluating Epoch 0  74.7% | batch:        56 of        75\t|\tloss: 19.8613\n",
      "Evaluating Epoch 0  76.0% | batch:        57 of        75\t|\tloss: 20.2464\n",
      "Evaluating Epoch 0  77.3% | batch:        58 of        75\t|\tloss: 17.6441\n",
      "Evaluating Epoch 0  78.7% | batch:        59 of        75\t|\tloss: 151.552\n",
      "Evaluating Epoch 0  80.0% | batch:        60 of        75\t|\tloss: 35.7835\n",
      "Evaluating Epoch 0  81.3% | batch:        61 of        75\t|\tloss: 12.9857\n",
      "Evaluating Epoch 0  82.7% | batch:        62 of        75\t|\tloss: 24.2851\n",
      "Evaluating Epoch 0  84.0% | batch:        63 of        75\t|\tloss: 46.4368\n",
      "Evaluating Epoch 0  85.3% | batch:        64 of        75\t|\tloss: 38.041\n",
      "Evaluating Epoch 0  86.7% | batch:        65 of        75\t|\tloss: 29.9147\n",
      "Evaluating Epoch 0  88.0% | batch:        66 of        75\t|\tloss: 22.1787\n",
      "Evaluating Epoch 0  89.3% | batch:        67 of        75\t|\tloss: 53.4826\n",
      "Evaluating Epoch 0  90.7% | batch:        68 of        75\t|\tloss: 18.3568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:35:57,949 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.1719434261322021 seconds\n",
      "\n",
      "2023-06-22 14:35:57,950 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.1719434261322021 seconds\n",
      "2023-06-22 14:35:57,951 | INFO : Avg batch val. time: 0.01562591234842936 seconds\n",
      "2023-06-22 14:35:57,952 | INFO : Avg sample val. time: 0.0004915870076057895 seconds\n",
      "2023-06-22 14:35:57,952 | INFO : Epoch 0 Validation Summary: epoch: 0.000000 | loss: 33.298641 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 0  92.0% | batch:        69 of        75\t|\tloss: 17.0672\n",
      "Evaluating Epoch 0  93.3% | batch:        70 of        75\t|\tloss: 18.6113\n",
      "Evaluating Epoch 0  94.7% | batch:        71 of        75\t|\tloss: 22.8909\n",
      "Evaluating Epoch 0  96.0% | batch:        72 of        75\t|\tloss: 14.2727\n",
      "Evaluating Epoch 0  97.3% | batch:        73 of        75\t|\tloss: 23.9697\n",
      "Evaluating Epoch 0  98.7% | batch:        74 of        75\t|\tloss: 55.3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = runner_class(model, train_loader, device, loss_module, optimizer, l2_reg=output_reg,\n",
    "                       print_interval=config['print_interval'], console=config['console'])\n",
    "val_evaluator = runner_class(model, val_loader, device, loss_module,\n",
    "                             print_interval=config['print_interval'], console=config['console'])\n",
    "\n",
    "tensorboard_writer = SummaryWriter(config['tensorboard_dir'])\n",
    "\n",
    "best_value = 1e16 if config['key_metric'] in NEG_METRICS else -1e16  # initialize with +inf or -inf depending on key metric\n",
    "metrics = []  # (for validation) list of lists: for each epoch, stores metrics like loss, ...\n",
    "best_metrics = {}\n",
    "\n",
    "# 학습되지 않은 모델로 초기 성능을 확인합니다.\n",
    "aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, \n",
    "                                                      tensorboard_writer, \n",
    "                                                      config, best_metrics,\n",
    "                                                      best_value, \n",
    "                                                      epoch=0)\n",
    "\n",
    "metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "metrics.append(list(metrics_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd9e7de",
   "metadata": {},
   "source": [
    "pre-training을 시작해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41e46d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:35:59.343Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Epoch:   0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1   0.0% | batch:         0 of       298\t|\tloss: 1.33582\n",
      "Training Epoch 1   0.3% | batch:         1 of       298\t|\tloss: 0.754723\n",
      "Training Epoch 1   0.7% | batch:         2 of       298\t|\tloss: 1.12153\n",
      "Training Epoch 1   1.0% | batch:         3 of       298\t|\tloss: 1.12192\n",
      "Training Epoch 1   1.3% | batch:         4 of       298\t|\tloss: 0.780175\n",
      "Training Epoch 1   1.7% | batch:         5 of       298\t|\tloss: 0.80955\n",
      "Training Epoch 1   2.0% | batch:         6 of       298\t|\tloss: 1.02738\n",
      "Training Epoch 1   2.3% | batch:         7 of       298\t|\tloss: 0.836399\n",
      "Training Epoch 1   2.7% | batch:         8 of       298\t|\tloss: 1.24348\n",
      "Training Epoch 1   3.0% | batch:         9 of       298\t|\tloss: 1.02266\n",
      "Training Epoch 1   3.4% | batch:        10 of       298\t|\tloss: 0.7257\n",
      "Training Epoch 1   3.7% | batch:        11 of       298\t|\tloss: 0.978266\n",
      "Training Epoch 1   4.0% | batch:        12 of       298\t|\tloss: 1.05213\n",
      "Training Epoch 1   4.4% | batch:        13 of       298\t|\tloss: 0.944962\n",
      "Training Epoch 1   4.7% | batch:        14 of       298\t|\tloss: 0.736492\n",
      "Training Epoch 1   5.0% | batch:        15 of       298\t|\tloss: 0.86563\n",
      "Training Epoch 1   5.4% | batch:        16 of       298\t|\tloss: 0.956742\n",
      "Training Epoch 1   5.7% | batch:        17 of       298\t|\tloss: 1.72151\n",
      "Training Epoch 1   6.0% | batch:        18 of       298\t|\tloss: 0.799121\n",
      "Training Epoch 1   6.4% | batch:        19 of       298\t|\tloss: 0.7764\n",
      "Training Epoch 1   6.7% | batch:        20 of       298\t|\tloss: 0.877091\n",
      "Training Epoch 1   7.0% | batch:        21 of       298\t|\tloss: 0.645279\n",
      "Training Epoch 1   7.4% | batch:        22 of       298\t|\tloss: 0.940021\n",
      "Training Epoch 1   7.7% | batch:        23 of       298\t|\tloss: 0.897677\n",
      "Training Epoch 1   8.1% | batch:        24 of       298\t|\tloss: 0.745466\n",
      "Training Epoch 1   8.4% | batch:        25 of       298\t|\tloss: 0.649763\n",
      "Training Epoch 1   8.7% | batch:        26 of       298\t|\tloss: 0.809262\n",
      "Training Epoch 1   9.1% | batch:        27 of       298\t|\tloss: 0.655816\n",
      "Training Epoch 1   9.4% | batch:        28 of       298\t|\tloss: 2.48496\n",
      "Training Epoch 1   9.7% | batch:        29 of       298\t|\tloss: 0.653792\n",
      "Training Epoch 1  10.1% | batch:        30 of       298\t|\tloss: 0.523681\n",
      "Training Epoch 1  10.4% | batch:        31 of       298\t|\tloss: 1.1275\n",
      "Training Epoch 1  10.7% | batch:        32 of       298\t|\tloss: 0.924231\n",
      "Training Epoch 1  11.1% | batch:        33 of       298\t|\tloss: 0.746342\n",
      "Training Epoch 1  11.4% | batch:        34 of       298\t|\tloss: 0.64938\n",
      "Training Epoch 1  11.7% | batch:        35 of       298\t|\tloss: 0.587394\n",
      "Training Epoch 1  12.1% | batch:        36 of       298\t|\tloss: 0.651785\n",
      "Training Epoch 1  12.4% | batch:        37 of       298\t|\tloss: 0.445758\n",
      "Training Epoch 1  12.8% | batch:        38 of       298\t|\tloss: 0.913583\n",
      "Training Epoch 1  13.1% | batch:        39 of       298\t|\tloss: 0.526278\n",
      "Training Epoch 1  13.4% | batch:        40 of       298\t|\tloss: 0.546718\n",
      "Training Epoch 1  13.8% | batch:        41 of       298\t|\tloss: 0.851121\n",
      "Training Epoch 1  14.1% | batch:        42 of       298\t|\tloss: 0.601125\n",
      "Training Epoch 1  14.4% | batch:        43 of       298\t|\tloss: 0.706966\n",
      "Training Epoch 1  14.8% | batch:        44 of       298\t|\tloss: 0.558283\n",
      "Training Epoch 1  15.1% | batch:        45 of       298\t|\tloss: 0.579156\n",
      "Training Epoch 1  15.4% | batch:        46 of       298\t|\tloss: 0.48039\n",
      "Training Epoch 1  15.8% | batch:        47 of       298\t|\tloss: 1.46404\n",
      "Training Epoch 1  16.1% | batch:        48 of       298\t|\tloss: 0.706463\n",
      "Training Epoch 1  16.4% | batch:        49 of       298\t|\tloss: 0.548204\n",
      "Training Epoch 1  16.8% | batch:        50 of       298\t|\tloss: 0.547083\n",
      "Training Epoch 1  17.1% | batch:        51 of       298\t|\tloss: 0.959498\n",
      "Training Epoch 1  17.4% | batch:        52 of       298\t|\tloss: 0.513918\n",
      "Training Epoch 1  17.8% | batch:        53 of       298\t|\tloss: 0.367001\n",
      "Training Epoch 1  18.1% | batch:        54 of       298\t|\tloss: 0.52832\n",
      "Training Epoch 1  18.5% | batch:        55 of       298\t|\tloss: 0.353936\n",
      "Training Epoch 1  18.8% | batch:        56 of       298\t|\tloss: 0.494812\n",
      "Training Epoch 1  19.1% | batch:        57 of       298\t|\tloss: 0.460058\n",
      "Training Epoch 1  19.5% | batch:        58 of       298\t|\tloss: 0.387207\n",
      "Training Epoch 1  19.8% | batch:        59 of       298\t|\tloss: 0.483766\n",
      "Training Epoch 1  20.1% | batch:        60 of       298\t|\tloss: 0.365538\n",
      "Training Epoch 1  20.5% | batch:        61 of       298\t|\tloss: 0.468134\n",
      "Training Epoch 1  20.8% | batch:        62 of       298\t|\tloss: 0.4331\n",
      "Training Epoch 1  21.1% | batch:        63 of       298\t|\tloss: 0.447059\n",
      "Training Epoch 1  21.5% | batch:        64 of       298\t|\tloss: 0.370805\n",
      "Training Epoch 1  21.8% | batch:        65 of       298\t|\tloss: 0.400029\n",
      "Training Epoch 1  22.1% | batch:        66 of       298\t|\tloss: 0.46972\n",
      "Training Epoch 1  22.5% | batch:        67 of       298\t|\tloss: 0.616345\n",
      "Training Epoch 1  22.8% | batch:        68 of       298\t|\tloss: 0.428221\n",
      "Training Epoch 1  23.2% | batch:        69 of       298\t|\tloss: 0.402923\n",
      "Training Epoch 1  23.5% | batch:        70 of       298\t|\tloss: 0.532187\n",
      "Training Epoch 1  23.8% | batch:        71 of       298\t|\tloss: 0.424173\n",
      "Training Epoch 1  24.2% | batch:        72 of       298\t|\tloss: 0.41964\n",
      "Training Epoch 1  24.5% | batch:        73 of       298\t|\tloss: 0.374524\n",
      "Training Epoch 1  24.8% | batch:        74 of       298\t|\tloss: 0.402396\n",
      "Training Epoch 1  25.2% | batch:        75 of       298\t|\tloss: 0.31221\n",
      "Training Epoch 1  25.5% | batch:        76 of       298\t|\tloss: 0.601463\n",
      "Training Epoch 1  25.8% | batch:        77 of       298\t|\tloss: 1.21037\n",
      "Training Epoch 1  26.2% | batch:        78 of       298\t|\tloss: 0.51697\n",
      "Training Epoch 1  26.5% | batch:        79 of       298\t|\tloss: 0.419759\n",
      "Training Epoch 1  26.8% | batch:        80 of       298\t|\tloss: 0.466634\n",
      "Training Epoch 1  27.2% | batch:        81 of       298\t|\tloss: 0.479668\n",
      "Training Epoch 1  27.5% | batch:        82 of       298\t|\tloss: 0.375794\n",
      "Training Epoch 1  27.9% | batch:        83 of       298\t|\tloss: 0.476525\n",
      "Training Epoch 1  28.2% | batch:        84 of       298\t|\tloss: 0.356824\n",
      "Training Epoch 1  28.5% | batch:        85 of       298\t|\tloss: 0.33637\n",
      "Training Epoch 1  28.9% | batch:        86 of       298\t|\tloss: 0.382882\n",
      "Training Epoch 1  29.2% | batch:        87 of       298\t|\tloss: 0.301458\n",
      "Training Epoch 1  29.5% | batch:        88 of       298\t|\tloss: 0.514454\n",
      "Training Epoch 1  29.9% | batch:        89 of       298\t|\tloss: 0.347185\n",
      "Training Epoch 1  30.2% | batch:        90 of       298\t|\tloss: 0.52919\n",
      "Training Epoch 1  30.5% | batch:        91 of       298\t|\tloss: 0.336502\n",
      "Training Epoch 1  30.9% | batch:        92 of       298\t|\tloss: 0.378631\n",
      "Training Epoch 1  31.2% | batch:        93 of       298\t|\tloss: 0.31156\n",
      "Training Epoch 1  31.5% | batch:        94 of       298\t|\tloss: 0.408473\n",
      "Training Epoch 1  31.9% | batch:        95 of       298\t|\tloss: 0.39146\n",
      "Training Epoch 1  32.2% | batch:        96 of       298\t|\tloss: 0.51654\n",
      "Training Epoch 1  32.6% | batch:        97 of       298\t|\tloss: 0.380863\n",
      "Training Epoch 1  32.9% | batch:        98 of       298\t|\tloss: 0.361068\n",
      "Training Epoch 1  33.2% | batch:        99 of       298\t|\tloss: 0.414228\n",
      "Training Epoch 1  33.6% | batch:       100 of       298\t|\tloss: 1.65342\n",
      "Training Epoch 1  33.9% | batch:       101 of       298\t|\tloss: 0.421631\n",
      "Training Epoch 1  34.2% | batch:       102 of       298\t|\tloss: 0.53011\n",
      "Training Epoch 1  34.6% | batch:       103 of       298\t|\tloss: 0.483572\n",
      "Training Epoch 1  34.9% | batch:       104 of       298\t|\tloss: 0.337014\n",
      "Training Epoch 1  35.2% | batch:       105 of       298\t|\tloss: 0.404183\n",
      "Training Epoch 1  35.6% | batch:       106 of       298\t|\tloss: 0.519992\n",
      "Training Epoch 1  35.9% | batch:       107 of       298\t|\tloss: 0.39866\n",
      "Training Epoch 1  36.2% | batch:       108 of       298\t|\tloss: 0.281325\n",
      "Training Epoch 1  36.6% | batch:       109 of       298\t|\tloss: 0.39764\n",
      "Training Epoch 1  36.9% | batch:       110 of       298\t|\tloss: 0.278974\n",
      "Training Epoch 1  37.2% | batch:       111 of       298\t|\tloss: 0.334426\n",
      "Training Epoch 1  37.6% | batch:       112 of       298\t|\tloss: 0.479289\n",
      "Training Epoch 1  37.9% | batch:       113 of       298\t|\tloss: 0.356195\n",
      "Training Epoch 1  38.3% | batch:       114 of       298\t|\tloss: 1.90604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  38.6% | batch:       115 of       298\t|\tloss: 0.30958\n",
      "Training Epoch 1  38.9% | batch:       116 of       298\t|\tloss: 0.959631\n",
      "Training Epoch 1  39.3% | batch:       117 of       298\t|\tloss: 3.00887\n",
      "Training Epoch 1  39.6% | batch:       118 of       298\t|\tloss: 0.331467\n",
      "Training Epoch 1  39.9% | batch:       119 of       298\t|\tloss: 0.436543\n",
      "Training Epoch 1  40.3% | batch:       120 of       298\t|\tloss: 0.352147\n",
      "Training Epoch 1  40.6% | batch:       121 of       298\t|\tloss: 0.489487\n",
      "Training Epoch 1  40.9% | batch:       122 of       298\t|\tloss: 0.357692\n",
      "Training Epoch 1  41.3% | batch:       123 of       298\t|\tloss: 1.86368\n",
      "Training Epoch 1  41.6% | batch:       124 of       298\t|\tloss: 0.330332\n",
      "Training Epoch 1  41.9% | batch:       125 of       298\t|\tloss: 0.414822\n",
      "Training Epoch 1  42.3% | batch:       126 of       298\t|\tloss: 0.30075\n",
      "Training Epoch 1  42.6% | batch:       127 of       298\t|\tloss: 0.30904\n",
      "Training Epoch 1  43.0% | batch:       128 of       298\t|\tloss: 0.277526\n",
      "Training Epoch 1  43.3% | batch:       129 of       298\t|\tloss: 0.433496\n",
      "Training Epoch 1  43.6% | batch:       130 of       298\t|\tloss: 0.234069\n",
      "Training Epoch 1  44.0% | batch:       131 of       298\t|\tloss: 0.287267\n",
      "Training Epoch 1  44.3% | batch:       132 of       298\t|\tloss: 0.389442\n",
      "Training Epoch 1  44.6% | batch:       133 of       298\t|\tloss: 0.366306\n",
      "Training Epoch 1  45.0% | batch:       134 of       298\t|\tloss: 0.375599\n",
      "Training Epoch 1  45.3% | batch:       135 of       298\t|\tloss: 0.342934\n",
      "Training Epoch 1  45.6% | batch:       136 of       298\t|\tloss: 0.26892\n",
      "Training Epoch 1  46.0% | batch:       137 of       298\t|\tloss: 0.365408\n",
      "Training Epoch 1  46.3% | batch:       138 of       298\t|\tloss: 0.389636\n",
      "Training Epoch 1  46.6% | batch:       139 of       298\t|\tloss: 0.350144\n",
      "Training Epoch 1  47.0% | batch:       140 of       298\t|\tloss: 0.330477\n",
      "Training Epoch 1  47.3% | batch:       141 of       298\t|\tloss: 0.473314\n",
      "Training Epoch 1  47.7% | batch:       142 of       298\t|\tloss: 0.291356\n",
      "Training Epoch 1  48.0% | batch:       143 of       298\t|\tloss: 0.258341\n",
      "Training Epoch 1  48.3% | batch:       144 of       298\t|\tloss: 0.383334\n",
      "Training Epoch 1  48.7% | batch:       145 of       298\t|\tloss: 0.314547\n",
      "Training Epoch 1  49.0% | batch:       146 of       298\t|\tloss: 0.595107\n",
      "Training Epoch 1  49.3% | batch:       147 of       298\t|\tloss: 0.364587\n",
      "Training Epoch 1  49.7% | batch:       148 of       298\t|\tloss: 0.351187\n",
      "Training Epoch 1  50.0% | batch:       149 of       298\t|\tloss: 0.298397\n",
      "Training Epoch 1  50.3% | batch:       150 of       298\t|\tloss: 0.327928\n",
      "Training Epoch 1  50.7% | batch:       151 of       298\t|\tloss: 0.212632\n",
      "Training Epoch 1  51.0% | batch:       152 of       298\t|\tloss: 0.302142\n",
      "Training Epoch 1  51.3% | batch:       153 of       298\t|\tloss: 0.315741\n",
      "Training Epoch 1  51.7% | batch:       154 of       298\t|\tloss: 0.39828\n",
      "Training Epoch 1  52.0% | batch:       155 of       298\t|\tloss: 0.307444\n",
      "Training Epoch 1  52.3% | batch:       156 of       298\t|\tloss: 0.356845\n",
      "Training Epoch 1  52.7% | batch:       157 of       298\t|\tloss: 0.424158\n",
      "Training Epoch 1  53.0% | batch:       158 of       298\t|\tloss: 0.352053\n",
      "Training Epoch 1  53.4% | batch:       159 of       298\t|\tloss: 0.375224\n",
      "Training Epoch 1  53.7% | batch:       160 of       298\t|\tloss: 0.425554\n",
      "Training Epoch 1  54.0% | batch:       161 of       298\t|\tloss: 0.4191\n",
      "Training Epoch 1  54.4% | batch:       162 of       298\t|\tloss: 0.367839\n",
      "Training Epoch 1  54.7% | batch:       163 of       298\t|\tloss: 0.357576\n",
      "Training Epoch 1  55.0% | batch:       164 of       298\t|\tloss: 0.36371\n",
      "Training Epoch 1  55.4% | batch:       165 of       298\t|\tloss: 0.304607\n",
      "Training Epoch 1  55.7% | batch:       166 of       298\t|\tloss: 0.425207\n",
      "Training Epoch 1  56.0% | batch:       167 of       298\t|\tloss: 0.385167\n",
      "Training Epoch 1  56.4% | batch:       168 of       298\t|\tloss: 0.28337\n",
      "Training Epoch 1  56.7% | batch:       169 of       298\t|\tloss: 0.262325\n",
      "Training Epoch 1  57.0% | batch:       170 of       298\t|\tloss: 0.338157\n",
      "Training Epoch 1  57.4% | batch:       171 of       298\t|\tloss: 0.276173\n",
      "Training Epoch 1  57.7% | batch:       172 of       298\t|\tloss: 0.345462\n",
      "Training Epoch 1  58.1% | batch:       173 of       298\t|\tloss: 0.43915\n",
      "Training Epoch 1  58.4% | batch:       174 of       298\t|\tloss: 0.95307\n",
      "Training Epoch 1  58.7% | batch:       175 of       298\t|\tloss: 0.397036\n",
      "Training Epoch 1  59.1% | batch:       176 of       298\t|\tloss: 0.614557\n",
      "Training Epoch 1  59.4% | batch:       177 of       298\t|\tloss: 0.332466\n",
      "Training Epoch 1  59.7% | batch:       178 of       298\t|\tloss: 0.300188\n",
      "Training Epoch 1  60.1% | batch:       179 of       298\t|\tloss: 0.299442\n",
      "Training Epoch 1  60.4% | batch:       180 of       298\t|\tloss: 0.312011\n",
      "Training Epoch 1  60.7% | batch:       181 of       298\t|\tloss: 0.34944\n",
      "Training Epoch 1  61.1% | batch:       182 of       298\t|\tloss: 0.512053\n",
      "Training Epoch 1  61.4% | batch:       183 of       298\t|\tloss: 0.261756\n",
      "Training Epoch 1  61.7% | batch:       184 of       298\t|\tloss: 0.351137\n",
      "Training Epoch 1  62.1% | batch:       185 of       298\t|\tloss: 0.27543\n",
      "Training Epoch 1  62.4% | batch:       186 of       298\t|\tloss: 0.448455\n",
      "Training Epoch 1  62.8% | batch:       187 of       298\t|\tloss: 0.279559\n",
      "Training Epoch 1  63.1% | batch:       188 of       298\t|\tloss: 0.305296\n",
      "Training Epoch 1  63.4% | batch:       189 of       298\t|\tloss: 0.473704\n",
      "Training Epoch 1  63.8% | batch:       190 of       298\t|\tloss: 0.352391\n",
      "Training Epoch 1  64.1% | batch:       191 of       298\t|\tloss: 0.290001\n",
      "Training Epoch 1  64.4% | batch:       192 of       298\t|\tloss: 0.275503\n",
      "Training Epoch 1  64.8% | batch:       193 of       298\t|\tloss: 0.364496\n",
      "Training Epoch 1  65.1% | batch:       194 of       298\t|\tloss: 0.277127\n",
      "Training Epoch 1  65.4% | batch:       195 of       298\t|\tloss: 0.350069\n",
      "Training Epoch 1  65.8% | batch:       196 of       298\t|\tloss: 0.319318\n",
      "Training Epoch 1  66.1% | batch:       197 of       298\t|\tloss: 0.274768\n",
      "Training Epoch 1  66.4% | batch:       198 of       298\t|\tloss: 0.302356\n",
      "Training Epoch 1  66.8% | batch:       199 of       298\t|\tloss: 0.255837\n",
      "Training Epoch 1  67.1% | batch:       200 of       298\t|\tloss: 0.306421\n",
      "Training Epoch 1  67.4% | batch:       201 of       298\t|\tloss: 0.322331\n",
      "Training Epoch 1  67.8% | batch:       202 of       298\t|\tloss: 0.304128\n",
      "Training Epoch 1  68.1% | batch:       203 of       298\t|\tloss: 0.242415\n",
      "Training Epoch 1  68.5% | batch:       204 of       298\t|\tloss: 0.382137\n",
      "Training Epoch 1  68.8% | batch:       205 of       298\t|\tloss: 0.315794\n",
      "Training Epoch 1  69.1% | batch:       206 of       298\t|\tloss: 0.343595\n",
      "Training Epoch 1  69.5% | batch:       207 of       298\t|\tloss: 0.241368\n",
      "Training Epoch 1  69.8% | batch:       208 of       298\t|\tloss: 0.259132\n",
      "Training Epoch 1  70.1% | batch:       209 of       298\t|\tloss: 0.309073\n",
      "Training Epoch 1  70.5% | batch:       210 of       298\t|\tloss: 0.312602\n",
      "Training Epoch 1  70.8% | batch:       211 of       298\t|\tloss: 0.258058\n",
      "Training Epoch 1  71.1% | batch:       212 of       298\t|\tloss: 0.332921\n",
      "Training Epoch 1  71.5% | batch:       213 of       298\t|\tloss: 0.273165\n",
      "Training Epoch 1  71.8% | batch:       214 of       298\t|\tloss: 0.385924\n",
      "Training Epoch 1  72.1% | batch:       215 of       298\t|\tloss: 0.312264\n",
      "Training Epoch 1  72.5% | batch:       216 of       298\t|\tloss: 0.538402\n",
      "Training Epoch 1  72.8% | batch:       217 of       298\t|\tloss: 0.289243\n",
      "Training Epoch 1  73.2% | batch:       218 of       298\t|\tloss: 0.226555\n",
      "Training Epoch 1  73.5% | batch:       219 of       298\t|\tloss: 0.170596\n",
      "Training Epoch 1  73.8% | batch:       220 of       298\t|\tloss: 0.286134\n",
      "Training Epoch 1  74.2% | batch:       221 of       298\t|\tloss: 0.690297\n",
      "Training Epoch 1  74.5% | batch:       222 of       298\t|\tloss: 1.83067\n",
      "Training Epoch 1  74.8% | batch:       223 of       298\t|\tloss: 0.226402\n",
      "Training Epoch 1  75.2% | batch:       224 of       298\t|\tloss: 0.353527\n",
      "Training Epoch 1  75.5% | batch:       225 of       298\t|\tloss: 0.78011\n",
      "Training Epoch 1  75.8% | batch:       226 of       298\t|\tloss: 0.546154\n",
      "Training Epoch 1  76.2% | batch:       227 of       298\t|\tloss: 0.320772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  76.5% | batch:       228 of       298\t|\tloss: 0.421114\n",
      "Training Epoch 1  76.8% | batch:       229 of       298\t|\tloss: 0.29088\n",
      "Training Epoch 1  77.2% | batch:       230 of       298\t|\tloss: 0.318196\n",
      "Training Epoch 1  77.5% | batch:       231 of       298\t|\tloss: 0.375216\n",
      "Training Epoch 1  77.9% | batch:       232 of       298\t|\tloss: 0.269664\n",
      "Training Epoch 1  78.2% | batch:       233 of       298\t|\tloss: 0.292773\n",
      "Training Epoch 1  78.5% | batch:       234 of       298\t|\tloss: 0.340447\n",
      "Training Epoch 1  78.9% | batch:       235 of       298\t|\tloss: 0.323675\n",
      "Training Epoch 1  79.2% | batch:       236 of       298\t|\tloss: 0.248331\n",
      "Training Epoch 1  79.5% | batch:       237 of       298\t|\tloss: 0.305548\n",
      "Training Epoch 1  79.9% | batch:       238 of       298\t|\tloss: 0.21894\n",
      "Training Epoch 1  80.2% | batch:       239 of       298\t|\tloss: 0.378518\n",
      "Training Epoch 1  80.5% | batch:       240 of       298\t|\tloss: 0.218388\n",
      "Training Epoch 1  80.9% | batch:       241 of       298\t|\tloss: 0.316456\n",
      "Training Epoch 1  81.2% | batch:       242 of       298\t|\tloss: 0.2721\n",
      "Training Epoch 1  81.5% | batch:       243 of       298\t|\tloss: 0.259136\n",
      "Training Epoch 1  81.9% | batch:       244 of       298\t|\tloss: 0.430398\n",
      "Training Epoch 1  82.2% | batch:       245 of       298\t|\tloss: 0.262752\n",
      "Training Epoch 1  82.6% | batch:       246 of       298\t|\tloss: 0.309811\n",
      "Training Epoch 1  82.9% | batch:       247 of       298\t|\tloss: 2.17175\n",
      "Training Epoch 1  83.2% | batch:       248 of       298\t|\tloss: 0.43634\n",
      "Training Epoch 1  83.6% | batch:       249 of       298\t|\tloss: 0.3002\n",
      "Training Epoch 1  83.9% | batch:       250 of       298\t|\tloss: 0.312384\n",
      "Training Epoch 1  84.2% | batch:       251 of       298\t|\tloss: 0.270519\n",
      "Training Epoch 1  84.6% | batch:       252 of       298\t|\tloss: 0.25077\n",
      "Training Epoch 1  84.9% | batch:       253 of       298\t|\tloss: 0.192332\n",
      "Training Epoch 1  85.2% | batch:       254 of       298\t|\tloss: 0.31441\n",
      "Training Epoch 1  85.6% | batch:       255 of       298\t|\tloss: 0.330914\n",
      "Training Epoch 1  85.9% | batch:       256 of       298\t|\tloss: 0.2674\n",
      "Training Epoch 1  86.2% | batch:       257 of       298\t|\tloss: 0.394757\n",
      "Training Epoch 1  86.6% | batch:       258 of       298\t|\tloss: 0.528579\n",
      "Training Epoch 1  86.9% | batch:       259 of       298\t|\tloss: 0.308744\n",
      "Training Epoch 1  87.2% | batch:       260 of       298\t|\tloss: 0.215936\n",
      "Training Epoch 1  87.6% | batch:       261 of       298\t|\tloss: 0.222142\n",
      "Training Epoch 1  87.9% | batch:       262 of       298\t|\tloss: 0.264476\n",
      "Training Epoch 1  88.3% | batch:       263 of       298\t|\tloss: 0.260632\n",
      "Training Epoch 1  88.6% | batch:       264 of       298\t|\tloss: 0.316641\n",
      "Training Epoch 1  88.9% | batch:       265 of       298\t|\tloss: 0.262759\n",
      "Training Epoch 1  89.3% | batch:       266 of       298\t|\tloss: 0.264831\n",
      "Training Epoch 1  89.6% | batch:       267 of       298\t|\tloss: 0.273336\n",
      "Training Epoch 1  89.9% | batch:       268 of       298\t|\tloss: 0.332774\n",
      "Training Epoch 1  90.3% | batch:       269 of       298\t|\tloss: 0.334845\n",
      "Training Epoch 1  90.6% | batch:       270 of       298\t|\tloss: 0.334587\n",
      "Training Epoch 1  90.9% | batch:       271 of       298\t|\tloss: 0.216693\n",
      "Training Epoch 1  91.3% | batch:       272 of       298\t|\tloss: 0.31539\n",
      "Training Epoch 1  91.6% | batch:       273 of       298\t|\tloss: 0.242475\n",
      "Training Epoch 1  91.9% | batch:       274 of       298\t|\tloss: 0.687464\n",
      "Training Epoch 1  92.3% | batch:       275 of       298\t|\tloss: 0.282725\n",
      "Training Epoch 1  92.6% | batch:       276 of       298\t|\tloss: 0.258692\n",
      "Training Epoch 1  93.0% | batch:       277 of       298\t|\tloss: 0.263037\n",
      "Training Epoch 1  93.3% | batch:       278 of       298\t|\tloss: 0.246314\n",
      "Training Epoch 1  93.6% | batch:       279 of       298\t|\tloss: 0.33214\n",
      "Training Epoch 1  94.0% | batch:       280 of       298\t|\tloss: 0.404062\n",
      "Training Epoch 1  94.3% | batch:       281 of       298\t|\tloss: 0.301273\n",
      "Training Epoch 1  94.6% | batch:       282 of       298\t|\tloss: 0.476897\n",
      "Training Epoch 1  95.0% | batch:       283 of       298\t|\tloss: 0.366842\n",
      "Training Epoch 1  95.3% | batch:       284 of       298\t|\tloss: 0.269239\n",
      "Training Epoch 1  95.6% | batch:       285 of       298\t|\tloss: 0.343698\n",
      "Training Epoch 1  96.0% | batch:       286 of       298\t|\tloss: 0.258292\n",
      "Training Epoch 1  96.3% | batch:       287 of       298\t|\tloss: 0.497455\n",
      "Training Epoch 1  96.6% | batch:       288 of       298\t|\tloss: 0.42838\n",
      "Training Epoch 1  97.0% | batch:       289 of       298\t|\tloss: 0.32491\n",
      "Training Epoch 1  97.3% | batch:       290 of       298\t|\tloss: 0.328941\n",
      "Training Epoch 1  97.7% | batch:       291 of       298\t|\tloss: 0.32112\n",
      "Training Epoch 1  98.0% | batch:       292 of       298\t|\tloss: 0.193848\n",
      "Training Epoch 1  98.3% | batch:       293 of       298\t|\tloss: 0.321721\n",
      "Training Epoch 1  98.7% | batch:       294 of       298\t|\tloss: 0.47626\n",
      "Training Epoch 1  99.0% | batch:       295 of       298\t|\tloss: 0.275947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:36:13,661 | INFO : Epoch 1 Training Summary: epoch: 1.000000 | loss: 0.487118 | \n",
      "2023-06-22 14:36:13,663 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.202457189559937 seconds\n",
      "\n",
      "2023-06-22 14:36:13,663 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.202457189559937 seconds\n",
      "2023-06-22 14:36:13,664 | INFO : Avg batch train. time: 0.040947842917986366 seconds\n",
      "2023-06-22 14:36:13,666 | INFO : Avg sample train. time: 0.0012798885241829176 seconds\n",
      "2023-06-22 14:36:13,667 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1  99.3% | batch:       296 of       298\t|\tloss: 0.391188\n",
      "Training Epoch 1  99.7% | batch:       297 of       298\t|\tloss: 0.315896\n",
      "\n",
      "Evaluating Epoch 1   0.0% | batch:         0 of        75\t|\tloss: 0.75289\n",
      "Evaluating Epoch 1   1.3% | batch:         1 of        75\t|\tloss: 0.18148\n",
      "Evaluating Epoch 1   2.7% | batch:         2 of        75\t|\tloss: 0.178394\n",
      "Evaluating Epoch 1   4.0% | batch:         3 of        75\t|\tloss: 0.196378\n",
      "Evaluating Epoch 1   5.3% | batch:         4 of        75\t|\tloss: 1.515\n",
      "Evaluating Epoch 1   6.7% | batch:         5 of        75\t|\tloss: 0.219015\n",
      "Evaluating Epoch 1   8.0% | batch:         6 of        75\t|\tloss: 0.33412\n",
      "Evaluating Epoch 1   9.3% | batch:         7 of        75\t|\tloss: 0.184845\n",
      "Evaluating Epoch 1  10.7% | batch:         8 of        75\t|\tloss: 0.238412\n",
      "Evaluating Epoch 1  12.0% | batch:         9 of        75\t|\tloss: 0.363527\n",
      "Evaluating Epoch 1  13.3% | batch:        10 of        75\t|\tloss: 0.179013\n",
      "Evaluating Epoch 1  14.7% | batch:        11 of        75\t|\tloss: 0.198815\n",
      "Evaluating Epoch 1  16.0% | batch:        12 of        75\t|\tloss: 1.19281\n",
      "Evaluating Epoch 1  17.3% | batch:        13 of        75\t|\tloss: 0.216877\n",
      "Evaluating Epoch 1  18.7% | batch:        14 of        75\t|\tloss: 0.278713\n",
      "Evaluating Epoch 1  20.0% | batch:        15 of        75\t|\tloss: 0.319973\n",
      "Evaluating Epoch 1  21.3% | batch:        16 of        75\t|\tloss: 0.186415\n",
      "Evaluating Epoch 1  22.7% | batch:        17 of        75\t|\tloss: 0.367031\n",
      "Evaluating Epoch 1  24.0% | batch:        18 of        75\t|\tloss: 0.234141\n",
      "Evaluating Epoch 1  25.3% | batch:        19 of        75\t|\tloss: 0.612539\n",
      "Evaluating Epoch 1  26.7% | batch:        20 of        75\t|\tloss: 0.233752\n",
      "Evaluating Epoch 1  28.0% | batch:        21 of        75\t|\tloss: 0.251015\n",
      "Evaluating Epoch 1  29.3% | batch:        22 of        75\t|\tloss: 0.212771\n",
      "Evaluating Epoch 1  30.7% | batch:        23 of        75\t|\tloss: 1.45187\n",
      "Evaluating Epoch 1  32.0% | batch:        24 of        75\t|\tloss: 0.316327\n",
      "Evaluating Epoch 1  33.3% | batch:        25 of        75\t|\tloss: 0.370473\n",
      "Evaluating Epoch 1  34.7% | batch:        26 of        75\t|\tloss: 0.191982\n",
      "Evaluating Epoch 1  36.0% | batch:        27 of        75\t|\tloss: 0.249597\n",
      "Evaluating Epoch 1  37.3% | batch:        28 of        75\t|\tloss: 0.191542\n",
      "Evaluating Epoch 1  38.7% | batch:        29 of        75\t|\tloss: 0.268738\n",
      "Evaluating Epoch 1  40.0% | batch:        30 of        75\t|\tloss: 0.512847\n",
      "Evaluating Epoch 1  41.3% | batch:        31 of        75\t|\tloss: 0.26694\n",
      "Evaluating Epoch 1  42.7% | batch:        32 of        75\t|\tloss: 0.30825\n",
      "Evaluating Epoch 1  44.0% | batch:        33 of        75\t|\tloss: 0.38843\n",
      "Evaluating Epoch 1  45.3% | batch:        34 of        75\t|\tloss: 0.305839\n",
      "Evaluating Epoch 1  46.7% | batch:        35 of        75\t|\tloss: 0.532895\n",
      "Evaluating Epoch 1  48.0% | batch:        36 of        75\t|\tloss: 0.176102\n",
      "Evaluating Epoch 1  49.3% | batch:        37 of        75\t|\tloss: 0.150998\n",
      "Evaluating Epoch 1  50.7% | batch:        38 of        75\t|\tloss: 0.37457\n",
      "Evaluating Epoch 1  52.0% | batch:        39 of        75\t|\tloss: 0.198284\n",
      "Evaluating Epoch 1  53.3% | batch:        40 of        75\t|\tloss: 0.250638\n",
      "Evaluating Epoch 1  54.7% | batch:        41 of        75\t|\tloss: 0.158435\n",
      "Evaluating Epoch 1  56.0% | batch:        42 of        75\t|\tloss: 1.04658\n",
      "Evaluating Epoch 1  57.3% | batch:        43 of        75\t|\tloss: 0.205179\n",
      "Evaluating Epoch 1  58.7% | batch:        44 of        75\t|\tloss: 0.235223\n",
      "Evaluating Epoch 1  60.0% | batch:        45 of        75\t|\tloss: 0.404332\n",
      "Evaluating Epoch 1  61.3% | batch:        46 of        75\t|\tloss: 0.288695\n",
      "Evaluating Epoch 1  62.7% | batch:        47 of        75\t|\tloss: 0.17142\n",
      "Evaluating Epoch 1  64.0% | batch:        48 of        75\t|\tloss: 0.215252\n",
      "Evaluating Epoch 1  65.3% | batch:        49 of        75\t|\tloss: 0.204942\n",
      "Evaluating Epoch 1  66.7% | batch:        50 of        75\t|\tloss: 0.208597\n",
      "Evaluating Epoch 1  68.0% | batch:        51 of        75\t|\tloss: 0.241109\n",
      "Evaluating Epoch 1  69.3% | batch:        52 of        75\t|\tloss: 0.223996\n",
      "Evaluating Epoch 1  70.7% | batch:        53 of        75\t|\tloss: 0.173447\n",
      "Evaluating Epoch 1  72.0% | batch:        54 of        75\t|\tloss: 0.358665\n",
      "Evaluating Epoch 1  73.3% | batch:        55 of        75\t|\tloss: 0.194795\n",
      "Evaluating Epoch 1  74.7% | batch:        56 of        75\t|\tloss: 0.221131\n",
      "Evaluating Epoch 1  76.0% | batch:        57 of        75\t|\tloss: 0.242206\n",
      "Evaluating Epoch 1  77.3% | batch:        58 of        75\t|\tloss: 0.230116\n",
      "Evaluating Epoch 1  78.7% | batch:        59 of        75\t|\tloss: 0.244741\n",
      "Evaluating Epoch 1  80.0% | batch:        60 of        75\t|\tloss: 0.284176\n",
      "Evaluating Epoch 1  81.3% | batch:        61 of        75\t|\tloss: 0.171486\n",
      "Evaluating Epoch 1  82.7% | batch:        62 of        75\t|\tloss: 0.220143\n",
      "Evaluating Epoch 1  84.0% | batch:        63 of        75\t|\tloss: 0.209979\n",
      "Evaluating Epoch 1  85.3% | batch:        64 of        75\t|\tloss: 0.327759\n",
      "Evaluating Epoch 1  86.7% | batch:        65 of        75\t|\tloss: 0.164828\n",
      "Evaluating Epoch 1  88.0% | batch:        66 of        75\t|\tloss: 0.222479\n",
      "Evaluating Epoch 1  89.3% | batch:        67 of        75\t|\tloss: 0.205376\n",
      "Evaluating Epoch 1  90.7% | batch:        68 of        75\t|\tloss: 0.241877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:36:14,964 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.2962024211883545 seconds\n",
      "\n",
      "2023-06-22 14:36:14,965 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.2340729236602783 seconds\n",
      "2023-06-22 14:36:14,966 | INFO : Avg batch val. time: 0.01645430564880371 seconds\n",
      "2023-06-22 14:36:14,966 | INFO : Avg sample val. time: 0.0005176480384481033 seconds\n",
      "2023-06-22 14:36:14,967 | INFO : Epoch 1 Validation Summary: epoch: 1.000000 | loss: 0.327433 | \n",
      "Training Epoch:   0%|          | 1/400 [00:13<1:30:08, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 1  92.0% | batch:        69 of        75\t|\tloss: 0.220971\n",
      "Evaluating Epoch 1  93.3% | batch:        70 of        75\t|\tloss: 0.379347\n",
      "Evaluating Epoch 1  94.7% | batch:        71 of        75\t|\tloss: 0.237577\n",
      "Evaluating Epoch 1  96.0% | batch:        72 of        75\t|\tloss: 0.188008\n",
      "Evaluating Epoch 1  97.3% | batch:        73 of        75\t|\tloss: 0.204049\n",
      "Evaluating Epoch 1  98.7% | batch:        74 of        75\t|\tloss: 0.894837\n",
      "\n",
      "Training Epoch 2   0.0% | batch:         0 of       298\t|\tloss: 0.268593\n",
      "Training Epoch 2   0.3% | batch:         1 of       298\t|\tloss: 1.51081\n",
      "Training Epoch 2   0.7% | batch:         2 of       298\t|\tloss: 0.634883\n",
      "Training Epoch 2   1.0% | batch:         3 of       298\t|\tloss: 0.258298\n",
      "Training Epoch 2   1.3% | batch:         4 of       298\t|\tloss: 1.02152\n",
      "Training Epoch 2   1.7% | batch:         5 of       298\t|\tloss: 0.341819\n",
      "Training Epoch 2   2.0% | batch:         6 of       298\t|\tloss: 2.11456\n",
      "Training Epoch 2   2.3% | batch:         7 of       298\t|\tloss: 0.32253\n",
      "Training Epoch 2   2.7% | batch:         8 of       298\t|\tloss: 0.315101\n",
      "Training Epoch 2   3.0% | batch:         9 of       298\t|\tloss: 0.205492\n",
      "Training Epoch 2   3.4% | batch:        10 of       298\t|\tloss: 0.280208\n",
      "Training Epoch 2   3.7% | batch:        11 of       298\t|\tloss: 0.213923\n",
      "Training Epoch 2   4.0% | batch:        12 of       298\t|\tloss: 0.328069\n",
      "Training Epoch 2   4.4% | batch:        13 of       298\t|\tloss: 0.364706\n",
      "Training Epoch 2   4.7% | batch:        14 of       298\t|\tloss: 0.484462\n",
      "Training Epoch 2   5.0% | batch:        15 of       298\t|\tloss: 0.363066\n",
      "Training Epoch 2   5.4% | batch:        16 of       298\t|\tloss: 0.327478\n",
      "Training Epoch 2   5.7% | batch:        17 of       298\t|\tloss: 1.06922\n",
      "Training Epoch 2   6.0% | batch:        18 of       298\t|\tloss: 0.296118\n",
      "Training Epoch 2   6.4% | batch:        19 of       298\t|\tloss: 0.235663\n",
      "Training Epoch 2   6.7% | batch:        20 of       298\t|\tloss: 0.275028\n",
      "Training Epoch 2   7.0% | batch:        21 of       298\t|\tloss: 0.27592\n",
      "Training Epoch 2   7.4% | batch:        22 of       298\t|\tloss: 0.32071\n",
      "Training Epoch 2   7.7% | batch:        23 of       298\t|\tloss: 0.273944\n",
      "Training Epoch 2   8.1% | batch:        24 of       298\t|\tloss: 0.551686\n",
      "Training Epoch 2   8.4% | batch:        25 of       298\t|\tloss: 0.495104\n",
      "Training Epoch 2   8.7% | batch:        26 of       298\t|\tloss: 0.294851\n",
      "Training Epoch 2   9.1% | batch:        27 of       298\t|\tloss: 0.348824\n",
      "Training Epoch 2   9.4% | batch:        28 of       298\t|\tloss: 0.31394\n",
      "Training Epoch 2   9.7% | batch:        29 of       298\t|\tloss: 0.222335\n",
      "Training Epoch 2  10.1% | batch:        30 of       298\t|\tloss: 0.26203\n",
      "Training Epoch 2  10.4% | batch:        31 of       298\t|\tloss: 0.306923\n",
      "Training Epoch 2  10.7% | batch:        32 of       298\t|\tloss: 0.259342\n",
      "Training Epoch 2  11.1% | batch:        33 of       298\t|\tloss: 0.298425\n",
      "Training Epoch 2  11.4% | batch:        34 of       298\t|\tloss: 0.250998\n",
      "Training Epoch 2  11.7% | batch:        35 of       298\t|\tloss: 0.57603\n",
      "Training Epoch 2  12.1% | batch:        36 of       298\t|\tloss: 0.293762\n",
      "Training Epoch 2  12.4% | batch:        37 of       298\t|\tloss: 0.359795\n",
      "Training Epoch 2  12.8% | batch:        38 of       298\t|\tloss: 0.239402\n",
      "Training Epoch 2  13.1% | batch:        39 of       298\t|\tloss: 0.234314\n",
      "Training Epoch 2  13.4% | batch:        40 of       298\t|\tloss: 0.263154\n",
      "Training Epoch 2  13.8% | batch:        41 of       298\t|\tloss: 0.369317\n",
      "Training Epoch 2  14.1% | batch:        42 of       298\t|\tloss: 0.30386\n",
      "Training Epoch 2  14.4% | batch:        43 of       298\t|\tloss: 0.508162\n",
      "Training Epoch 2  14.8% | batch:        44 of       298\t|\tloss: 0.224378\n",
      "Training Epoch 2  15.1% | batch:        45 of       298\t|\tloss: 0.252583\n",
      "Training Epoch 2  15.4% | batch:        46 of       298\t|\tloss: 0.424861\n",
      "Training Epoch 2  15.8% | batch:        47 of       298\t|\tloss: 0.274305\n",
      "Training Epoch 2  16.1% | batch:        48 of       298\t|\tloss: 0.245659\n",
      "Training Epoch 2  16.4% | batch:        49 of       298\t|\tloss: 0.285561\n",
      "Training Epoch 2  16.8% | batch:        50 of       298\t|\tloss: 0.316265\n",
      "Training Epoch 2  17.1% | batch:        51 of       298\t|\tloss: 0.544343\n",
      "Training Epoch 2  17.4% | batch:        52 of       298\t|\tloss: 0.260851\n",
      "Training Epoch 2  17.8% | batch:        53 of       298\t|\tloss: 0.267824\n",
      "Training Epoch 2  18.1% | batch:        54 of       298\t|\tloss: 1.22586\n",
      "Training Epoch 2  18.5% | batch:        55 of       298\t|\tloss: 1.90406\n",
      "Training Epoch 2  18.8% | batch:        56 of       298\t|\tloss: 0.249483\n",
      "Training Epoch 2  19.1% | batch:        57 of       298\t|\tloss: 0.274693\n",
      "Training Epoch 2  19.5% | batch:        58 of       298\t|\tloss: 0.252116\n",
      "Training Epoch 2  19.8% | batch:        59 of       298\t|\tloss: 0.244049\n",
      "Training Epoch 2  20.1% | batch:        60 of       298\t|\tloss: 0.456879\n",
      "Training Epoch 2  20.5% | batch:        61 of       298\t|\tloss: 0.284443\n",
      "Training Epoch 2  20.8% | batch:        62 of       298\t|\tloss: 0.336111\n",
      "Training Epoch 2  21.1% | batch:        63 of       298\t|\tloss: 0.266701\n",
      "Training Epoch 2  21.5% | batch:        64 of       298\t|\tloss: 0.30314\n",
      "Training Epoch 2  21.8% | batch:        65 of       298\t|\tloss: 0.20581\n",
      "Training Epoch 2  22.1% | batch:        66 of       298\t|\tloss: 0.394821\n",
      "Training Epoch 2  22.5% | batch:        67 of       298\t|\tloss: 0.244535\n",
      "Training Epoch 2  22.8% | batch:        68 of       298\t|\tloss: 0.469179\n",
      "Training Epoch 2  23.2% | batch:        69 of       298\t|\tloss: 0.429516\n",
      "Training Epoch 2  23.5% | batch:        70 of       298\t|\tloss: 0.500084\n",
      "Training Epoch 2  23.8% | batch:        71 of       298\t|\tloss: 0.168839\n",
      "Training Epoch 2  24.2% | batch:        72 of       298\t|\tloss: 0.323187\n",
      "Training Epoch 2  24.5% | batch:        73 of       298\t|\tloss: 0.288327\n",
      "Training Epoch 2  24.8% | batch:        74 of       298\t|\tloss: 0.179316\n",
      "Training Epoch 2  25.2% | batch:        75 of       298\t|\tloss: 0.29849\n",
      "Training Epoch 2  25.5% | batch:        76 of       298\t|\tloss: 0.327099\n",
      "Training Epoch 2  25.8% | batch:        77 of       298\t|\tloss: 0.346473\n",
      "Training Epoch 2  26.2% | batch:        78 of       298\t|\tloss: 0.309941\n",
      "Training Epoch 2  26.5% | batch:        79 of       298\t|\tloss: 0.267953\n",
      "Training Epoch 2  26.8% | batch:        80 of       298\t|\tloss: 0.330151\n",
      "Training Epoch 2  27.2% | batch:        81 of       298\t|\tloss: 0.395316\n",
      "Training Epoch 2  27.5% | batch:        82 of       298\t|\tloss: 0.248106\n",
      "Training Epoch 2  27.9% | batch:        83 of       298\t|\tloss: 0.265654\n",
      "Training Epoch 2  28.2% | batch:        84 of       298\t|\tloss: 0.288528\n",
      "Training Epoch 2  28.5% | batch:        85 of       298\t|\tloss: 0.302446\n",
      "Training Epoch 2  28.9% | batch:        86 of       298\t|\tloss: 0.31973\n",
      "Training Epoch 2  29.2% | batch:        87 of       298\t|\tloss: 0.233583\n",
      "Training Epoch 2  29.5% | batch:        88 of       298\t|\tloss: 0.221181\n",
      "Training Epoch 2  29.9% | batch:        89 of       298\t|\tloss: 0.322961\n",
      "Training Epoch 2  30.2% | batch:        90 of       298\t|\tloss: 0.321355\n",
      "Training Epoch 2  30.5% | batch:        91 of       298\t|\tloss: 0.215923\n",
      "Training Epoch 2  30.9% | batch:        92 of       298\t|\tloss: 0.256868\n",
      "Training Epoch 2  31.2% | batch:        93 of       298\t|\tloss: 0.299412\n",
      "Training Epoch 2  31.5% | batch:        94 of       298\t|\tloss: 0.197533\n",
      "Training Epoch 2  31.9% | batch:        95 of       298\t|\tloss: 0.300156\n",
      "Training Epoch 2  32.2% | batch:        96 of       298\t|\tloss: 0.412913\n",
      "Training Epoch 2  32.6% | batch:        97 of       298\t|\tloss: 0.186791\n",
      "Training Epoch 2  32.9% | batch:        98 of       298\t|\tloss: 0.330971\n",
      "Training Epoch 2  33.2% | batch:        99 of       298\t|\tloss: 0.424005\n",
      "Training Epoch 2  33.6% | batch:       100 of       298\t|\tloss: 0.287528\n",
      "Training Epoch 2  33.9% | batch:       101 of       298\t|\tloss: 0.24222\n",
      "Training Epoch 2  34.2% | batch:       102 of       298\t|\tloss: 0.371498\n",
      "Training Epoch 2  34.6% | batch:       103 of       298\t|\tloss: 0.372849\n",
      "Training Epoch 2  34.9% | batch:       104 of       298\t|\tloss: 0.264978\n",
      "Training Epoch 2  35.2% | batch:       105 of       298\t|\tloss: 0.267125\n",
      "Training Epoch 2  35.6% | batch:       106 of       298\t|\tloss: 0.22714\n",
      "Training Epoch 2  35.9% | batch:       107 of       298\t|\tloss: 0.323825\n",
      "Training Epoch 2  36.2% | batch:       108 of       298\t|\tloss: 0.246789\n",
      "Training Epoch 2  36.6% | batch:       109 of       298\t|\tloss: 4.01104\n",
      "Training Epoch 2  36.9% | batch:       110 of       298\t|\tloss: 0.234714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  37.2% | batch:       111 of       298\t|\tloss: 0.24864\n",
      "Training Epoch 2  37.6% | batch:       112 of       298\t|\tloss: 0.419456\n",
      "Training Epoch 2  37.9% | batch:       113 of       298\t|\tloss: 0.417861\n",
      "Training Epoch 2  38.3% | batch:       114 of       298\t|\tloss: 1.21881\n",
      "Training Epoch 2  38.6% | batch:       115 of       298\t|\tloss: 0.25325\n",
      "Training Epoch 2  38.9% | batch:       116 of       298\t|\tloss: 0.250232\n",
      "Training Epoch 2  39.3% | batch:       117 of       298\t|\tloss: 0.348275\n",
      "Training Epoch 2  39.6% | batch:       118 of       298\t|\tloss: 0.556507\n",
      "Training Epoch 2  39.9% | batch:       119 of       298\t|\tloss: 0.252694\n",
      "Training Epoch 2  40.3% | batch:       120 of       298\t|\tloss: 0.244956\n",
      "Training Epoch 2  40.6% | batch:       121 of       298\t|\tloss: 1.18818\n",
      "Training Epoch 2  40.9% | batch:       122 of       298\t|\tloss: 0.262909\n",
      "Training Epoch 2  41.3% | batch:       123 of       298\t|\tloss: 0.320965\n",
      "Training Epoch 2  41.6% | batch:       124 of       298\t|\tloss: 0.258655\n",
      "Training Epoch 2  41.9% | batch:       125 of       298\t|\tloss: 0.270988\n",
      "Training Epoch 2  42.3% | batch:       126 of       298\t|\tloss: 0.285062\n",
      "Training Epoch 2  42.6% | batch:       127 of       298\t|\tloss: 0.580557\n",
      "Training Epoch 2  43.0% | batch:       128 of       298\t|\tloss: 0.269244\n",
      "Training Epoch 2  43.3% | batch:       129 of       298\t|\tloss: 0.279527\n",
      "Training Epoch 2  43.6% | batch:       130 of       298\t|\tloss: 0.224006\n",
      "Training Epoch 2  44.0% | batch:       131 of       298\t|\tloss: 8.05685\n",
      "Training Epoch 2  44.3% | batch:       132 of       298\t|\tloss: 0.358893\n",
      "Training Epoch 2  44.6% | batch:       133 of       298\t|\tloss: 0.24388\n",
      "Training Epoch 2  45.0% | batch:       134 of       298\t|\tloss: 0.215923\n",
      "Training Epoch 2  45.3% | batch:       135 of       298\t|\tloss: 0.361858\n",
      "Training Epoch 2  45.6% | batch:       136 of       298\t|\tloss: 0.219481\n",
      "Training Epoch 2  46.0% | batch:       137 of       298\t|\tloss: 0.250883\n",
      "Training Epoch 2  46.3% | batch:       138 of       298\t|\tloss: 0.305779\n",
      "Training Epoch 2  46.6% | batch:       139 of       298\t|\tloss: 0.218759\n",
      "Training Epoch 2  47.0% | batch:       140 of       298\t|\tloss: 0.33525\n",
      "Training Epoch 2  47.3% | batch:       141 of       298\t|\tloss: 0.257437\n",
      "Training Epoch 2  47.7% | batch:       142 of       298\t|\tloss: 0.425781\n",
      "Training Epoch 2  48.0% | batch:       143 of       298\t|\tloss: 0.275857\n",
      "Training Epoch 2  48.3% | batch:       144 of       298\t|\tloss: 0.32263\n",
      "Training Epoch 2  48.7% | batch:       145 of       298\t|\tloss: 0.216694\n",
      "Training Epoch 2  49.0% | batch:       146 of       298\t|\tloss: 0.27432\n",
      "Training Epoch 2  49.3% | batch:       147 of       298\t|\tloss: 0.289926\n",
      "Training Epoch 2  49.7% | batch:       148 of       298\t|\tloss: 0.319585\n",
      "Training Epoch 2  50.0% | batch:       149 of       298\t|\tloss: 0.293969\n",
      "Training Epoch 2  50.3% | batch:       150 of       298\t|\tloss: 0.167275\n",
      "Training Epoch 2  50.7% | batch:       151 of       298\t|\tloss: 0.4651\n",
      "Training Epoch 2  51.0% | batch:       152 of       298\t|\tloss: 0.474631\n",
      "Training Epoch 2  51.3% | batch:       153 of       298\t|\tloss: 2.19615\n",
      "Training Epoch 2  51.7% | batch:       154 of       298\t|\tloss: 0.29092\n",
      "Training Epoch 2  52.0% | batch:       155 of       298\t|\tloss: 0.360058\n",
      "Training Epoch 2  52.3% | batch:       156 of       298\t|\tloss: 1.74369\n",
      "Training Epoch 2  52.7% | batch:       157 of       298\t|\tloss: 0.278023\n",
      "Training Epoch 2  53.0% | batch:       158 of       298\t|\tloss: 0.234122\n",
      "Training Epoch 2  53.4% | batch:       159 of       298\t|\tloss: 0.437889\n",
      "Training Epoch 2  53.7% | batch:       160 of       298\t|\tloss: 0.278361\n",
      "Training Epoch 2  54.0% | batch:       161 of       298\t|\tloss: 0.214236\n",
      "Training Epoch 2  54.4% | batch:       162 of       298\t|\tloss: 0.237167\n",
      "Training Epoch 2  54.7% | batch:       163 of       298\t|\tloss: 0.219197\n",
      "Training Epoch 2  55.0% | batch:       164 of       298\t|\tloss: 0.237754\n",
      "Training Epoch 2  55.4% | batch:       165 of       298\t|\tloss: 0.300092\n",
      "Training Epoch 2  55.7% | batch:       166 of       298\t|\tloss: 0.286139\n",
      "Training Epoch 2  56.0% | batch:       167 of       298\t|\tloss: 0.261055\n",
      "Training Epoch 2  56.4% | batch:       168 of       298\t|\tloss: 0.408329\n",
      "Training Epoch 2  56.7% | batch:       169 of       298\t|\tloss: 0.335226\n",
      "Training Epoch 2  57.0% | batch:       170 of       298\t|\tloss: 2.48865\n",
      "Training Epoch 2  57.4% | batch:       171 of       298\t|\tloss: 0.270687\n",
      "Training Epoch 2  57.7% | batch:       172 of       298\t|\tloss: 0.321206\n",
      "Training Epoch 2  58.1% | batch:       173 of       298\t|\tloss: 0.24674\n",
      "Training Epoch 2  58.4% | batch:       174 of       298\t|\tloss: 0.728836\n",
      "Training Epoch 2  58.7% | batch:       175 of       298\t|\tloss: 0.229092\n",
      "Training Epoch 2  59.1% | batch:       176 of       298\t|\tloss: 0.526153\n",
      "Training Epoch 2  59.4% | batch:       177 of       298\t|\tloss: 0.325842\n",
      "Training Epoch 2  59.7% | batch:       178 of       298\t|\tloss: 0.260765\n",
      "Training Epoch 2  60.1% | batch:       179 of       298\t|\tloss: 0.215963\n",
      "Training Epoch 2  60.4% | batch:       180 of       298\t|\tloss: 0.298361\n",
      "Training Epoch 2  60.7% | batch:       181 of       298\t|\tloss: 0.304528\n",
      "Training Epoch 2  61.1% | batch:       182 of       298\t|\tloss: 0.382866\n",
      "Training Epoch 2  61.4% | batch:       183 of       298\t|\tloss: 0.223695\n",
      "Training Epoch 2  61.7% | batch:       184 of       298\t|\tloss: 0.245848\n",
      "Training Epoch 2  62.1% | batch:       185 of       298\t|\tloss: 0.279634\n",
      "Training Epoch 2  62.4% | batch:       186 of       298\t|\tloss: 0.351192\n",
      "Training Epoch 2  62.8% | batch:       187 of       298\t|\tloss: 0.244043\n",
      "Training Epoch 2  63.1% | batch:       188 of       298\t|\tloss: 0.267696\n",
      "Training Epoch 2  63.4% | batch:       189 of       298\t|\tloss: 0.210093\n",
      "Training Epoch 2  63.8% | batch:       190 of       298\t|\tloss: 0.35009\n",
      "Training Epoch 2  64.1% | batch:       191 of       298\t|\tloss: 0.298896\n",
      "Training Epoch 2  64.4% | batch:       192 of       298\t|\tloss: 0.200807\n",
      "Training Epoch 2  64.8% | batch:       193 of       298\t|\tloss: 0.207949\n",
      "Training Epoch 2  65.1% | batch:       194 of       298\t|\tloss: 0.345689\n",
      "Training Epoch 2  65.4% | batch:       195 of       298\t|\tloss: 0.242922\n",
      "Training Epoch 2  65.8% | batch:       196 of       298\t|\tloss: 0.236602\n",
      "Training Epoch 2  66.1% | batch:       197 of       298\t|\tloss: 0.486277\n",
      "Training Epoch 2  66.4% | batch:       198 of       298\t|\tloss: 0.668135\n",
      "Training Epoch 2  66.8% | batch:       199 of       298\t|\tloss: 2.19836\n",
      "Training Epoch 2  67.1% | batch:       200 of       298\t|\tloss: 0.389724\n",
      "Training Epoch 2  67.4% | batch:       201 of       298\t|\tloss: 0.364971\n",
      "Training Epoch 2  67.8% | batch:       202 of       298\t|\tloss: 0.208421\n",
      "Training Epoch 2  68.1% | batch:       203 of       298\t|\tloss: 0.199995\n",
      "Training Epoch 2  68.5% | batch:       204 of       298\t|\tloss: 0.25963\n",
      "Training Epoch 2  68.8% | batch:       205 of       298\t|\tloss: 0.23882\n",
      "Training Epoch 2  69.1% | batch:       206 of       298\t|\tloss: 0.268178\n",
      "Training Epoch 2  69.5% | batch:       207 of       298\t|\tloss: 0.297416\n",
      "Training Epoch 2  69.8% | batch:       208 of       298\t|\tloss: 0.192575\n",
      "Training Epoch 2  70.1% | batch:       209 of       298\t|\tloss: 0.258229\n",
      "Training Epoch 2  70.5% | batch:       210 of       298\t|\tloss: 0.604199\n",
      "Training Epoch 2  70.8% | batch:       211 of       298\t|\tloss: 0.268631\n",
      "Training Epoch 2  71.1% | batch:       212 of       298\t|\tloss: 0.269922\n",
      "Training Epoch 2  71.5% | batch:       213 of       298\t|\tloss: 0.244213\n",
      "Training Epoch 2  71.8% | batch:       214 of       298\t|\tloss: 0.205811\n",
      "Training Epoch 2  72.1% | batch:       215 of       298\t|\tloss: 0.313898\n",
      "Training Epoch 2  72.5% | batch:       216 of       298\t|\tloss: 0.266525\n",
      "Training Epoch 2  72.8% | batch:       217 of       298\t|\tloss: 0.559561\n",
      "Training Epoch 2  73.2% | batch:       218 of       298\t|\tloss: 0.16545\n",
      "Training Epoch 2  73.5% | batch:       219 of       298\t|\tloss: 0.228297\n",
      "Training Epoch 2  73.8% | batch:       220 of       298\t|\tloss: 0.239342\n",
      "Training Epoch 2  74.2% | batch:       221 of       298\t|\tloss: 0.384655\n",
      "Training Epoch 2  74.5% | batch:       222 of       298\t|\tloss: 0.266601\n",
      "Training Epoch 2  74.8% | batch:       223 of       298\t|\tloss: 2.22332\n",
      "Training Epoch 2  75.2% | batch:       224 of       298\t|\tloss: 0.360277\n",
      "Training Epoch 2  75.5% | batch:       225 of       298\t|\tloss: 0.207443\n",
      "Training Epoch 2  75.8% | batch:       226 of       298\t|\tloss: 0.295087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  76.2% | batch:       227 of       298\t|\tloss: 0.275449\n",
      "Training Epoch 2  76.5% | batch:       228 of       298\t|\tloss: 0.209561\n",
      "Training Epoch 2  76.8% | batch:       229 of       298\t|\tloss: 0.331591\n",
      "Training Epoch 2  77.2% | batch:       230 of       298\t|\tloss: 0.253285\n",
      "Training Epoch 2  77.5% | batch:       231 of       298\t|\tloss: 0.265752\n",
      "Training Epoch 2  77.9% | batch:       232 of       298\t|\tloss: 0.235184\n",
      "Training Epoch 2  78.2% | batch:       233 of       298\t|\tloss: 0.205687\n",
      "Training Epoch 2  78.5% | batch:       234 of       298\t|\tloss: 0.493829\n",
      "Training Epoch 2  78.9% | batch:       235 of       298\t|\tloss: 0.234795\n",
      "Training Epoch 2  79.2% | batch:       236 of       298\t|\tloss: 0.264788\n",
      "Training Epoch 2  79.5% | batch:       237 of       298\t|\tloss: 0.341761\n",
      "Training Epoch 2  79.9% | batch:       238 of       298\t|\tloss: 0.202651\n",
      "Training Epoch 2  80.2% | batch:       239 of       298\t|\tloss: 0.228505\n",
      "Training Epoch 2  80.5% | batch:       240 of       298\t|\tloss: 0.298648\n",
      "Training Epoch 2  80.9% | batch:       241 of       298\t|\tloss: 0.275526\n",
      "Training Epoch 2  81.2% | batch:       242 of       298\t|\tloss: 0.200277\n",
      "Training Epoch 2  81.5% | batch:       243 of       298\t|\tloss: 0.198373\n",
      "Training Epoch 2  81.9% | batch:       244 of       298\t|\tloss: 0.315298\n",
      "Training Epoch 2  82.2% | batch:       245 of       298\t|\tloss: 0.205752\n",
      "Training Epoch 2  82.6% | batch:       246 of       298\t|\tloss: 0.279005\n",
      "Training Epoch 2  82.9% | batch:       247 of       298\t|\tloss: 0.303734\n",
      "Training Epoch 2  83.2% | batch:       248 of       298\t|\tloss: 0.220881\n",
      "Training Epoch 2  83.6% | batch:       249 of       298\t|\tloss: 0.295616\n",
      "Training Epoch 2  83.9% | batch:       250 of       298\t|\tloss: 0.260336\n",
      "Training Epoch 2  84.2% | batch:       251 of       298\t|\tloss: 0.214501\n",
      "Training Epoch 2  84.6% | batch:       252 of       298\t|\tloss: 0.269537\n",
      "Training Epoch 2  84.9% | batch:       253 of       298\t|\tloss: 0.211024\n",
      "Training Epoch 2  85.2% | batch:       254 of       298\t|\tloss: 0.575132\n",
      "Training Epoch 2  85.6% | batch:       255 of       298\t|\tloss: 0.225691\n",
      "Training Epoch 2  85.9% | batch:       256 of       298\t|\tloss: 0.396348\n",
      "Training Epoch 2  86.2% | batch:       257 of       298\t|\tloss: 0.282463\n",
      "Training Epoch 2  86.6% | batch:       258 of       298\t|\tloss: 0.29625\n",
      "Training Epoch 2  86.9% | batch:       259 of       298\t|\tloss: 0.26845\n",
      "Training Epoch 2  87.2% | batch:       260 of       298\t|\tloss: 0.178726\n",
      "Training Epoch 2  87.6% | batch:       261 of       298\t|\tloss: 0.292119\n",
      "Training Epoch 2  87.9% | batch:       262 of       298\t|\tloss: 0.257029\n",
      "Training Epoch 2  88.3% | batch:       263 of       298\t|\tloss: 0.274099\n",
      "Training Epoch 2  88.6% | batch:       264 of       298\t|\tloss: 0.194242\n",
      "Training Epoch 2  88.9% | batch:       265 of       298\t|\tloss: 0.224018\n",
      "Training Epoch 2  89.3% | batch:       266 of       298\t|\tloss: 0.236091\n",
      "Training Epoch 2  89.6% | batch:       267 of       298\t|\tloss: 0.297219\n",
      "Training Epoch 2  89.9% | batch:       268 of       298\t|\tloss: 0.252963\n",
      "Training Epoch 2  90.3% | batch:       269 of       298\t|\tloss: 0.232735\n",
      "Training Epoch 2  90.6% | batch:       270 of       298\t|\tloss: 0.377572\n",
      "Training Epoch 2  90.9% | batch:       271 of       298\t|\tloss: 0.374485\n",
      "Training Epoch 2  91.3% | batch:       272 of       298\t|\tloss: 0.222992\n",
      "Training Epoch 2  91.6% | batch:       273 of       298\t|\tloss: 1.58866\n",
      "Training Epoch 2  91.9% | batch:       274 of       298\t|\tloss: 1.33983\n",
      "Training Epoch 2  92.3% | batch:       275 of       298\t|\tloss: 0.282945\n",
      "Training Epoch 2  92.6% | batch:       276 of       298\t|\tloss: 1.20062\n",
      "Training Epoch 2  93.0% | batch:       277 of       298\t|\tloss: 0.310257\n",
      "Training Epoch 2  93.3% | batch:       278 of       298\t|\tloss: 0.230835\n",
      "Training Epoch 2  93.6% | batch:       279 of       298\t|\tloss: 0.233947\n",
      "Training Epoch 2  94.0% | batch:       280 of       298\t|\tloss: 0.190914\n",
      "Training Epoch 2  94.3% | batch:       281 of       298\t|\tloss: 0.428534\n",
      "Training Epoch 2  94.6% | batch:       282 of       298\t|\tloss: 0.300805\n",
      "Training Epoch 2  95.0% | batch:       283 of       298\t|\tloss: 1.57194\n",
      "Training Epoch 2  95.3% | batch:       284 of       298\t|\tloss: 0.276877\n",
      "Training Epoch 2  95.6% | batch:       285 of       298\t|\tloss: 0.201106\n",
      "Training Epoch 2  96.0% | batch:       286 of       298\t|\tloss: 0.276325\n",
      "Training Epoch 2  96.3% | batch:       287 of       298\t|\tloss: 1.7813\n",
      "Training Epoch 2  96.6% | batch:       288 of       298\t|\tloss: 0.331278\n",
      "Training Epoch 2  97.0% | batch:       289 of       298\t|\tloss: 0.420999\n",
      "Training Epoch 2  97.3% | batch:       290 of       298\t|\tloss: 0.289191\n",
      "Training Epoch 2  97.7% | batch:       291 of       298\t|\tloss: 0.303283\n",
      "Training Epoch 2  98.0% | batch:       292 of       298\t|\tloss: 0.244629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:36:27,295 | INFO : Epoch 2 Training Summary: epoch: 2.000000 | loss: 0.421315 | \n",
      "2023-06-22 14:36:27,297 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.281769752502441 seconds\n",
      "\n",
      "2023-06-22 14:36:27,298 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.242113471031189 seconds\n",
      "2023-06-22 14:36:27,299 | INFO : Avg batch train. time: 0.041080917688024125 seconds\n",
      "2023-06-22 14:36:27,299 | INFO : Avg sample train. time: 0.0012840479831163403 seconds\n",
      "2023-06-22 14:36:27,300 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2  98.3% | batch:       293 of       298\t|\tloss: 0.310825\n",
      "Training Epoch 2  98.7% | batch:       294 of       298\t|\tloss: 0.261435\n",
      "Training Epoch 2  99.0% | batch:       295 of       298\t|\tloss: 0.42802\n",
      "Training Epoch 2  99.3% | batch:       296 of       298\t|\tloss: 0.355879\n",
      "Training Epoch 2  99.7% | batch:       297 of       298\t|\tloss: 0.268051\n",
      "\n",
      "Evaluating Epoch 2   0.0% | batch:         0 of        75\t|\tloss: 0.309279\n",
      "Evaluating Epoch 2   1.3% | batch:         1 of        75\t|\tloss: 1.0088\n",
      "Evaluating Epoch 2   2.7% | batch:         2 of        75\t|\tloss: 0.197185\n",
      "Evaluating Epoch 2   4.0% | batch:         3 of        75\t|\tloss: 0.304054\n",
      "Evaluating Epoch 2   5.3% | batch:         4 of        75\t|\tloss: 0.234757\n",
      "Evaluating Epoch 2   6.7% | batch:         5 of        75\t|\tloss: 0.230714\n",
      "Evaluating Epoch 2   8.0% | batch:         6 of        75\t|\tloss: 0.265792\n",
      "Evaluating Epoch 2   9.3% | batch:         7 of        75\t|\tloss: 0.182102\n",
      "Evaluating Epoch 2  10.7% | batch:         8 of        75\t|\tloss: 0.220858\n",
      "Evaluating Epoch 2  12.0% | batch:         9 of        75\t|\tloss: 0.423797\n",
      "Evaluating Epoch 2  13.3% | batch:        10 of        75\t|\tloss: 0.180394\n",
      "Evaluating Epoch 2  14.7% | batch:        11 of        75\t|\tloss: 0.170179\n",
      "Evaluating Epoch 2  16.0% | batch:        12 of        75\t|\tloss: 0.297951\n",
      "Evaluating Epoch 2  17.3% | batch:        13 of        75\t|\tloss: 0.277285\n",
      "Evaluating Epoch 2  18.7% | batch:        14 of        75\t|\tloss: 0.190708\n",
      "Evaluating Epoch 2  20.0% | batch:        15 of        75\t|\tloss: 0.264715\n",
      "Evaluating Epoch 2  21.3% | batch:        16 of        75\t|\tloss: 0.221258\n",
      "Evaluating Epoch 2  22.7% | batch:        17 of        75\t|\tloss: 0.173082\n",
      "Evaluating Epoch 2  24.0% | batch:        18 of        75\t|\tloss: 0.238576\n",
      "Evaluating Epoch 2  25.3% | batch:        19 of        75\t|\tloss: 0.243158\n",
      "Evaluating Epoch 2  26.7% | batch:        20 of        75\t|\tloss: 0.240446\n",
      "Evaluating Epoch 2  28.0% | batch:        21 of        75\t|\tloss: 0.270396\n",
      "Evaluating Epoch 2  29.3% | batch:        22 of        75\t|\tloss: 0.251503\n",
      "Evaluating Epoch 2  30.7% | batch:        23 of        75\t|\tloss: 1.39666\n",
      "Evaluating Epoch 2  32.0% | batch:        24 of        75\t|\tloss: 0.201336\n",
      "Evaluating Epoch 2  33.3% | batch:        25 of        75\t|\tloss: 0.201615\n",
      "Evaluating Epoch 2  34.7% | batch:        26 of        75\t|\tloss: 0.184239\n",
      "Evaluating Epoch 2  36.0% | batch:        27 of        75\t|\tloss: 0.299756\n",
      "Evaluating Epoch 2  37.3% | batch:        28 of        75\t|\tloss: 0.188445\n",
      "Evaluating Epoch 2  38.7% | batch:        29 of        75\t|\tloss: 0.289477\n",
      "Evaluating Epoch 2  40.0% | batch:        30 of        75\t|\tloss: 0.203356\n",
      "Evaluating Epoch 2  41.3% | batch:        31 of        75\t|\tloss: 0.215326\n",
      "Evaluating Epoch 2  42.7% | batch:        32 of        75\t|\tloss: 0.252106\n",
      "Evaluating Epoch 2  44.0% | batch:        33 of        75\t|\tloss: 0.303611\n",
      "Evaluating Epoch 2  45.3% | batch:        34 of        75\t|\tloss: 0.290778\n",
      "Evaluating Epoch 2  46.7% | batch:        35 of        75\t|\tloss: 1.02041\n",
      "Evaluating Epoch 2  48.0% | batch:        36 of        75\t|\tloss: 0.19374\n",
      "Evaluating Epoch 2  49.3% | batch:        37 of        75\t|\tloss: 0.188939\n",
      "Evaluating Epoch 2  50.7% | batch:        38 of        75\t|\tloss: 0.315316\n",
      "Evaluating Epoch 2  52.0% | batch:        39 of        75\t|\tloss: 0.611404\n",
      "Evaluating Epoch 2  53.3% | batch:        40 of        75\t|\tloss: 0.251923\n",
      "Evaluating Epoch 2  54.7% | batch:        41 of        75\t|\tloss: 0.177008\n",
      "Evaluating Epoch 2  56.0% | batch:        42 of        75\t|\tloss: 0.545724\n",
      "Evaluating Epoch 2  57.3% | batch:        43 of        75\t|\tloss: 0.207144\n",
      "Evaluating Epoch 2  58.7% | batch:        44 of        75\t|\tloss: 0.180128\n",
      "Evaluating Epoch 2  60.0% | batch:        45 of        75\t|\tloss: 0.189897\n",
      "Evaluating Epoch 2  61.3% | batch:        46 of        75\t|\tloss: 0.187499\n",
      "Evaluating Epoch 2  62.7% | batch:        47 of        75\t|\tloss: 0.1558\n",
      "Evaluating Epoch 2  64.0% | batch:        48 of        75\t|\tloss: 0.281019\n",
      "Evaluating Epoch 2  65.3% | batch:        49 of        75\t|\tloss: 0.243655\n",
      "Evaluating Epoch 2  66.7% | batch:        50 of        75\t|\tloss: 0.249634\n",
      "Evaluating Epoch 2  68.0% | batch:        51 of        75\t|\tloss: 0.176341\n",
      "Evaluating Epoch 2  69.3% | batch:        52 of        75\t|\tloss: 0.230245\n",
      "Evaluating Epoch 2  70.7% | batch:        53 of        75\t|\tloss: 0.166198\n",
      "Evaluating Epoch 2  72.0% | batch:        54 of        75\t|\tloss: 0.334831\n",
      "Evaluating Epoch 2  73.3% | batch:        55 of        75\t|\tloss: 0.168473\n",
      "Evaluating Epoch 2  74.7% | batch:        56 of        75\t|\tloss: 0.182179\n",
      "Evaluating Epoch 2  76.0% | batch:        57 of        75\t|\tloss: 0.229146\n",
      "Evaluating Epoch 2  77.3% | batch:        58 of        75\t|\tloss: 0.250589\n",
      "Evaluating Epoch 2  78.7% | batch:        59 of        75\t|\tloss: 0.19573\n",
      "Evaluating Epoch 2  80.0% | batch:        60 of        75\t|\tloss: 0.549494\n",
      "Evaluating Epoch 2  81.3% | batch:        61 of        75\t|\tloss: 0.176742\n",
      "Evaluating Epoch 2  82.7% | batch:        62 of        75\t|\tloss: 0.193407\n",
      "Evaluating Epoch 2  84.0% | batch:        63 of        75\t|\tloss: 0.272724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:36:28,563 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.262603998184204 seconds\n",
      "\n",
      "2023-06-22 14:36:28,564 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.2435832818349202 seconds\n",
      "2023-06-22 14:36:28,565 | INFO : Avg batch val. time: 0.0165811104244656 seconds\n",
      "2023-06-22 14:36:28,565 | INFO : Avg sample val. time: 0.0005216372826488759 seconds\n",
      "2023-06-22 14:36:28,566 | INFO : Epoch 2 Validation Summary: epoch: 2.000000 | loss: 0.287541 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 2  85.3% | batch:        64 of        75\t|\tloss: 0.808197\n",
      "Evaluating Epoch 2  86.7% | batch:        65 of        75\t|\tloss: 0.197834\n",
      "Evaluating Epoch 2  88.0% | batch:        66 of        75\t|\tloss: 0.173561\n",
      "Evaluating Epoch 2  89.3% | batch:        67 of        75\t|\tloss: 0.202207\n",
      "Evaluating Epoch 2  90.7% | batch:        68 of        75\t|\tloss: 0.201962\n",
      "Evaluating Epoch 2  92.0% | batch:        69 of        75\t|\tloss: 0.198868\n",
      "Evaluating Epoch 2  93.3% | batch:        70 of        75\t|\tloss: 0.177642\n",
      "Evaluating Epoch 2  94.7% | batch:        71 of        75\t|\tloss: 0.237155\n",
      "Evaluating Epoch 2  96.0% | batch:        72 of        75\t|\tloss: 0.183432\n",
      "Evaluating Epoch 2  97.3% | batch:        73 of        75\t|\tloss: 0.199276\n",
      "Evaluating Epoch 2  98.7% | batch:        74 of        75\t|\tloss: 0.422934\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Epoch:   0%|          | 2/400 [00:27<1:30:13, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3   0.0% | batch:         0 of       298\t|\tloss: 0.279529\n",
      "Training Epoch 3   0.3% | batch:         1 of       298\t|\tloss: 0.371718\n",
      "Training Epoch 3   0.7% | batch:         2 of       298\t|\tloss: 0.210916\n",
      "Training Epoch 3   1.0% | batch:         3 of       298\t|\tloss: 1.13954\n",
      "Training Epoch 3   1.3% | batch:         4 of       298\t|\tloss: 0.311659\n",
      "Training Epoch 3   1.7% | batch:         5 of       298\t|\tloss: 0.432885\n",
      "Training Epoch 3   2.0% | batch:         6 of       298\t|\tloss: 0.189457\n",
      "Training Epoch 3   2.3% | batch:         7 of       298\t|\tloss: 0.210895\n",
      "Training Epoch 3   2.7% | batch:         8 of       298\t|\tloss: 0.274103\n",
      "Training Epoch 3   3.0% | batch:         9 of       298\t|\tloss: 0.20468\n",
      "Training Epoch 3   3.4% | batch:        10 of       298\t|\tloss: 0.615169\n",
      "Training Epoch 3   3.7% | batch:        11 of       298\t|\tloss: 0.412208\n",
      "Training Epoch 3   4.0% | batch:        12 of       298\t|\tloss: 0.296321\n",
      "Training Epoch 3   4.4% | batch:        13 of       298\t|\tloss: 0.375852\n",
      "Training Epoch 3   4.7% | batch:        14 of       298\t|\tloss: 0.338807\n",
      "Training Epoch 3   5.0% | batch:        15 of       298\t|\tloss: 0.312498\n",
      "Training Epoch 3   5.4% | batch:        16 of       298\t|\tloss: 0.25057\n",
      "Training Epoch 3   5.7% | batch:        17 of       298\t|\tloss: 0.323957\n",
      "Training Epoch 3   6.0% | batch:        18 of       298\t|\tloss: 0.378137\n",
      "Training Epoch 3   6.4% | batch:        19 of       298\t|\tloss: 0.643242\n",
      "Training Epoch 3   6.7% | batch:        20 of       298\t|\tloss: 0.185864\n",
      "Training Epoch 3   7.0% | batch:        21 of       298\t|\tloss: 0.201456\n",
      "Training Epoch 3   7.4% | batch:        22 of       298\t|\tloss: 0.384213\n",
      "Training Epoch 3   7.7% | batch:        23 of       298\t|\tloss: 0.241874\n",
      "Training Epoch 3   8.1% | batch:        24 of       298\t|\tloss: 0.269784\n",
      "Training Epoch 3   8.4% | batch:        25 of       298\t|\tloss: 0.333448\n",
      "Training Epoch 3   8.7% | batch:        26 of       298\t|\tloss: 0.762862\n",
      "Training Epoch 3   9.1% | batch:        27 of       298\t|\tloss: 0.275169\n",
      "Training Epoch 3   9.4% | batch:        28 of       298\t|\tloss: 0.230141\n",
      "Training Epoch 3   9.7% | batch:        29 of       298\t|\tloss: 0.250718\n",
      "Training Epoch 3  10.1% | batch:        30 of       298\t|\tloss: 0.437014\n",
      "Training Epoch 3  10.4% | batch:        31 of       298\t|\tloss: 0.24397\n",
      "Training Epoch 3  10.7% | batch:        32 of       298\t|\tloss: 0.229918\n",
      "Training Epoch 3  11.1% | batch:        33 of       298\t|\tloss: 0.204262\n",
      "Training Epoch 3  11.4% | batch:        34 of       298\t|\tloss: 0.225138\n",
      "Training Epoch 3  11.7% | batch:        35 of       298\t|\tloss: 0.304591\n",
      "Training Epoch 3  12.1% | batch:        36 of       298\t|\tloss: 0.339005\n",
      "Training Epoch 3  12.4% | batch:        37 of       298\t|\tloss: 0.322144\n",
      "Training Epoch 3  12.8% | batch:        38 of       298\t|\tloss: 0.434777\n",
      "Training Epoch 3  13.1% | batch:        39 of       298\t|\tloss: 0.318156\n",
      "Training Epoch 3  13.4% | batch:        40 of       298\t|\tloss: 0.295473\n",
      "Training Epoch 3  13.8% | batch:        41 of       298\t|\tloss: 0.325808\n",
      "Training Epoch 3  14.1% | batch:        42 of       298\t|\tloss: 0.266488\n",
      "Training Epoch 3  14.4% | batch:        43 of       298\t|\tloss: 0.594979\n",
      "Training Epoch 3  14.8% | batch:        44 of       298\t|\tloss: 0.207962\n",
      "Training Epoch 3  15.1% | batch:        45 of       298\t|\tloss: 0.506918\n",
      "Training Epoch 3  15.4% | batch:        46 of       298\t|\tloss: 0.271589\n",
      "Training Epoch 3  15.8% | batch:        47 of       298\t|\tloss: 0.259483\n",
      "Training Epoch 3  16.1% | batch:        48 of       298\t|\tloss: 0.231906\n",
      "Training Epoch 3  16.4% | batch:        49 of       298\t|\tloss: 0.261385\n",
      "Training Epoch 3  16.8% | batch:        50 of       298\t|\tloss: 0.199377\n",
      "Training Epoch 3  17.1% | batch:        51 of       298\t|\tloss: 0.291516\n",
      "Training Epoch 3  17.4% | batch:        52 of       298\t|\tloss: 0.210988\n",
      "Training Epoch 3  17.8% | batch:        53 of       298\t|\tloss: 0.303787\n",
      "Training Epoch 3  18.1% | batch:        54 of       298\t|\tloss: 0.313407\n",
      "Training Epoch 3  18.5% | batch:        55 of       298\t|\tloss: 0.183738\n",
      "Training Epoch 3  18.8% | batch:        56 of       298\t|\tloss: 0.209409\n",
      "Training Epoch 3  19.1% | batch:        57 of       298\t|\tloss: 0.408794\n",
      "Training Epoch 3  19.5% | batch:        58 of       298\t|\tloss: 0.191267\n",
      "Training Epoch 3  19.8% | batch:        59 of       298\t|\tloss: 0.222152\n",
      "Training Epoch 3  20.1% | batch:        60 of       298\t|\tloss: 0.284274\n",
      "Training Epoch 3  20.5% | batch:        61 of       298\t|\tloss: 0.531057\n",
      "Training Epoch 3  20.8% | batch:        62 of       298\t|\tloss: 0.206402\n",
      "Training Epoch 3  21.1% | batch:        63 of       298\t|\tloss: 0.198506\n",
      "Training Epoch 3  21.5% | batch:        64 of       298\t|\tloss: 0.240573\n",
      "Training Epoch 3  21.8% | batch:        65 of       298\t|\tloss: 0.355994\n",
      "Training Epoch 3  22.1% | batch:        66 of       298\t|\tloss: 0.215569\n",
      "Training Epoch 3  22.5% | batch:        67 of       298\t|\tloss: 0.235701\n",
      "Training Epoch 3  22.8% | batch:        68 of       298\t|\tloss: 0.257928\n",
      "Training Epoch 3  23.2% | batch:        69 of       298\t|\tloss: 0.287187\n",
      "Training Epoch 3  23.5% | batch:        70 of       298\t|\tloss: 0.419669\n",
      "Training Epoch 3  23.8% | batch:        71 of       298\t|\tloss: 0.229087\n",
      "Training Epoch 3  24.2% | batch:        72 of       298\t|\tloss: 0.272819\n",
      "Training Epoch 3  24.5% | batch:        73 of       298\t|\tloss: 0.274408\n",
      "Training Epoch 3  24.8% | batch:        74 of       298\t|\tloss: 0.249727\n",
      "Training Epoch 3  25.2% | batch:        75 of       298\t|\tloss: 0.31532\n",
      "Training Epoch 3  25.5% | batch:        76 of       298\t|\tloss: 0.321252\n",
      "Training Epoch 3  25.8% | batch:        77 of       298\t|\tloss: 0.218133\n",
      "Training Epoch 3  26.2% | batch:        78 of       298\t|\tloss: 0.618468\n",
      "Training Epoch 3  26.5% | batch:        79 of       298\t|\tloss: 2.65836\n",
      "Training Epoch 3  26.8% | batch:        80 of       298\t|\tloss: 0.496632\n",
      "Training Epoch 3  27.2% | batch:        81 of       298\t|\tloss: 0.334131\n",
      "Training Epoch 3  27.5% | batch:        82 of       298\t|\tloss: 0.191407\n",
      "Training Epoch 3  27.9% | batch:        83 of       298\t|\tloss: 0.291129\n",
      "Training Epoch 3  28.2% | batch:        84 of       298\t|\tloss: 0.200993\n",
      "Training Epoch 3  28.5% | batch:        85 of       298\t|\tloss: 0.216576\n",
      "Training Epoch 3  28.9% | batch:        86 of       298\t|\tloss: 0.303642\n",
      "Training Epoch 3  29.2% | batch:        87 of       298\t|\tloss: 0.63778\n",
      "Training Epoch 3  29.5% | batch:        88 of       298\t|\tloss: 0.256195\n",
      "Training Epoch 3  29.9% | batch:        89 of       298\t|\tloss: 0.214846\n",
      "Training Epoch 3  30.2% | batch:        90 of       298\t|\tloss: 0.19224\n",
      "Training Epoch 3  30.5% | batch:        91 of       298\t|\tloss: 0.234122\n",
      "Training Epoch 3  30.9% | batch:        92 of       298\t|\tloss: 0.376232\n",
      "Training Epoch 3  31.2% | batch:        93 of       298\t|\tloss: 0.328638\n",
      "Training Epoch 3  31.5% | batch:        94 of       298\t|\tloss: 0.676389\n",
      "Training Epoch 3  31.9% | batch:        95 of       298\t|\tloss: 0.334419\n",
      "Training Epoch 3  32.2% | batch:        96 of       298\t|\tloss: 0.1897\n",
      "Training Epoch 3  32.6% | batch:        97 of       298\t|\tloss: 0.391882\n",
      "Training Epoch 3  32.9% | batch:        98 of       298\t|\tloss: 0.74456\n",
      "Training Epoch 3  33.2% | batch:        99 of       298\t|\tloss: 0.260468\n",
      "Training Epoch 3  33.6% | batch:       100 of       298\t|\tloss: 0.446027\n",
      "Training Epoch 3  33.9% | batch:       101 of       298\t|\tloss: 0.178766\n",
      "Training Epoch 3  34.2% | batch:       102 of       298\t|\tloss: 0.397638\n",
      "Training Epoch 3  34.6% | batch:       103 of       298\t|\tloss: 0.174894\n",
      "Training Epoch 3  34.9% | batch:       104 of       298\t|\tloss: 0.246589\n",
      "Training Epoch 3  35.2% | batch:       105 of       298\t|\tloss: 0.213421\n",
      "Training Epoch 3  35.6% | batch:       106 of       298\t|\tloss: 0.301207\n",
      "Training Epoch 3  35.9% | batch:       107 of       298\t|\tloss: 1.66612\n",
      "Training Epoch 3  36.2% | batch:       108 of       298\t|\tloss: 0.2221\n",
      "Training Epoch 3  36.6% | batch:       109 of       298\t|\tloss: 0.274821\n",
      "Training Epoch 3  36.9% | batch:       110 of       298\t|\tloss: 0.305067\n",
      "Training Epoch 3  37.2% | batch:       111 of       298\t|\tloss: 0.222852\n",
      "Training Epoch 3  37.6% | batch:       112 of       298\t|\tloss: 0.230584\n",
      "Training Epoch 3  37.9% | batch:       113 of       298\t|\tloss: 0.78792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  38.3% | batch:       114 of       298\t|\tloss: 0.220564\n",
      "Training Epoch 3  38.6% | batch:       115 of       298\t|\tloss: 0.213466\n",
      "Training Epoch 3  38.9% | batch:       116 of       298\t|\tloss: 0.254513\n",
      "Training Epoch 3  39.3% | batch:       117 of       298\t|\tloss: 0.220948\n",
      "Training Epoch 3  39.6% | batch:       118 of       298\t|\tloss: 0.29086\n",
      "Training Epoch 3  39.9% | batch:       119 of       298\t|\tloss: 0.203691\n",
      "Training Epoch 3  40.3% | batch:       120 of       298\t|\tloss: 0.265271\n",
      "Training Epoch 3  40.6% | batch:       121 of       298\t|\tloss: 0.254087\n",
      "Training Epoch 3  40.9% | batch:       122 of       298\t|\tloss: 0.2703\n",
      "Training Epoch 3  41.3% | batch:       123 of       298\t|\tloss: 0.208527\n",
      "Training Epoch 3  41.6% | batch:       124 of       298\t|\tloss: 0.237054\n",
      "Training Epoch 3  41.9% | batch:       125 of       298\t|\tloss: 0.217623\n",
      "Training Epoch 3  42.3% | batch:       126 of       298\t|\tloss: 0.372317\n",
      "Training Epoch 3  42.6% | batch:       127 of       298\t|\tloss: 0.705927\n",
      "Training Epoch 3  43.0% | batch:       128 of       298\t|\tloss: 0.176302\n",
      "Training Epoch 3  43.3% | batch:       129 of       298\t|\tloss: 0.431746\n",
      "Training Epoch 3  43.6% | batch:       130 of       298\t|\tloss: 0.226651\n",
      "Training Epoch 3  44.0% | batch:       131 of       298\t|\tloss: 0.347128\n",
      "Training Epoch 3  44.3% | batch:       132 of       298\t|\tloss: 0.615633\n",
      "Training Epoch 3  44.6% | batch:       133 of       298\t|\tloss: 0.536682\n",
      "Training Epoch 3  45.0% | batch:       134 of       298\t|\tloss: 0.174539\n",
      "Training Epoch 3  45.3% | batch:       135 of       298\t|\tloss: 0.243303\n",
      "Training Epoch 3  45.6% | batch:       136 of       298\t|\tloss: 0.304769\n",
      "Training Epoch 3  46.0% | batch:       137 of       298\t|\tloss: 0.29362\n",
      "Training Epoch 3  46.3% | batch:       138 of       298\t|\tloss: 2.17246\n",
      "Training Epoch 3  46.6% | batch:       139 of       298\t|\tloss: 0.23235\n",
      "Training Epoch 3  47.0% | batch:       140 of       298\t|\tloss: 0.245922\n",
      "Training Epoch 3  47.3% | batch:       141 of       298\t|\tloss: 0.221062\n",
      "Training Epoch 3  47.7% | batch:       142 of       298\t|\tloss: 0.240484\n",
      "Training Epoch 3  48.0% | batch:       143 of       298\t|\tloss: 0.283521\n",
      "Training Epoch 3  48.3% | batch:       144 of       298\t|\tloss: 0.235121\n",
      "Training Epoch 3  48.7% | batch:       145 of       298\t|\tloss: 0.201538\n",
      "Training Epoch 3  49.0% | batch:       146 of       298\t|\tloss: 0.180036\n",
      "Training Epoch 3  49.3% | batch:       147 of       298\t|\tloss: 0.297501\n",
      "Training Epoch 3  49.7% | batch:       148 of       298\t|\tloss: 0.225544\n",
      "Training Epoch 3  50.0% | batch:       149 of       298\t|\tloss: 0.242345\n",
      "Training Epoch 3  50.3% | batch:       150 of       298\t|\tloss: 0.296452\n",
      "Training Epoch 3  50.7% | batch:       151 of       298\t|\tloss: 0.311829\n",
      "Training Epoch 3  51.0% | batch:       152 of       298\t|\tloss: 0.248241\n",
      "Training Epoch 3  51.3% | batch:       153 of       298\t|\tloss: 0.244846\n",
      "Training Epoch 3  51.7% | batch:       154 of       298\t|\tloss: 0.315494\n",
      "Training Epoch 3  52.0% | batch:       155 of       298\t|\tloss: 0.354552\n",
      "Training Epoch 3  52.3% | batch:       156 of       298\t|\tloss: 1.46188\n",
      "Training Epoch 3  52.7% | batch:       157 of       298\t|\tloss: 0.164341\n",
      "Training Epoch 3  53.0% | batch:       158 of       298\t|\tloss: 0.208354\n",
      "Training Epoch 3  53.4% | batch:       159 of       298\t|\tloss: 0.223778\n",
      "Training Epoch 3  53.7% | batch:       160 of       298\t|\tloss: 0.320589\n",
      "Training Epoch 3  54.0% | batch:       161 of       298\t|\tloss: 0.364331\n",
      "Training Epoch 3  54.4% | batch:       162 of       298\t|\tloss: 0.343402\n",
      "Training Epoch 3  54.7% | batch:       163 of       298\t|\tloss: 0.301513\n",
      "Training Epoch 3  55.0% | batch:       164 of       298\t|\tloss: 0.296459\n",
      "Training Epoch 3  55.4% | batch:       165 of       298\t|\tloss: 0.189629\n",
      "Training Epoch 3  55.7% | batch:       166 of       298\t|\tloss: 0.247613\n",
      "Training Epoch 3  56.0% | batch:       167 of       298\t|\tloss: 0.194542\n",
      "Training Epoch 3  56.4% | batch:       168 of       298\t|\tloss: 0.377173\n",
      "Training Epoch 3  56.7% | batch:       169 of       298\t|\tloss: 0.167754\n",
      "Training Epoch 3  57.0% | batch:       170 of       298\t|\tloss: 0.411238\n",
      "Training Epoch 3  57.4% | batch:       171 of       298\t|\tloss: 0.240447\n",
      "Training Epoch 3  57.7% | batch:       172 of       298\t|\tloss: 0.55179\n",
      "Training Epoch 3  58.1% | batch:       173 of       298\t|\tloss: 0.270642\n",
      "Training Epoch 3  58.4% | batch:       174 of       298\t|\tloss: 0.275684\n",
      "Training Epoch 3  58.7% | batch:       175 of       298\t|\tloss: 0.303885\n",
      "Training Epoch 3  59.1% | batch:       176 of       298\t|\tloss: 0.52343\n",
      "Training Epoch 3  59.4% | batch:       177 of       298\t|\tloss: 0.304795\n",
      "Training Epoch 3  59.7% | batch:       178 of       298\t|\tloss: 0.268672\n",
      "Training Epoch 3  60.1% | batch:       179 of       298\t|\tloss: 0.208192\n",
      "Training Epoch 3  60.4% | batch:       180 of       298\t|\tloss: 0.671921\n",
      "Training Epoch 3  60.7% | batch:       181 of       298\t|\tloss: 0.20591\n",
      "Training Epoch 3  61.1% | batch:       182 of       298\t|\tloss: 0.258264\n",
      "Training Epoch 3  61.4% | batch:       183 of       298\t|\tloss: 0.290667\n",
      "Training Epoch 3  61.7% | batch:       184 of       298\t|\tloss: 0.262792\n",
      "Training Epoch 3  62.1% | batch:       185 of       298\t|\tloss: 0.26718\n",
      "Training Epoch 3  62.4% | batch:       186 of       298\t|\tloss: 0.24937\n",
      "Training Epoch 3  62.8% | batch:       187 of       298\t|\tloss: 0.404124\n",
      "Training Epoch 3  63.1% | batch:       188 of       298\t|\tloss: 0.499952\n",
      "Training Epoch 3  63.4% | batch:       189 of       298\t|\tloss: 0.245897\n",
      "Training Epoch 3  63.8% | batch:       190 of       298\t|\tloss: 0.412902\n",
      "Training Epoch 3  64.1% | batch:       191 of       298\t|\tloss: 0.184973\n",
      "Training Epoch 3  64.4% | batch:       192 of       298\t|\tloss: 0.335266\n",
      "Training Epoch 3  64.8% | batch:       193 of       298\t|\tloss: 0.215933\n",
      "Training Epoch 3  65.1% | batch:       194 of       298\t|\tloss: 0.263098\n",
      "Training Epoch 3  65.4% | batch:       195 of       298\t|\tloss: 0.238409\n",
      "Training Epoch 3  65.8% | batch:       196 of       298\t|\tloss: 0.281943\n",
      "Training Epoch 3  66.1% | batch:       197 of       298\t|\tloss: 0.246185\n",
      "Training Epoch 3  66.4% | batch:       198 of       298\t|\tloss: 0.242534\n",
      "Training Epoch 3  66.8% | batch:       199 of       298\t|\tloss: 0.293083\n",
      "Training Epoch 3  67.1% | batch:       200 of       298\t|\tloss: 0.212477\n",
      "Training Epoch 3  67.4% | batch:       201 of       298\t|\tloss: 0.237043\n",
      "Training Epoch 3  67.8% | batch:       202 of       298\t|\tloss: 0.33688\n",
      "Training Epoch 3  68.1% | batch:       203 of       298\t|\tloss: 0.283624\n",
      "Training Epoch 3  68.5% | batch:       204 of       298\t|\tloss: 0.226745\n",
      "Training Epoch 3  68.8% | batch:       205 of       298\t|\tloss: 0.19169\n",
      "Training Epoch 3  69.1% | batch:       206 of       298\t|\tloss: 0.234011\n",
      "Training Epoch 3  69.5% | batch:       207 of       298\t|\tloss: 0.183523\n",
      "Training Epoch 3  69.8% | batch:       208 of       298\t|\tloss: 0.228024\n",
      "Training Epoch 3  70.1% | batch:       209 of       298\t|\tloss: 0.202765\n",
      "Training Epoch 3  70.5% | batch:       210 of       298\t|\tloss: 0.236234\n",
      "Training Epoch 3  70.8% | batch:       211 of       298\t|\tloss: 0.250918\n",
      "Training Epoch 3  71.1% | batch:       212 of       298\t|\tloss: 1.12778\n",
      "Training Epoch 3  71.5% | batch:       213 of       298\t|\tloss: 0.252461\n",
      "Training Epoch 3  71.8% | batch:       214 of       298\t|\tloss: 0.199875\n",
      "Training Epoch 3  72.1% | batch:       215 of       298\t|\tloss: 0.310358\n",
      "Training Epoch 3  72.5% | batch:       216 of       298\t|\tloss: 0.217124\n",
      "Training Epoch 3  72.8% | batch:       217 of       298\t|\tloss: 0.251918\n",
      "Training Epoch 3  73.2% | batch:       218 of       298\t|\tloss: 0.287969\n",
      "Training Epoch 3  73.5% | batch:       219 of       298\t|\tloss: 0.200395\n",
      "Training Epoch 3  73.8% | batch:       220 of       298\t|\tloss: 0.239932\n",
      "Training Epoch 3  74.2% | batch:       221 of       298\t|\tloss: 0.223791\n",
      "Training Epoch 3  74.5% | batch:       222 of       298\t|\tloss: 0.186263\n",
      "Training Epoch 3  74.8% | batch:       223 of       298\t|\tloss: 0.321996\n",
      "Training Epoch 3  75.2% | batch:       224 of       298\t|\tloss: 0.222064\n",
      "Training Epoch 3  75.5% | batch:       225 of       298\t|\tloss: 0.263607\n",
      "Training Epoch 3  75.8% | batch:       226 of       298\t|\tloss: 0.181003\n",
      "Training Epoch 3  76.2% | batch:       227 of       298\t|\tloss: 0.403257\n",
      "Training Epoch 3  76.5% | batch:       228 of       298\t|\tloss: 0.21845\n",
      "Training Epoch 3  76.8% | batch:       229 of       298\t|\tloss: 0.243227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  77.2% | batch:       230 of       298\t|\tloss: 0.304992\n",
      "Training Epoch 3  77.5% | batch:       231 of       298\t|\tloss: 0.197569\n",
      "Training Epoch 3  77.9% | batch:       232 of       298\t|\tloss: 0.224784\n",
      "Training Epoch 3  78.2% | batch:       233 of       298\t|\tloss: 0.303042\n",
      "Training Epoch 3  78.5% | batch:       234 of       298\t|\tloss: 0.251046\n",
      "Training Epoch 3  78.9% | batch:       235 of       298\t|\tloss: 0.197351\n",
      "Training Epoch 3  79.2% | batch:       236 of       298\t|\tloss: 0.183253\n",
      "Training Epoch 3  79.5% | batch:       237 of       298\t|\tloss: 0.237731\n",
      "Training Epoch 3  79.9% | batch:       238 of       298\t|\tloss: 0.202186\n",
      "Training Epoch 3  80.2% | batch:       239 of       298\t|\tloss: 0.28995\n",
      "Training Epoch 3  80.5% | batch:       240 of       298\t|\tloss: 0.240728\n",
      "Training Epoch 3  80.9% | batch:       241 of       298\t|\tloss: 0.335338\n",
      "Training Epoch 3  81.2% | batch:       242 of       298\t|\tloss: 0.373469\n",
      "Training Epoch 3  81.5% | batch:       243 of       298\t|\tloss: 1.28511\n",
      "Training Epoch 3  81.9% | batch:       244 of       298\t|\tloss: 0.218769\n",
      "Training Epoch 3  82.2% | batch:       245 of       298\t|\tloss: 0.183039\n",
      "Training Epoch 3  82.6% | batch:       246 of       298\t|\tloss: 0.486257\n",
      "Training Epoch 3  82.9% | batch:       247 of       298\t|\tloss: 0.198558\n",
      "Training Epoch 3  83.2% | batch:       248 of       298\t|\tloss: 0.568691\n",
      "Training Epoch 3  83.6% | batch:       249 of       298\t|\tloss: 0.266349\n",
      "Training Epoch 3  83.9% | batch:       250 of       298\t|\tloss: 0.216922\n",
      "Training Epoch 3  84.2% | batch:       251 of       298\t|\tloss: 0.246539\n",
      "Training Epoch 3  84.6% | batch:       252 of       298\t|\tloss: 0.810246\n",
      "Training Epoch 3  84.9% | batch:       253 of       298\t|\tloss: 0.192768\n",
      "Training Epoch 3  85.2% | batch:       254 of       298\t|\tloss: 0.292692\n",
      "Training Epoch 3  85.6% | batch:       255 of       298\t|\tloss: 0.254089\n",
      "Training Epoch 3  85.9% | batch:       256 of       298\t|\tloss: 0.279999\n",
      "Training Epoch 3  86.2% | batch:       257 of       298\t|\tloss: 0.23895\n",
      "Training Epoch 3  86.6% | batch:       258 of       298\t|\tloss: 0.219494\n",
      "Training Epoch 3  86.9% | batch:       259 of       298\t|\tloss: 0.235684\n",
      "Training Epoch 3  87.2% | batch:       260 of       298\t|\tloss: 0.593034\n",
      "Training Epoch 3  87.6% | batch:       261 of       298\t|\tloss: 0.232411\n",
      "Training Epoch 3  87.9% | batch:       262 of       298\t|\tloss: 0.20789\n",
      "Training Epoch 3  88.3% | batch:       263 of       298\t|\tloss: 0.189084\n",
      "Training Epoch 3  88.6% | batch:       264 of       298\t|\tloss: 0.17797\n",
      "Training Epoch 3  88.9% | batch:       265 of       298\t|\tloss: 0.244936\n",
      "Training Epoch 3  89.3% | batch:       266 of       298\t|\tloss: 0.179597\n",
      "Training Epoch 3  89.6% | batch:       267 of       298\t|\tloss: 0.851737\n",
      "Training Epoch 3  89.9% | batch:       268 of       298\t|\tloss: 0.201607\n",
      "Training Epoch 3  90.3% | batch:       269 of       298\t|\tloss: 0.324903\n",
      "Training Epoch 3  90.6% | batch:       270 of       298\t|\tloss: 0.204227\n",
      "Training Epoch 3  90.9% | batch:       271 of       298\t|\tloss: 0.248114\n",
      "Training Epoch 3  91.3% | batch:       272 of       298\t|\tloss: 0.348819\n",
      "Training Epoch 3  91.6% | batch:       273 of       298\t|\tloss: 0.267212\n",
      "Training Epoch 3  91.9% | batch:       274 of       298\t|\tloss: 1.03914\n",
      "Training Epoch 3  92.3% | batch:       275 of       298\t|\tloss: 0.203695\n",
      "Training Epoch 3  92.6% | batch:       276 of       298\t|\tloss: 0.238317\n",
      "Training Epoch 3  93.0% | batch:       277 of       298\t|\tloss: 0.237586\n",
      "Training Epoch 3  93.3% | batch:       278 of       298\t|\tloss: 0.185513\n",
      "Training Epoch 3  93.6% | batch:       279 of       298\t|\tloss: 0.364413\n",
      "Training Epoch 3  94.0% | batch:       280 of       298\t|\tloss: 0.278346\n",
      "Training Epoch 3  94.3% | batch:       281 of       298\t|\tloss: 0.174434\n",
      "Training Epoch 3  94.6% | batch:       282 of       298\t|\tloss: 0.303791\n",
      "Training Epoch 3  95.0% | batch:       283 of       298\t|\tloss: 0.671224\n",
      "Training Epoch 3  95.3% | batch:       284 of       298\t|\tloss: 0.310783\n",
      "Training Epoch 3  95.6% | batch:       285 of       298\t|\tloss: 0.239693\n",
      "Training Epoch 3  96.0% | batch:       286 of       298\t|\tloss: 0.228793\n",
      "Training Epoch 3  96.3% | batch:       287 of       298\t|\tloss: 0.274285\n",
      "Training Epoch 3  96.6% | batch:       288 of       298\t|\tloss: 0.225396\n",
      "Training Epoch 3  97.0% | batch:       289 of       298\t|\tloss: 3.22428\n",
      "Training Epoch 3  97.3% | batch:       290 of       298\t|\tloss: 0.239937\n",
      "Training Epoch 3  97.7% | batch:       291 of       298\t|\tloss: 0.236888\n",
      "Training Epoch 3  98.0% | batch:       292 of       298\t|\tloss: 0.227165\n",
      "Training Epoch 3  98.3% | batch:       293 of       298\t|\tloss: 0.235635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:36:40,955 | INFO : Epoch 3 Training Summary: epoch: 3.000000 | loss: 0.342327 | \n",
      "2023-06-22 14:36:40,956 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.305079936981201 seconds\n",
      "\n",
      "2023-06-22 14:36:40,957 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.263102293014526 seconds\n",
      "2023-06-22 14:36:40,957 | INFO : Avg batch train. time: 0.04115134997655882 seconds\n",
      "2023-06-22 14:36:40,959 | INFO : Avg sample train. time: 0.0012862494538509047 seconds\n",
      "Training Epoch:   1%|          | 3/400 [00:39<1:26:10, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3  98.7% | batch:       294 of       298\t|\tloss: 0.244461\n",
      "Training Epoch 3  99.0% | batch:       295 of       298\t|\tloss: 1.52216\n",
      "Training Epoch 3  99.3% | batch:       296 of       298\t|\tloss: 0.265747\n",
      "Training Epoch 3  99.7% | batch:       297 of       298\t|\tloss: 0.301884\n",
      "\n",
      "Training Epoch 4   0.0% | batch:         0 of       298\t|\tloss: 0.266762\n",
      "Training Epoch 4   0.3% | batch:         1 of       298\t|\tloss: 0.250684\n",
      "Training Epoch 4   0.7% | batch:         2 of       298\t|\tloss: 0.182544\n",
      "Training Epoch 4   1.0% | batch:         3 of       298\t|\tloss: 0.252123\n",
      "Training Epoch 4   1.3% | batch:         4 of       298\t|\tloss: 0.192592\n",
      "Training Epoch 4   1.7% | batch:         5 of       298\t|\tloss: 0.198487\n",
      "Training Epoch 4   2.0% | batch:         6 of       298\t|\tloss: 0.253433\n",
      "Training Epoch 4   2.3% | batch:         7 of       298\t|\tloss: 0.192754\n",
      "Training Epoch 4   2.7% | batch:         8 of       298\t|\tloss: 0.440513\n",
      "Training Epoch 4   3.0% | batch:         9 of       298\t|\tloss: 0.266104\n",
      "Training Epoch 4   3.4% | batch:        10 of       298\t|\tloss: 0.198973\n",
      "Training Epoch 4   3.7% | batch:        11 of       298\t|\tloss: 0.246697\n",
      "Training Epoch 4   4.0% | batch:        12 of       298\t|\tloss: 1.76689\n",
      "Training Epoch 4   4.4% | batch:        13 of       298\t|\tloss: 0.176402\n",
      "Training Epoch 4   4.7% | batch:        14 of       298\t|\tloss: 2.48863\n",
      "Training Epoch 4   5.0% | batch:        15 of       298\t|\tloss: 0.258594\n",
      "Training Epoch 4   5.4% | batch:        16 of       298\t|\tloss: 0.279321\n",
      "Training Epoch 4   5.7% | batch:        17 of       298\t|\tloss: 0.329879\n",
      "Training Epoch 4   6.0% | batch:        18 of       298\t|\tloss: 0.208874\n",
      "Training Epoch 4   6.4% | batch:        19 of       298\t|\tloss: 0.251225\n",
      "Training Epoch 4   6.7% | batch:        20 of       298\t|\tloss: 0.178199\n",
      "Training Epoch 4   7.0% | batch:        21 of       298\t|\tloss: 0.382405\n",
      "Training Epoch 4   7.4% | batch:        22 of       298\t|\tloss: 0.24376\n",
      "Training Epoch 4   7.7% | batch:        23 of       298\t|\tloss: 1.11372\n",
      "Training Epoch 4   8.1% | batch:        24 of       298\t|\tloss: 0.384124\n",
      "Training Epoch 4   8.4% | batch:        25 of       298\t|\tloss: 0.31056\n",
      "Training Epoch 4   8.7% | batch:        26 of       298\t|\tloss: 1.06013\n",
      "Training Epoch 4   9.1% | batch:        27 of       298\t|\tloss: 0.283632\n",
      "Training Epoch 4   9.4% | batch:        28 of       298\t|\tloss: 0.394286\n",
      "Training Epoch 4   9.7% | batch:        29 of       298\t|\tloss: 0.300182\n",
      "Training Epoch 4  10.1% | batch:        30 of       298\t|\tloss: 0.372658\n",
      "Training Epoch 4  10.4% | batch:        31 of       298\t|\tloss: 0.309114\n",
      "Training Epoch 4  10.7% | batch:        32 of       298\t|\tloss: 0.362635\n",
      "Training Epoch 4  11.1% | batch:        33 of       298\t|\tloss: 0.28299\n",
      "Training Epoch 4  11.4% | batch:        34 of       298\t|\tloss: 0.254149\n",
      "Training Epoch 4  11.7% | batch:        35 of       298\t|\tloss: 0.17896\n",
      "Training Epoch 4  12.1% | batch:        36 of       298\t|\tloss: 0.254454\n",
      "Training Epoch 4  12.4% | batch:        37 of       298\t|\tloss: 0.590624\n",
      "Training Epoch 4  12.8% | batch:        38 of       298\t|\tloss: 0.2118\n",
      "Training Epoch 4  13.1% | batch:        39 of       298\t|\tloss: 0.211564\n",
      "Training Epoch 4  13.4% | batch:        40 of       298\t|\tloss: 0.236028\n",
      "Training Epoch 4  13.8% | batch:        41 of       298\t|\tloss: 0.230867\n",
      "Training Epoch 4  14.1% | batch:        42 of       298\t|\tloss: 0.189644\n",
      "Training Epoch 4  14.4% | batch:        43 of       298\t|\tloss: 0.255856\n",
      "Training Epoch 4  14.8% | batch:        44 of       298\t|\tloss: 0.287658\n",
      "Training Epoch 4  15.1% | batch:        45 of       298\t|\tloss: 0.189226\n",
      "Training Epoch 4  15.4% | batch:        46 of       298\t|\tloss: 0.256013\n",
      "Training Epoch 4  15.8% | batch:        47 of       298\t|\tloss: 0.44508\n",
      "Training Epoch 4  16.1% | batch:        48 of       298\t|\tloss: 0.279784\n",
      "Training Epoch 4  16.4% | batch:        49 of       298\t|\tloss: 0.251481\n",
      "Training Epoch 4  16.8% | batch:        50 of       298\t|\tloss: 1.27579\n",
      "Training Epoch 4  17.1% | batch:        51 of       298\t|\tloss: 0.272514\n",
      "Training Epoch 4  17.4% | batch:        52 of       298\t|\tloss: 1.36655\n",
      "Training Epoch 4  17.8% | batch:        53 of       298\t|\tloss: 0.19151\n",
      "Training Epoch 4  18.1% | batch:        54 of       298\t|\tloss: 0.214682\n",
      "Training Epoch 4  18.5% | batch:        55 of       298\t|\tloss: 0.214165\n",
      "Training Epoch 4  18.8% | batch:        56 of       298\t|\tloss: 0.189204\n",
      "Training Epoch 4  19.1% | batch:        57 of       298\t|\tloss: 0.249954\n",
      "Training Epoch 4  19.5% | batch:        58 of       298\t|\tloss: 0.179159\n",
      "Training Epoch 4  19.8% | batch:        59 of       298\t|\tloss: 0.350616\n",
      "Training Epoch 4  20.1% | batch:        60 of       298\t|\tloss: 0.19738\n",
      "Training Epoch 4  20.5% | batch:        61 of       298\t|\tloss: 0.376099\n",
      "Training Epoch 4  20.8% | batch:        62 of       298\t|\tloss: 0.320554\n",
      "Training Epoch 4  21.1% | batch:        63 of       298\t|\tloss: 0.285898\n",
      "Training Epoch 4  21.5% | batch:        64 of       298\t|\tloss: 0.172306\n",
      "Training Epoch 4  21.8% | batch:        65 of       298\t|\tloss: 0.225556\n",
      "Training Epoch 4  22.1% | batch:        66 of       298\t|\tloss: 0.351928\n",
      "Training Epoch 4  22.5% | batch:        67 of       298\t|\tloss: 0.198869\n",
      "Training Epoch 4  22.8% | batch:        68 of       298\t|\tloss: 0.230869\n",
      "Training Epoch 4  23.2% | batch:        69 of       298\t|\tloss: 1.1432\n",
      "Training Epoch 4  23.5% | batch:        70 of       298\t|\tloss: 0.203949\n",
      "Training Epoch 4  23.8% | batch:        71 of       298\t|\tloss: 0.284479\n",
      "Training Epoch 4  24.2% | batch:        72 of       298\t|\tloss: 0.224248\n",
      "Training Epoch 4  24.5% | batch:        73 of       298\t|\tloss: 0.252146\n",
      "Training Epoch 4  24.8% | batch:        74 of       298\t|\tloss: 0.224481\n",
      "Training Epoch 4  25.2% | batch:        75 of       298\t|\tloss: 0.314038\n",
      "Training Epoch 4  25.5% | batch:        76 of       298\t|\tloss: 0.185891\n",
      "Training Epoch 4  25.8% | batch:        77 of       298\t|\tloss: 0.259365\n",
      "Training Epoch 4  26.2% | batch:        78 of       298\t|\tloss: 0.235249\n",
      "Training Epoch 4  26.5% | batch:        79 of       298\t|\tloss: 0.21427\n",
      "Training Epoch 4  26.8% | batch:        80 of       298\t|\tloss: 0.242542\n",
      "Training Epoch 4  27.2% | batch:        81 of       298\t|\tloss: 0.270289\n",
      "Training Epoch 4  27.5% | batch:        82 of       298\t|\tloss: 0.163594\n",
      "Training Epoch 4  27.9% | batch:        83 of       298\t|\tloss: 0.214447\n",
      "Training Epoch 4  28.2% | batch:        84 of       298\t|\tloss: 0.204765\n",
      "Training Epoch 4  28.5% | batch:        85 of       298\t|\tloss: 0.159304\n",
      "Training Epoch 4  28.9% | batch:        86 of       298\t|\tloss: 0.256965\n",
      "Training Epoch 4  29.2% | batch:        87 of       298\t|\tloss: 0.312924\n",
      "Training Epoch 4  29.5% | batch:        88 of       298\t|\tloss: 0.267287\n",
      "Training Epoch 4  29.9% | batch:        89 of       298\t|\tloss: 0.244527\n",
      "Training Epoch 4  30.2% | batch:        90 of       298\t|\tloss: 0.25238\n",
      "Training Epoch 4  30.5% | batch:        91 of       298\t|\tloss: 0.212837\n",
      "Training Epoch 4  30.9% | batch:        92 of       298\t|\tloss: 0.624368\n",
      "Training Epoch 4  31.2% | batch:        93 of       298\t|\tloss: 0.218275\n",
      "Training Epoch 4  31.5% | batch:        94 of       298\t|\tloss: 1.97318\n",
      "Training Epoch 4  31.9% | batch:        95 of       298\t|\tloss: 0.227362\n",
      "Training Epoch 4  32.2% | batch:        96 of       298\t|\tloss: 0.212879\n",
      "Training Epoch 4  32.6% | batch:        97 of       298\t|\tloss: 0.253735\n",
      "Training Epoch 4  32.9% | batch:        98 of       298\t|\tloss: 0.21775\n",
      "Training Epoch 4  33.2% | batch:        99 of       298\t|\tloss: 0.198835\n",
      "Training Epoch 4  33.6% | batch:       100 of       298\t|\tloss: 0.260687\n",
      "Training Epoch 4  33.9% | batch:       101 of       298\t|\tloss: 0.267971\n",
      "Training Epoch 4  34.2% | batch:       102 of       298\t|\tloss: 0.328668\n",
      "Training Epoch 4  34.6% | batch:       103 of       298\t|\tloss: 0.253657\n",
      "Training Epoch 4  34.9% | batch:       104 of       298\t|\tloss: 0.236492\n",
      "Training Epoch 4  35.2% | batch:       105 of       298\t|\tloss: 0.184283\n",
      "Training Epoch 4  35.6% | batch:       106 of       298\t|\tloss: 0.387409\n",
      "Training Epoch 4  35.9% | batch:       107 of       298\t|\tloss: 0.284204\n",
      "Training Epoch 4  36.2% | batch:       108 of       298\t|\tloss: 0.219111\n",
      "Training Epoch 4  36.6% | batch:       109 of       298\t|\tloss: 0.201431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  36.9% | batch:       110 of       298\t|\tloss: 0.240061\n",
      "Training Epoch 4  37.2% | batch:       111 of       298\t|\tloss: 0.202381\n",
      "Training Epoch 4  37.6% | batch:       112 of       298\t|\tloss: 0.174844\n",
      "Training Epoch 4  37.9% | batch:       113 of       298\t|\tloss: 1.17596\n",
      "Training Epoch 4  38.3% | batch:       114 of       298\t|\tloss: 0.192122\n",
      "Training Epoch 4  38.6% | batch:       115 of       298\t|\tloss: 0.284857\n",
      "Training Epoch 4  38.9% | batch:       116 of       298\t|\tloss: 0.32138\n",
      "Training Epoch 4  39.3% | batch:       117 of       298\t|\tloss: 0.266311\n",
      "Training Epoch 4  39.6% | batch:       118 of       298\t|\tloss: 0.220978\n",
      "Training Epoch 4  39.9% | batch:       119 of       298\t|\tloss: 0.270461\n",
      "Training Epoch 4  40.3% | batch:       120 of       298\t|\tloss: 0.252154\n",
      "Training Epoch 4  40.6% | batch:       121 of       298\t|\tloss: 0.204451\n",
      "Training Epoch 4  40.9% | batch:       122 of       298\t|\tloss: 0.206054\n",
      "Training Epoch 4  41.3% | batch:       123 of       298\t|\tloss: 0.286487\n",
      "Training Epoch 4  41.6% | batch:       124 of       298\t|\tloss: 0.350514\n",
      "Training Epoch 4  41.9% | batch:       125 of       298\t|\tloss: 0.204057\n",
      "Training Epoch 4  42.3% | batch:       126 of       298\t|\tloss: 0.173009\n",
      "Training Epoch 4  42.6% | batch:       127 of       298\t|\tloss: 0.191031\n",
      "Training Epoch 4  43.0% | batch:       128 of       298\t|\tloss: 0.266741\n",
      "Training Epoch 4  43.3% | batch:       129 of       298\t|\tloss: 0.220661\n",
      "Training Epoch 4  43.6% | batch:       130 of       298\t|\tloss: 0.196933\n",
      "Training Epoch 4  44.0% | batch:       131 of       298\t|\tloss: 0.206211\n",
      "Training Epoch 4  44.3% | batch:       132 of       298\t|\tloss: 0.243558\n",
      "Training Epoch 4  44.6% | batch:       133 of       298\t|\tloss: 0.202635\n",
      "Training Epoch 4  45.0% | batch:       134 of       298\t|\tloss: 0.33102\n",
      "Training Epoch 4  45.3% | batch:       135 of       298\t|\tloss: 0.270167\n",
      "Training Epoch 4  45.6% | batch:       136 of       298\t|\tloss: 0.179871\n",
      "Training Epoch 4  46.0% | batch:       137 of       298\t|\tloss: 0.206933\n",
      "Training Epoch 4  46.3% | batch:       138 of       298\t|\tloss: 0.19794\n",
      "Training Epoch 4  46.6% | batch:       139 of       298\t|\tloss: 0.234851\n",
      "Training Epoch 4  47.0% | batch:       140 of       298\t|\tloss: 0.218745\n",
      "Training Epoch 4  47.3% | batch:       141 of       298\t|\tloss: 0.214993\n",
      "Training Epoch 4  47.7% | batch:       142 of       298\t|\tloss: 0.190558\n",
      "Training Epoch 4  48.0% | batch:       143 of       298\t|\tloss: 0.207414\n",
      "Training Epoch 4  48.3% | batch:       144 of       298\t|\tloss: 0.377035\n",
      "Training Epoch 4  48.7% | batch:       145 of       298\t|\tloss: 0.225965\n",
      "Training Epoch 4  49.0% | batch:       146 of       298\t|\tloss: 0.150585\n",
      "Training Epoch 4  49.3% | batch:       147 of       298\t|\tloss: 0.196683\n",
      "Training Epoch 4  49.7% | batch:       148 of       298\t|\tloss: 0.391155\n",
      "Training Epoch 4  50.0% | batch:       149 of       298\t|\tloss: 0.285337\n",
      "Training Epoch 4  50.3% | batch:       150 of       298\t|\tloss: 0.182502\n",
      "Training Epoch 4  50.7% | batch:       151 of       298\t|\tloss: 0.186607\n",
      "Training Epoch 4  51.0% | batch:       152 of       298\t|\tloss: 0.245367\n",
      "Training Epoch 4  51.3% | batch:       153 of       298\t|\tloss: 0.222178\n",
      "Training Epoch 4  51.7% | batch:       154 of       298\t|\tloss: 0.168661\n",
      "Training Epoch 4  52.0% | batch:       155 of       298\t|\tloss: 0.43675\n",
      "Training Epoch 4  52.3% | batch:       156 of       298\t|\tloss: 0.236106\n",
      "Training Epoch 4  52.7% | batch:       157 of       298\t|\tloss: 0.211163\n",
      "Training Epoch 4  53.0% | batch:       158 of       298\t|\tloss: 0.267329\n",
      "Training Epoch 4  53.4% | batch:       159 of       298\t|\tloss: 0.334678\n",
      "Training Epoch 4  53.7% | batch:       160 of       298\t|\tloss: 0.17737\n",
      "Training Epoch 4  54.0% | batch:       161 of       298\t|\tloss: 0.465004\n",
      "Training Epoch 4  54.4% | batch:       162 of       298\t|\tloss: 0.232765\n",
      "Training Epoch 4  54.7% | batch:       163 of       298\t|\tloss: 0.398127\n",
      "Training Epoch 4  55.0% | batch:       164 of       298\t|\tloss: 0.217674\n",
      "Training Epoch 4  55.4% | batch:       165 of       298\t|\tloss: 0.126451\n",
      "Training Epoch 4  55.7% | batch:       166 of       298\t|\tloss: 0.210579\n",
      "Training Epoch 4  56.0% | batch:       167 of       298\t|\tloss: 0.271358\n",
      "Training Epoch 4  56.4% | batch:       168 of       298\t|\tloss: 0.491712\n",
      "Training Epoch 4  56.7% | batch:       169 of       298\t|\tloss: 0.307429\n",
      "Training Epoch 4  57.0% | batch:       170 of       298\t|\tloss: 0.261493\n",
      "Training Epoch 4  57.4% | batch:       171 of       298\t|\tloss: 0.202683\n",
      "Training Epoch 4  57.7% | batch:       172 of       298\t|\tloss: 0.153856\n",
      "Training Epoch 4  58.1% | batch:       173 of       298\t|\tloss: 0.224616\n",
      "Training Epoch 4  58.4% | batch:       174 of       298\t|\tloss: 0.230468\n",
      "Training Epoch 4  58.7% | batch:       175 of       298\t|\tloss: 0.177201\n",
      "Training Epoch 4  59.1% | batch:       176 of       298\t|\tloss: 0.207508\n",
      "Training Epoch 4  59.4% | batch:       177 of       298\t|\tloss: 0.191605\n",
      "Training Epoch 4  59.7% | batch:       178 of       298\t|\tloss: 0.421363\n",
      "Training Epoch 4  60.1% | batch:       179 of       298\t|\tloss: 0.211869\n",
      "Training Epoch 4  60.4% | batch:       180 of       298\t|\tloss: 0.202156\n",
      "Training Epoch 4  60.7% | batch:       181 of       298\t|\tloss: 0.352676\n",
      "Training Epoch 4  61.1% | batch:       182 of       298\t|\tloss: 0.228696\n",
      "Training Epoch 4  61.4% | batch:       183 of       298\t|\tloss: 0.302533\n",
      "Training Epoch 4  61.7% | batch:       184 of       298\t|\tloss: 0.367063\n",
      "Training Epoch 4  62.1% | batch:       185 of       298\t|\tloss: 0.295425\n",
      "Training Epoch 4  62.4% | batch:       186 of       298\t|\tloss: 0.241878\n",
      "Training Epoch 4  62.8% | batch:       187 of       298\t|\tloss: 0.363375\n",
      "Training Epoch 4  63.1% | batch:       188 of       298\t|\tloss: 0.230543\n",
      "Training Epoch 4  63.4% | batch:       189 of       298\t|\tloss: 0.28117\n",
      "Training Epoch 4  63.8% | batch:       190 of       298\t|\tloss: 0.188496\n",
      "Training Epoch 4  64.1% | batch:       191 of       298\t|\tloss: 0.225003\n",
      "Training Epoch 4  64.4% | batch:       192 of       298\t|\tloss: 0.220909\n",
      "Training Epoch 4  64.8% | batch:       193 of       298\t|\tloss: 0.484471\n",
      "Training Epoch 4  65.1% | batch:       194 of       298\t|\tloss: 0.238821\n",
      "Training Epoch 4  65.4% | batch:       195 of       298\t|\tloss: 0.284099\n",
      "Training Epoch 4  65.8% | batch:       196 of       298\t|\tloss: 0.353676\n",
      "Training Epoch 4  66.1% | batch:       197 of       298\t|\tloss: 0.197974\n",
      "Training Epoch 4  66.4% | batch:       198 of       298\t|\tloss: 0.382601\n",
      "Training Epoch 4  66.8% | batch:       199 of       298\t|\tloss: 0.227277\n",
      "Training Epoch 4  67.1% | batch:       200 of       298\t|\tloss: 0.344462\n",
      "Training Epoch 4  67.4% | batch:       201 of       298\t|\tloss: 0.319933\n",
      "Training Epoch 4  67.8% | batch:       202 of       298\t|\tloss: 0.249488\n",
      "Training Epoch 4  68.1% | batch:       203 of       298\t|\tloss: 0.1732\n",
      "Training Epoch 4  68.5% | batch:       204 of       298\t|\tloss: 0.264773\n",
      "Training Epoch 4  68.8% | batch:       205 of       298\t|\tloss: 0.269373\n",
      "Training Epoch 4  69.1% | batch:       206 of       298\t|\tloss: 0.254236\n",
      "Training Epoch 4  69.5% | batch:       207 of       298\t|\tloss: 0.592129\n",
      "Training Epoch 4  69.8% | batch:       208 of       298\t|\tloss: 0.236386\n",
      "Training Epoch 4  70.1% | batch:       209 of       298\t|\tloss: 0.189728\n",
      "Training Epoch 4  70.5% | batch:       210 of       298\t|\tloss: 0.19991\n",
      "Training Epoch 4  70.8% | batch:       211 of       298\t|\tloss: 0.258223\n",
      "Training Epoch 4  71.1% | batch:       212 of       298\t|\tloss: 0.264914\n",
      "Training Epoch 4  71.5% | batch:       213 of       298\t|\tloss: 0.208256\n",
      "Training Epoch 4  71.8% | batch:       214 of       298\t|\tloss: 0.197781\n",
      "Training Epoch 4  72.1% | batch:       215 of       298\t|\tloss: 1.54035\n",
      "Training Epoch 4  72.5% | batch:       216 of       298\t|\tloss: 0.242461\n",
      "Training Epoch 4  72.8% | batch:       217 of       298\t|\tloss: 0.174422\n",
      "Training Epoch 4  73.2% | batch:       218 of       298\t|\tloss: 0.21911\n",
      "Training Epoch 4  73.5% | batch:       219 of       298\t|\tloss: 0.365804\n",
      "Training Epoch 4  73.8% | batch:       220 of       298\t|\tloss: 0.224205\n",
      "Training Epoch 4  74.2% | batch:       221 of       298\t|\tloss: 0.205786\n",
      "Training Epoch 4  74.5% | batch:       222 of       298\t|\tloss: 0.297359\n",
      "Training Epoch 4  74.8% | batch:       223 of       298\t|\tloss: 0.187981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  75.2% | batch:       224 of       298\t|\tloss: 0.519565\n",
      "Training Epoch 4  75.5% | batch:       225 of       298\t|\tloss: 0.22076\n",
      "Training Epoch 4  75.8% | batch:       226 of       298\t|\tloss: 0.19362\n",
      "Training Epoch 4  76.2% | batch:       227 of       298\t|\tloss: 0.254066\n",
      "Training Epoch 4  76.5% | batch:       228 of       298\t|\tloss: 0.284582\n",
      "Training Epoch 4  76.8% | batch:       229 of       298\t|\tloss: 0.289846\n",
      "Training Epoch 4  77.2% | batch:       230 of       298\t|\tloss: 0.230557\n",
      "Training Epoch 4  77.5% | batch:       231 of       298\t|\tloss: 0.223497\n",
      "Training Epoch 4  77.9% | batch:       232 of       298\t|\tloss: 0.378636\n",
      "Training Epoch 4  78.2% | batch:       233 of       298\t|\tloss: 0.249605\n",
      "Training Epoch 4  78.5% | batch:       234 of       298\t|\tloss: 2.36367\n",
      "Training Epoch 4  78.9% | batch:       235 of       298\t|\tloss: 0.179184\n",
      "Training Epoch 4  79.2% | batch:       236 of       298\t|\tloss: 0.232067\n",
      "Training Epoch 4  79.5% | batch:       237 of       298\t|\tloss: 0.419116\n",
      "Training Epoch 4  79.9% | batch:       238 of       298\t|\tloss: 0.22291\n",
      "Training Epoch 4  80.2% | batch:       239 of       298\t|\tloss: 0.180895\n",
      "Training Epoch 4  80.5% | batch:       240 of       298\t|\tloss: 0.384812\n",
      "Training Epoch 4  80.9% | batch:       241 of       298\t|\tloss: 0.336957\n",
      "Training Epoch 4  81.2% | batch:       242 of       298\t|\tloss: 0.317132\n",
      "Training Epoch 4  81.5% | batch:       243 of       298\t|\tloss: 0.861697\n",
      "Training Epoch 4  81.9% | batch:       244 of       298\t|\tloss: 0.362792\n",
      "Training Epoch 4  82.2% | batch:       245 of       298\t|\tloss: 0.231772\n",
      "Training Epoch 4  82.6% | batch:       246 of       298\t|\tloss: 2.33495\n",
      "Training Epoch 4  82.9% | batch:       247 of       298\t|\tloss: 0.218715\n",
      "Training Epoch 4  83.2% | batch:       248 of       298\t|\tloss: 0.296962\n",
      "Training Epoch 4  83.6% | batch:       249 of       298\t|\tloss: 0.202891\n",
      "Training Epoch 4  83.9% | batch:       250 of       298\t|\tloss: 0.199223\n",
      "Training Epoch 4  84.2% | batch:       251 of       298\t|\tloss: 0.275694\n",
      "Training Epoch 4  84.6% | batch:       252 of       298\t|\tloss: 0.325199\n",
      "Training Epoch 4  84.9% | batch:       253 of       298\t|\tloss: 0.161594\n",
      "Training Epoch 4  85.2% | batch:       254 of       298\t|\tloss: 0.338302\n",
      "Training Epoch 4  85.6% | batch:       255 of       298\t|\tloss: 0.388333\n",
      "Training Epoch 4  85.9% | batch:       256 of       298\t|\tloss: 0.200685\n",
      "Training Epoch 4  86.2% | batch:       257 of       298\t|\tloss: 0.321178\n",
      "Training Epoch 4  86.6% | batch:       258 of       298\t|\tloss: 0.292518\n",
      "Training Epoch 4  86.9% | batch:       259 of       298\t|\tloss: 3.44325\n",
      "Training Epoch 4  87.2% | batch:       260 of       298\t|\tloss: 0.548034\n",
      "Training Epoch 4  87.6% | batch:       261 of       298\t|\tloss: 0.73124\n",
      "Training Epoch 4  87.9% | batch:       262 of       298\t|\tloss: 0.265027\n",
      "Training Epoch 4  88.3% | batch:       263 of       298\t|\tloss: 0.265766\n",
      "Training Epoch 4  88.6% | batch:       264 of       298\t|\tloss: 0.307488\n",
      "Training Epoch 4  88.9% | batch:       265 of       298\t|\tloss: 0.233422\n",
      "Training Epoch 4  89.3% | batch:       266 of       298\t|\tloss: 0.240279\n",
      "Training Epoch 4  89.6% | batch:       267 of       298\t|\tloss: 0.198815\n",
      "Training Epoch 4  89.9% | batch:       268 of       298\t|\tloss: 0.275379\n",
      "Training Epoch 4  90.3% | batch:       269 of       298\t|\tloss: 0.192723\n",
      "Training Epoch 4  90.6% | batch:       270 of       298\t|\tloss: 0.205494\n",
      "Training Epoch 4  90.9% | batch:       271 of       298\t|\tloss: 0.275216\n",
      "Training Epoch 4  91.3% | batch:       272 of       298\t|\tloss: 0.186624\n",
      "Training Epoch 4  91.6% | batch:       273 of       298\t|\tloss: 0.312375\n",
      "Training Epoch 4  91.9% | batch:       274 of       298\t|\tloss: 0.224306\n",
      "Training Epoch 4  92.3% | batch:       275 of       298\t|\tloss: 0.28163\n",
      "Training Epoch 4  92.6% | batch:       276 of       298\t|\tloss: 0.258415\n",
      "Training Epoch 4  93.0% | batch:       277 of       298\t|\tloss: 0.172021\n",
      "Training Epoch 4  93.3% | batch:       278 of       298\t|\tloss: 0.298416\n",
      "Training Epoch 4  93.6% | batch:       279 of       298\t|\tloss: 0.227149\n",
      "Training Epoch 4  94.0% | batch:       280 of       298\t|\tloss: 0.222643\n",
      "Training Epoch 4  94.3% | batch:       281 of       298\t|\tloss: 0.268693\n",
      "Training Epoch 4  94.6% | batch:       282 of       298\t|\tloss: 0.190171\n",
      "Training Epoch 4  95.0% | batch:       283 of       298\t|\tloss: 0.244078\n",
      "Training Epoch 4  95.3% | batch:       284 of       298\t|\tloss: 0.330425\n",
      "Training Epoch 4  95.6% | batch:       285 of       298\t|\tloss: 0.207715\n",
      "Training Epoch 4  96.0% | batch:       286 of       298\t|\tloss: 0.333654\n",
      "Training Epoch 4  96.3% | batch:       287 of       298\t|\tloss: 0.204517\n",
      "Training Epoch 4  96.6% | batch:       288 of       298\t|\tloss: 0.183509\n",
      "Training Epoch 4  97.0% | batch:       289 of       298\t|\tloss: 0.154604\n",
      "Training Epoch 4  97.3% | batch:       290 of       298\t|\tloss: 0.243852\n",
      "Training Epoch 4  97.7% | batch:       291 of       298\t|\tloss: 0.233829\n",
      "Training Epoch 4  98.0% | batch:       292 of       298\t|\tloss: 0.197135\n",
      "Training Epoch 4  98.3% | batch:       293 of       298\t|\tloss: 0.353447\n",
      "Training Epoch 4  98.7% | batch:       294 of       298\t|\tloss: 0.173113\n",
      "Training Epoch 4  99.0% | batch:       295 of       298\t|\tloss: 0.481945\n",
      "Training Epoch 4  99.3% | batch:       296 of       298\t|\tloss: 0.211955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:36:53,518 | INFO : Epoch 4 Training Summary: epoch: 4.000000 | loss: 0.328881 | \n",
      "2023-06-22 14:36:53,520 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.5333993434906 seconds\n",
      "\n",
      "2023-06-22 14:36:53,521 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.330676555633545 seconds\n",
      "2023-06-22 14:36:53,521 | INFO : Avg batch train. time: 0.04137810924709243 seconds\n",
      "2023-06-22 14:36:53,522 | INFO : Avg sample train. time: 0.0012933371675722199 seconds\n",
      "2023-06-22 14:36:53,523 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4  99.7% | batch:       297 of       298\t|\tloss: 0.316035\n",
      "\n",
      "Evaluating Epoch 4   0.0% | batch:         0 of        75\t|\tloss: 0.301788\n",
      "Evaluating Epoch 4   1.3% | batch:         1 of        75\t|\tloss: 0.175836\n",
      "Evaluating Epoch 4   2.7% | batch:         2 of        75\t|\tloss: 0.206766\n",
      "Evaluating Epoch 4   4.0% | batch:         3 of        75\t|\tloss: 0.187433\n",
      "Evaluating Epoch 4   5.3% | batch:         4 of        75\t|\tloss: 0.280598\n",
      "Evaluating Epoch 4   6.7% | batch:         5 of        75\t|\tloss: 0.887641\n",
      "Evaluating Epoch 4   8.0% | batch:         6 of        75\t|\tloss: 0.263469\n",
      "Evaluating Epoch 4   9.3% | batch:         7 of        75\t|\tloss: 0.227116\n",
      "Evaluating Epoch 4  10.7% | batch:         8 of        75\t|\tloss: 0.20757\n",
      "Evaluating Epoch 4  12.0% | batch:         9 of        75\t|\tloss: 0.209115\n",
      "Evaluating Epoch 4  13.3% | batch:        10 of        75\t|\tloss: 0.175525\n",
      "Evaluating Epoch 4  14.7% | batch:        11 of        75\t|\tloss: 0.162932\n",
      "Evaluating Epoch 4  16.0% | batch:        12 of        75\t|\tloss: 0.336851\n",
      "Evaluating Epoch 4  17.3% | batch:        13 of        75\t|\tloss: 0.152837\n",
      "Evaluating Epoch 4  18.7% | batch:        14 of        75\t|\tloss: 0.213329\n",
      "Evaluating Epoch 4  20.0% | batch:        15 of        75\t|\tloss: 0.196013\n",
      "Evaluating Epoch 4  21.3% | batch:        16 of        75\t|\tloss: 0.144186\n",
      "Evaluating Epoch 4  22.7% | batch:        17 of        75\t|\tloss: 0.380694\n",
      "Evaluating Epoch 4  24.0% | batch:        18 of        75\t|\tloss: 0.267508\n",
      "Evaluating Epoch 4  25.3% | batch:        19 of        75\t|\tloss: 0.1797\n",
      "Evaluating Epoch 4  26.7% | batch:        20 of        75\t|\tloss: 0.167408\n",
      "Evaluating Epoch 4  28.0% | batch:        21 of        75\t|\tloss: 0.238431\n",
      "Evaluating Epoch 4  29.3% | batch:        22 of        75\t|\tloss: 0.186025\n",
      "Evaluating Epoch 4  30.7% | batch:        23 of        75\t|\tloss: 0.324028\n",
      "Evaluating Epoch 4  32.0% | batch:        24 of        75\t|\tloss: 0.176963\n",
      "Evaluating Epoch 4  33.3% | batch:        25 of        75\t|\tloss: 1.89037\n",
      "Evaluating Epoch 4  34.7% | batch:        26 of        75\t|\tloss: 0.140732\n",
      "Evaluating Epoch 4  36.0% | batch:        27 of        75\t|\tloss: 0.187593\n",
      "Evaluating Epoch 4  37.3% | batch:        28 of        75\t|\tloss: 0.204891\n",
      "Evaluating Epoch 4  38.7% | batch:        29 of        75\t|\tloss: 0.226643\n",
      "Evaluating Epoch 4  40.0% | batch:        30 of        75\t|\tloss: 0.314993\n",
      "Evaluating Epoch 4  41.3% | batch:        31 of        75\t|\tloss: 0.264029\n",
      "Evaluating Epoch 4  42.7% | batch:        32 of        75\t|\tloss: 0.230288\n",
      "Evaluating Epoch 4  44.0% | batch:        33 of        75\t|\tloss: 0.852312\n",
      "Evaluating Epoch 4  45.3% | batch:        34 of        75\t|\tloss: 0.216379\n",
      "Evaluating Epoch 4  46.7% | batch:        35 of        75\t|\tloss: 0.122286\n",
      "Evaluating Epoch 4  48.0% | batch:        36 of        75\t|\tloss: 0.291897\n",
      "Evaluating Epoch 4  49.3% | batch:        37 of        75\t|\tloss: 0.181356\n",
      "Evaluating Epoch 4  50.7% | batch:        38 of        75\t|\tloss: 0.168551\n",
      "Evaluating Epoch 4  52.0% | batch:        39 of        75\t|\tloss: 0.143819\n",
      "Evaluating Epoch 4  53.3% | batch:        40 of        75\t|\tloss: 0.191055\n",
      "Evaluating Epoch 4  54.7% | batch:        41 of        75\t|\tloss: 0.193882\n",
      "Evaluating Epoch 4  56.0% | batch:        42 of        75\t|\tloss: 0.327595\n",
      "Evaluating Epoch 4  57.3% | batch:        43 of        75\t|\tloss: 0.154244\n",
      "Evaluating Epoch 4  58.7% | batch:        44 of        75\t|\tloss: 0.133317\n",
      "Evaluating Epoch 4  60.0% | batch:        45 of        75\t|\tloss: 0.201856\n",
      "Evaluating Epoch 4  61.3% | batch:        46 of        75\t|\tloss: 0.254168\n",
      "Evaluating Epoch 4  62.7% | batch:        47 of        75\t|\tloss: 0.158907\n",
      "Evaluating Epoch 4  64.0% | batch:        48 of        75\t|\tloss: 0.156523\n",
      "Evaluating Epoch 4  65.3% | batch:        49 of        75\t|\tloss: 0.228054\n",
      "Evaluating Epoch 4  66.7% | batch:        50 of        75\t|\tloss: 0.170401\n",
      "Evaluating Epoch 4  68.0% | batch:        51 of        75\t|\tloss: 0.379293\n",
      "Evaluating Epoch 4  69.3% | batch:        52 of        75\t|\tloss: 0.243248\n",
      "Evaluating Epoch 4  70.7% | batch:        53 of        75\t|\tloss: 0.335622\n",
      "Evaluating Epoch 4  72.0% | batch:        54 of        75\t|\tloss: 0.336516\n",
      "Evaluating Epoch 4  73.3% | batch:        55 of        75\t|\tloss: 0.132296\n",
      "Evaluating Epoch 4  74.7% | batch:        56 of        75\t|\tloss: 0.188105\n",
      "Evaluating Epoch 4  76.0% | batch:        57 of        75\t|\tloss: 0.214364\n",
      "Evaluating Epoch 4  77.3% | batch:        58 of        75\t|\tloss: 0.182112\n",
      "Evaluating Epoch 4  78.7% | batch:        59 of        75\t|\tloss: 0.649\n",
      "Evaluating Epoch 4  80.0% | batch:        60 of        75\t|\tloss: 0.156199\n",
      "Evaluating Epoch 4  81.3% | batch:        61 of        75\t|\tloss: 0.12947\n",
      "Evaluating Epoch 4  82.7% | batch:        62 of        75\t|\tloss: 0.15956\n",
      "Evaluating Epoch 4  84.0% | batch:        63 of        75\t|\tloss: 0.144545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:36:54,754 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.2295327186584473 seconds\n",
      "\n",
      "2023-06-22 14:36:54,755 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.240070641040802 seconds\n",
      "2023-06-22 14:36:54,756 | INFO : Avg batch val. time: 0.01653427521387736 seconds\n",
      "2023-06-22 14:36:54,757 | INFO : Avg sample val. time: 0.0005201638594969807 seconds\n",
      "2023-06-22 14:36:54,758 | INFO : Epoch 4 Validation Summary: epoch: 4.000000 | loss: 0.261742 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 4  85.3% | batch:        64 of        75\t|\tloss: 0.33487\n",
      "Evaluating Epoch 4  86.7% | batch:        65 of        75\t|\tloss: 0.172901\n",
      "Evaluating Epoch 4  88.0% | batch:        66 of        75\t|\tloss: 0.200612\n",
      "Evaluating Epoch 4  89.3% | batch:        67 of        75\t|\tloss: 0.234759\n",
      "Evaluating Epoch 4  90.7% | batch:        68 of        75\t|\tloss: 0.23187\n",
      "Evaluating Epoch 4  92.0% | batch:        69 of        75\t|\tloss: 0.219548\n",
      "Evaluating Epoch 4  93.3% | batch:        70 of        75\t|\tloss: 0.242654\n",
      "Evaluating Epoch 4  94.7% | batch:        71 of        75\t|\tloss: 0.298192\n",
      "Evaluating Epoch 4  96.0% | batch:        72 of        75\t|\tloss: 0.134447\n",
      "Evaluating Epoch 4  97.3% | batch:        73 of        75\t|\tloss: 0.183118\n",
      "Evaluating Epoch 4  98.7% | batch:        74 of        75\t|\tloss: 0.195597\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Epoch:   1%|          | 4/400 [00:53<1:28:03, 13.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5   0.0% | batch:         0 of       298\t|\tloss: 0.20592\n",
      "Training Epoch 5   0.3% | batch:         1 of       298\t|\tloss: 0.328299\n",
      "Training Epoch 5   0.7% | batch:         2 of       298\t|\tloss: 0.444079\n",
      "Training Epoch 5   1.0% | batch:         3 of       298\t|\tloss: 0.230073\n",
      "Training Epoch 5   1.3% | batch:         4 of       298\t|\tloss: 0.203472\n",
      "Training Epoch 5   1.7% | batch:         5 of       298\t|\tloss: 0.211755\n",
      "Training Epoch 5   2.0% | batch:         6 of       298\t|\tloss: 0.350292\n",
      "Training Epoch 5   2.3% | batch:         7 of       298\t|\tloss: 0.2487\n",
      "Training Epoch 5   2.7% | batch:         8 of       298\t|\tloss: 0.240486\n",
      "Training Epoch 5   3.0% | batch:         9 of       298\t|\tloss: 0.199285\n",
      "Training Epoch 5   3.4% | batch:        10 of       298\t|\tloss: 0.216571\n",
      "Training Epoch 5   3.7% | batch:        11 of       298\t|\tloss: 0.189944\n",
      "Training Epoch 5   4.0% | batch:        12 of       298\t|\tloss: 0.268289\n",
      "Training Epoch 5   4.4% | batch:        13 of       298\t|\tloss: 0.167945\n",
      "Training Epoch 5   4.7% | batch:        14 of       298\t|\tloss: 0.302138\n",
      "Training Epoch 5   5.0% | batch:        15 of       298\t|\tloss: 0.287251\n",
      "Training Epoch 5   5.4% | batch:        16 of       298\t|\tloss: 0.207541\n",
      "Training Epoch 5   5.7% | batch:        17 of       298\t|\tloss: 0.18448\n",
      "Training Epoch 5   6.0% | batch:        18 of       298\t|\tloss: 0.252192\n",
      "Training Epoch 5   6.4% | batch:        19 of       298\t|\tloss: 0.502446\n",
      "Training Epoch 5   6.7% | batch:        20 of       298\t|\tloss: 0.217389\n",
      "Training Epoch 5   7.0% | batch:        21 of       298\t|\tloss: 0.23471\n",
      "Training Epoch 5   7.4% | batch:        22 of       298\t|\tloss: 0.6474\n",
      "Training Epoch 5   7.7% | batch:        23 of       298\t|\tloss: 0.194711\n",
      "Training Epoch 5   8.1% | batch:        24 of       298\t|\tloss: 0.463145\n",
      "Training Epoch 5   8.4% | batch:        25 of       298\t|\tloss: 0.335976\n",
      "Training Epoch 5   8.7% | batch:        26 of       298\t|\tloss: 0.199612\n",
      "Training Epoch 5   9.1% | batch:        27 of       298\t|\tloss: 0.237361\n",
      "Training Epoch 5   9.4% | batch:        28 of       298\t|\tloss: 0.276476\n",
      "Training Epoch 5   9.7% | batch:        29 of       298\t|\tloss: 0.213133\n",
      "Training Epoch 5  10.1% | batch:        30 of       298\t|\tloss: 0.203293\n",
      "Training Epoch 5  10.4% | batch:        31 of       298\t|\tloss: 0.152361\n",
      "Training Epoch 5  10.7% | batch:        32 of       298\t|\tloss: 0.256701\n",
      "Training Epoch 5  11.1% | batch:        33 of       298\t|\tloss: 0.276945\n",
      "Training Epoch 5  11.4% | batch:        34 of       298\t|\tloss: 0.241318\n",
      "Training Epoch 5  11.7% | batch:        35 of       298\t|\tloss: 0.231621\n",
      "Training Epoch 5  12.1% | batch:        36 of       298\t|\tloss: 0.23084\n",
      "Training Epoch 5  12.4% | batch:        37 of       298\t|\tloss: 0.507307\n",
      "Training Epoch 5  12.8% | batch:        38 of       298\t|\tloss: 0.378346\n",
      "Training Epoch 5  13.1% | batch:        39 of       298\t|\tloss: 0.238944\n",
      "Training Epoch 5  13.4% | batch:        40 of       298\t|\tloss: 0.185424\n",
      "Training Epoch 5  13.8% | batch:        41 of       298\t|\tloss: 0.714976\n",
      "Training Epoch 5  14.1% | batch:        42 of       298\t|\tloss: 0.281016\n",
      "Training Epoch 5  14.4% | batch:        43 of       298\t|\tloss: 2.16314\n",
      "Training Epoch 5  14.8% | batch:        44 of       298\t|\tloss: 8.75399\n",
      "Training Epoch 5  15.1% | batch:        45 of       298\t|\tloss: 0.179911\n",
      "Training Epoch 5  15.4% | batch:        46 of       298\t|\tloss: 0.20972\n",
      "Training Epoch 5  15.8% | batch:        47 of       298\t|\tloss: 0.205655\n",
      "Training Epoch 5  16.1% | batch:        48 of       298\t|\tloss: 0.19153\n",
      "Training Epoch 5  16.4% | batch:        49 of       298\t|\tloss: 0.40302\n",
      "Training Epoch 5  16.8% | batch:        50 of       298\t|\tloss: 0.241582\n",
      "Training Epoch 5  17.1% | batch:        51 of       298\t|\tloss: 0.160362\n",
      "Training Epoch 5  17.4% | batch:        52 of       298\t|\tloss: 0.306528\n",
      "Training Epoch 5  17.8% | batch:        53 of       298\t|\tloss: 0.181571\n",
      "Training Epoch 5  18.1% | batch:        54 of       298\t|\tloss: 2.02497\n",
      "Training Epoch 5  18.5% | batch:        55 of       298\t|\tloss: 0.273756\n",
      "Training Epoch 5  18.8% | batch:        56 of       298\t|\tloss: 0.218303\n",
      "Training Epoch 5  19.1% | batch:        57 of       298\t|\tloss: 0.210085\n",
      "Training Epoch 5  19.5% | batch:        58 of       298\t|\tloss: 0.210222\n",
      "Training Epoch 5  19.8% | batch:        59 of       298\t|\tloss: 0.184156\n",
      "Training Epoch 5  20.1% | batch:        60 of       298\t|\tloss: 0.789884\n",
      "Training Epoch 5  20.5% | batch:        61 of       298\t|\tloss: 0.213088\n",
      "Training Epoch 5  20.8% | batch:        62 of       298\t|\tloss: 0.449499\n",
      "Training Epoch 5  21.1% | batch:        63 of       298\t|\tloss: 0.161177\n",
      "Training Epoch 5  21.5% | batch:        64 of       298\t|\tloss: 0.217025\n",
      "Training Epoch 5  21.8% | batch:        65 of       298\t|\tloss: 0.501651\n",
      "Training Epoch 5  22.1% | batch:        66 of       298\t|\tloss: 0.2502\n",
      "Training Epoch 5  22.5% | batch:        67 of       298\t|\tloss: 0.302644\n",
      "Training Epoch 5  22.8% | batch:        68 of       298\t|\tloss: 0.334116\n",
      "Training Epoch 5  23.2% | batch:        69 of       298\t|\tloss: 0.247253\n",
      "Training Epoch 5  23.5% | batch:        70 of       298\t|\tloss: 0.195349\n",
      "Training Epoch 5  23.8% | batch:        71 of       298\t|\tloss: 0.210652\n",
      "Training Epoch 5  24.2% | batch:        72 of       298\t|\tloss: 0.258285\n",
      "Training Epoch 5  24.5% | batch:        73 of       298\t|\tloss: 0.605157\n",
      "Training Epoch 5  24.8% | batch:        74 of       298\t|\tloss: 1.24085\n",
      "Training Epoch 5  25.2% | batch:        75 of       298\t|\tloss: 0.232887\n",
      "Training Epoch 5  25.5% | batch:        76 of       298\t|\tloss: 0.263878\n",
      "Training Epoch 5  25.8% | batch:        77 of       298\t|\tloss: 0.308806\n",
      "Training Epoch 5  26.2% | batch:        78 of       298\t|\tloss: 0.210386\n",
      "Training Epoch 5  26.5% | batch:        79 of       298\t|\tloss: 0.219014\n",
      "Training Epoch 5  26.8% | batch:        80 of       298\t|\tloss: 0.183894\n",
      "Training Epoch 5  27.2% | batch:        81 of       298\t|\tloss: 0.192669\n",
      "Training Epoch 5  27.5% | batch:        82 of       298\t|\tloss: 0.267848\n",
      "Training Epoch 5  27.9% | batch:        83 of       298\t|\tloss: 0.218545\n",
      "Training Epoch 5  28.2% | batch:        84 of       298\t|\tloss: 0.53417\n",
      "Training Epoch 5  28.5% | batch:        85 of       298\t|\tloss: 0.233994\n",
      "Training Epoch 5  28.9% | batch:        86 of       298\t|\tloss: 0.261607\n",
      "Training Epoch 5  29.2% | batch:        87 of       298\t|\tloss: 0.187817\n",
      "Training Epoch 5  29.5% | batch:        88 of       298\t|\tloss: 0.182918\n",
      "Training Epoch 5  29.9% | batch:        89 of       298\t|\tloss: 0.28371\n",
      "Training Epoch 5  30.2% | batch:        90 of       298\t|\tloss: 0.259889\n",
      "Training Epoch 5  30.5% | batch:        91 of       298\t|\tloss: 0.15861\n",
      "Training Epoch 5  30.9% | batch:        92 of       298\t|\tloss: 0.161499\n",
      "Training Epoch 5  31.2% | batch:        93 of       298\t|\tloss: 0.217552\n",
      "Training Epoch 5  31.5% | batch:        94 of       298\t|\tloss: 0.343682\n",
      "Training Epoch 5  31.9% | batch:        95 of       298\t|\tloss: 1.30582\n",
      "Training Epoch 5  32.2% | batch:        96 of       298\t|\tloss: 0.249158\n",
      "Training Epoch 5  32.6% | batch:        97 of       298\t|\tloss: 0.186258\n",
      "Training Epoch 5  32.9% | batch:        98 of       298\t|\tloss: 0.279633\n",
      "Training Epoch 5  33.2% | batch:        99 of       298\t|\tloss: 0.244587\n",
      "Training Epoch 5  33.6% | batch:       100 of       298\t|\tloss: 0.184415\n",
      "Training Epoch 5  33.9% | batch:       101 of       298\t|\tloss: 0.214823\n",
      "Training Epoch 5  34.2% | batch:       102 of       298\t|\tloss: 0.178085\n",
      "Training Epoch 5  34.6% | batch:       103 of       298\t|\tloss: 0.365574\n",
      "Training Epoch 5  34.9% | batch:       104 of       298\t|\tloss: 0.231026\n",
      "Training Epoch 5  35.2% | batch:       105 of       298\t|\tloss: 0.430446\n",
      "Training Epoch 5  35.6% | batch:       106 of       298\t|\tloss: 0.210333\n",
      "Training Epoch 5  35.9% | batch:       107 of       298\t|\tloss: 0.25053\n",
      "Training Epoch 5  36.2% | batch:       108 of       298\t|\tloss: 0.220111\n",
      "Training Epoch 5  36.6% | batch:       109 of       298\t|\tloss: 0.213817\n",
      "Training Epoch 5  36.9% | batch:       110 of       298\t|\tloss: 0.188664\n",
      "Training Epoch 5  37.2% | batch:       111 of       298\t|\tloss: 0.292087\n",
      "Training Epoch 5  37.6% | batch:       112 of       298\t|\tloss: 0.354741\n",
      "Training Epoch 5  37.9% | batch:       113 of       298\t|\tloss: 0.184686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  38.3% | batch:       114 of       298\t|\tloss: 0.169618\n",
      "Training Epoch 5  38.6% | batch:       115 of       298\t|\tloss: 3.19508\n",
      "Training Epoch 5  38.9% | batch:       116 of       298\t|\tloss: 0.166001\n",
      "Training Epoch 5  39.3% | batch:       117 of       298\t|\tloss: 0.204961\n",
      "Training Epoch 5  39.6% | batch:       118 of       298\t|\tloss: 0.194937\n",
      "Training Epoch 5  39.9% | batch:       119 of       298\t|\tloss: 0.204045\n",
      "Training Epoch 5  40.3% | batch:       120 of       298\t|\tloss: 0.193591\n",
      "Training Epoch 5  40.6% | batch:       121 of       298\t|\tloss: 0.450938\n",
      "Training Epoch 5  40.9% | batch:       122 of       298\t|\tloss: 0.311377\n",
      "Training Epoch 5  41.3% | batch:       123 of       298\t|\tloss: 0.234092\n",
      "Training Epoch 5  41.6% | batch:       124 of       298\t|\tloss: 0.238457\n",
      "Training Epoch 5  41.9% | batch:       125 of       298\t|\tloss: 0.218444\n",
      "Training Epoch 5  42.3% | batch:       126 of       298\t|\tloss: 0.178134\n",
      "Training Epoch 5  42.6% | batch:       127 of       298\t|\tloss: 0.159732\n",
      "Training Epoch 5  43.0% | batch:       128 of       298\t|\tloss: 0.202123\n",
      "Training Epoch 5  43.3% | batch:       129 of       298\t|\tloss: 0.173533\n",
      "Training Epoch 5  43.6% | batch:       130 of       298\t|\tloss: 0.289715\n",
      "Training Epoch 5  44.0% | batch:       131 of       298\t|\tloss: 0.217713\n",
      "Training Epoch 5  44.3% | batch:       132 of       298\t|\tloss: 0.243773\n",
      "Training Epoch 5  44.6% | batch:       133 of       298\t|\tloss: 0.193876\n",
      "Training Epoch 5  45.0% | batch:       134 of       298\t|\tloss: 0.205529\n",
      "Training Epoch 5  45.3% | batch:       135 of       298\t|\tloss: 0.219545\n",
      "Training Epoch 5  45.6% | batch:       136 of       298\t|\tloss: 0.203012\n",
      "Training Epoch 5  46.0% | batch:       137 of       298\t|\tloss: 0.222821\n",
      "Training Epoch 5  46.3% | batch:       138 of       298\t|\tloss: 0.259163\n",
      "Training Epoch 5  46.6% | batch:       139 of       298\t|\tloss: 0.233247\n",
      "Training Epoch 5  47.0% | batch:       140 of       298\t|\tloss: 0.219777\n",
      "Training Epoch 5  47.3% | batch:       141 of       298\t|\tloss: 0.31562\n",
      "Training Epoch 5  47.7% | batch:       142 of       298\t|\tloss: 0.265161\n",
      "Training Epoch 5  48.0% | batch:       143 of       298\t|\tloss: 0.193106\n",
      "Training Epoch 5  48.3% | batch:       144 of       298\t|\tloss: 0.478353\n",
      "Training Epoch 5  48.7% | batch:       145 of       298\t|\tloss: 0.252983\n",
      "Training Epoch 5  49.0% | batch:       146 of       298\t|\tloss: 0.190484\n",
      "Training Epoch 5  49.3% | batch:       147 of       298\t|\tloss: 0.370702\n",
      "Training Epoch 5  49.7% | batch:       148 of       298\t|\tloss: 0.228872\n",
      "Training Epoch 5  50.0% | batch:       149 of       298\t|\tloss: 0.430881\n",
      "Training Epoch 5  50.3% | batch:       150 of       298\t|\tloss: 0.255477\n",
      "Training Epoch 5  50.7% | batch:       151 of       298\t|\tloss: 0.293623\n",
      "Training Epoch 5  51.0% | batch:       152 of       298\t|\tloss: 0.310125\n",
      "Training Epoch 5  51.3% | batch:       153 of       298\t|\tloss: 0.225733\n",
      "Training Epoch 5  51.7% | batch:       154 of       298\t|\tloss: 0.245495\n",
      "Training Epoch 5  52.0% | batch:       155 of       298\t|\tloss: 0.296333\n",
      "Training Epoch 5  52.3% | batch:       156 of       298\t|\tloss: 0.172023\n",
      "Training Epoch 5  52.7% | batch:       157 of       298\t|\tloss: 0.179293\n",
      "Training Epoch 5  53.0% | batch:       158 of       298\t|\tloss: 1.44426\n",
      "Training Epoch 5  53.4% | batch:       159 of       298\t|\tloss: 0.205188\n",
      "Training Epoch 5  53.7% | batch:       160 of       298\t|\tloss: 0.243356\n",
      "Training Epoch 5  54.0% | batch:       161 of       298\t|\tloss: 0.304978\n",
      "Training Epoch 5  54.4% | batch:       162 of       298\t|\tloss: 0.228047\n",
      "Training Epoch 5  54.7% | batch:       163 of       298\t|\tloss: 0.252024\n",
      "Training Epoch 5  55.0% | batch:       164 of       298\t|\tloss: 0.360738\n",
      "Training Epoch 5  55.4% | batch:       165 of       298\t|\tloss: 0.205385\n",
      "Training Epoch 5  55.7% | batch:       166 of       298\t|\tloss: 0.236888\n",
      "Training Epoch 5  56.0% | batch:       167 of       298\t|\tloss: 0.189665\n",
      "Training Epoch 5  56.4% | batch:       168 of       298\t|\tloss: 0.226075\n",
      "Training Epoch 5  56.7% | batch:       169 of       298\t|\tloss: 0.183954\n",
      "Training Epoch 5  57.0% | batch:       170 of       298\t|\tloss: 0.275806\n",
      "Training Epoch 5  57.4% | batch:       171 of       298\t|\tloss: 1.01335\n",
      "Training Epoch 5  57.7% | batch:       172 of       298\t|\tloss: 0.298684\n",
      "Training Epoch 5  58.1% | batch:       173 of       298\t|\tloss: 0.181001\n",
      "Training Epoch 5  58.4% | batch:       174 of       298\t|\tloss: 0.221987\n",
      "Training Epoch 5  58.7% | batch:       175 of       298\t|\tloss: 0.276363\n",
      "Training Epoch 5  59.1% | batch:       176 of       298\t|\tloss: 0.286212\n",
      "Training Epoch 5  59.4% | batch:       177 of       298\t|\tloss: 0.148713\n",
      "Training Epoch 5  59.7% | batch:       178 of       298\t|\tloss: 0.148923\n",
      "Training Epoch 5  60.1% | batch:       179 of       298\t|\tloss: 0.234343\n",
      "Training Epoch 5  60.4% | batch:       180 of       298\t|\tloss: 0.223653\n",
      "Training Epoch 5  60.7% | batch:       181 of       298\t|\tloss: 0.161199\n",
      "Training Epoch 5  61.1% | batch:       182 of       298\t|\tloss: 0.211561\n",
      "Training Epoch 5  61.4% | batch:       183 of       298\t|\tloss: 0.205944\n",
      "Training Epoch 5  61.7% | batch:       184 of       298\t|\tloss: 0.217754\n",
      "Training Epoch 5  62.1% | batch:       185 of       298\t|\tloss: 0.28699\n",
      "Training Epoch 5  62.4% | batch:       186 of       298\t|\tloss: 0.251737\n",
      "Training Epoch 5  62.8% | batch:       187 of       298\t|\tloss: 0.223543\n",
      "Training Epoch 5  63.1% | batch:       188 of       298\t|\tloss: 0.204908\n",
      "Training Epoch 5  63.4% | batch:       189 of       298\t|\tloss: 0.256991\n",
      "Training Epoch 5  63.8% | batch:       190 of       298\t|\tloss: 0.722013\n",
      "Training Epoch 5  64.1% | batch:       191 of       298\t|\tloss: 0.205654\n",
      "Training Epoch 5  64.4% | batch:       192 of       298\t|\tloss: 0.4445\n",
      "Training Epoch 5  64.8% | batch:       193 of       298\t|\tloss: 0.221211\n",
      "Training Epoch 5  65.1% | batch:       194 of       298\t|\tloss: 0.213776\n",
      "Training Epoch 5  65.4% | batch:       195 of       298\t|\tloss: 0.193278\n",
      "Training Epoch 5  65.8% | batch:       196 of       298\t|\tloss: 0.216817\n",
      "Training Epoch 5  66.1% | batch:       197 of       298\t|\tloss: 0.34564\n",
      "Training Epoch 5  66.4% | batch:       198 of       298\t|\tloss: 0.261289\n",
      "Training Epoch 5  66.8% | batch:       199 of       298\t|\tloss: 0.21781\n",
      "Training Epoch 5  67.1% | batch:       200 of       298\t|\tloss: 0.24449\n",
      "Training Epoch 5  67.4% | batch:       201 of       298\t|\tloss: 0.150126\n",
      "Training Epoch 5  67.8% | batch:       202 of       298\t|\tloss: 0.253423\n",
      "Training Epoch 5  68.1% | batch:       203 of       298\t|\tloss: 0.257735\n",
      "Training Epoch 5  68.5% | batch:       204 of       298\t|\tloss: 0.194321\n",
      "Training Epoch 5  68.8% | batch:       205 of       298\t|\tloss: 0.183223\n",
      "Training Epoch 5  69.1% | batch:       206 of       298\t|\tloss: 0.323922\n",
      "Training Epoch 5  69.5% | batch:       207 of       298\t|\tloss: 0.205942\n",
      "Training Epoch 5  69.8% | batch:       208 of       298\t|\tloss: 0.168602\n",
      "Training Epoch 5  70.1% | batch:       209 of       298\t|\tloss: 0.227155\n",
      "Training Epoch 5  70.5% | batch:       210 of       298\t|\tloss: 0.250293\n",
      "Training Epoch 5  70.8% | batch:       211 of       298\t|\tloss: 1.18984\n",
      "Training Epoch 5  71.1% | batch:       212 of       298\t|\tloss: 0.20758\n",
      "Training Epoch 5  71.5% | batch:       213 of       298\t|\tloss: 0.252786\n",
      "Training Epoch 5  71.8% | batch:       214 of       298\t|\tloss: 0.32154\n",
      "Training Epoch 5  72.1% | batch:       215 of       298\t|\tloss: 0.398313\n",
      "Training Epoch 5  72.5% | batch:       216 of       298\t|\tloss: 0.200038\n",
      "Training Epoch 5  72.8% | batch:       217 of       298\t|\tloss: 0.219594\n",
      "Training Epoch 5  73.2% | batch:       218 of       298\t|\tloss: 0.37636\n",
      "Training Epoch 5  73.5% | batch:       219 of       298\t|\tloss: 0.236065\n",
      "Training Epoch 5  73.8% | batch:       220 of       298\t|\tloss: 0.237452\n",
      "Training Epoch 5  74.2% | batch:       221 of       298\t|\tloss: 0.171672\n",
      "Training Epoch 5  74.5% | batch:       222 of       298\t|\tloss: 0.304171\n",
      "Training Epoch 5  74.8% | batch:       223 of       298\t|\tloss: 0.187452\n",
      "Training Epoch 5  75.2% | batch:       224 of       298\t|\tloss: 0.240628\n",
      "Training Epoch 5  75.5% | batch:       225 of       298\t|\tloss: 0.222238\n",
      "Training Epoch 5  75.8% | batch:       226 of       298\t|\tloss: 0.339907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  76.2% | batch:       227 of       298\t|\tloss: 0.190578\n",
      "Training Epoch 5  76.5% | batch:       228 of       298\t|\tloss: 1.66122\n",
      "Training Epoch 5  76.8% | batch:       229 of       298\t|\tloss: 0.184362\n",
      "Training Epoch 5  77.2% | batch:       230 of       298\t|\tloss: 0.299724\n",
      "Training Epoch 5  77.5% | batch:       231 of       298\t|\tloss: 0.333276\n",
      "Training Epoch 5  77.9% | batch:       232 of       298\t|\tloss: 0.206621\n",
      "Training Epoch 5  78.2% | batch:       233 of       298\t|\tloss: 0.194996\n",
      "Training Epoch 5  78.5% | batch:       234 of       298\t|\tloss: 0.172641\n",
      "Training Epoch 5  78.9% | batch:       235 of       298\t|\tloss: 0.281533\n",
      "Training Epoch 5  79.2% | batch:       236 of       298\t|\tloss: 0.278307\n",
      "Training Epoch 5  79.5% | batch:       237 of       298\t|\tloss: 0.382944\n",
      "Training Epoch 5  79.9% | batch:       238 of       298\t|\tloss: 0.260896\n",
      "Training Epoch 5  80.2% | batch:       239 of       298\t|\tloss: 0.230439\n",
      "Training Epoch 5  80.5% | batch:       240 of       298\t|\tloss: 0.197749\n",
      "Training Epoch 5  80.9% | batch:       241 of       298\t|\tloss: 0.364505\n",
      "Training Epoch 5  81.2% | batch:       242 of       298\t|\tloss: 0.229088\n",
      "Training Epoch 5  81.5% | batch:       243 of       298\t|\tloss: 0.234809\n",
      "Training Epoch 5  81.9% | batch:       244 of       298\t|\tloss: 0.327515\n",
      "Training Epoch 5  82.2% | batch:       245 of       298\t|\tloss: 0.292796\n",
      "Training Epoch 5  82.6% | batch:       246 of       298\t|\tloss: 0.246857\n",
      "Training Epoch 5  82.9% | batch:       247 of       298\t|\tloss: 0.185753\n",
      "Training Epoch 5  83.2% | batch:       248 of       298\t|\tloss: 0.185992\n",
      "Training Epoch 5  83.6% | batch:       249 of       298\t|\tloss: 0.425566\n",
      "Training Epoch 5  83.9% | batch:       250 of       298\t|\tloss: 0.208512\n",
      "Training Epoch 5  84.2% | batch:       251 of       298\t|\tloss: 2.5226\n",
      "Training Epoch 5  84.6% | batch:       252 of       298\t|\tloss: 0.264702\n",
      "Training Epoch 5  84.9% | batch:       253 of       298\t|\tloss: 0.224722\n",
      "Training Epoch 5  85.2% | batch:       254 of       298\t|\tloss: 0.235293\n",
      "Training Epoch 5  85.6% | batch:       255 of       298\t|\tloss: 0.262196\n",
      "Training Epoch 5  85.9% | batch:       256 of       298\t|\tloss: 0.16044\n",
      "Training Epoch 5  86.2% | batch:       257 of       298\t|\tloss: 0.23157\n",
      "Training Epoch 5  86.6% | batch:       258 of       298\t|\tloss: 0.191358\n",
      "Training Epoch 5  86.9% | batch:       259 of       298\t|\tloss: 0.238164\n",
      "Training Epoch 5  87.2% | batch:       260 of       298\t|\tloss: 0.358582\n",
      "Training Epoch 5  87.6% | batch:       261 of       298\t|\tloss: 0.189696\n",
      "Training Epoch 5  87.9% | batch:       262 of       298\t|\tloss: 0.235888\n",
      "Training Epoch 5  88.3% | batch:       263 of       298\t|\tloss: 0.227615\n",
      "Training Epoch 5  88.6% | batch:       264 of       298\t|\tloss: 0.2144\n",
      "Training Epoch 5  88.9% | batch:       265 of       298\t|\tloss: 0.226736\n",
      "Training Epoch 5  89.3% | batch:       266 of       298\t|\tloss: 0.194352\n",
      "Training Epoch 5  89.6% | batch:       267 of       298\t|\tloss: 0.230957\n",
      "Training Epoch 5  89.9% | batch:       268 of       298\t|\tloss: 0.211467\n",
      "Training Epoch 5  90.3% | batch:       269 of       298\t|\tloss: 0.176448\n",
      "Training Epoch 5  90.6% | batch:       270 of       298\t|\tloss: 0.214179\n",
      "Training Epoch 5  90.9% | batch:       271 of       298\t|\tloss: 0.504859\n",
      "Training Epoch 5  91.3% | batch:       272 of       298\t|\tloss: 0.445341\n",
      "Training Epoch 5  91.6% | batch:       273 of       298\t|\tloss: 0.164807\n",
      "Training Epoch 5  91.9% | batch:       274 of       298\t|\tloss: 0.216345\n",
      "Training Epoch 5  92.3% | batch:       275 of       298\t|\tloss: 0.167945\n",
      "Training Epoch 5  92.6% | batch:       276 of       298\t|\tloss: 0.191794\n",
      "Training Epoch 5  93.0% | batch:       277 of       298\t|\tloss: 0.210086\n",
      "Training Epoch 5  93.3% | batch:       278 of       298\t|\tloss: 0.186079\n",
      "Training Epoch 5  93.6% | batch:       279 of       298\t|\tloss: 0.279642\n",
      "Training Epoch 5  94.0% | batch:       280 of       298\t|\tloss: 0.226361\n",
      "Training Epoch 5  94.3% | batch:       281 of       298\t|\tloss: 0.204654\n",
      "Training Epoch 5  94.6% | batch:       282 of       298\t|\tloss: 0.200094\n",
      "Training Epoch 5  95.0% | batch:       283 of       298\t|\tloss: 0.260651\n",
      "Training Epoch 5  95.3% | batch:       284 of       298\t|\tloss: 0.14582\n",
      "Training Epoch 5  95.6% | batch:       285 of       298\t|\tloss: 0.214132\n",
      "Training Epoch 5  96.0% | batch:       286 of       298\t|\tloss: 0.226842\n",
      "Training Epoch 5  96.3% | batch:       287 of       298\t|\tloss: 0.246835\n",
      "Training Epoch 5  96.6% | batch:       288 of       298\t|\tloss: 0.425739\n",
      "Training Epoch 5  97.0% | batch:       289 of       298\t|\tloss: 4.55838\n",
      "Training Epoch 5  97.3% | batch:       290 of       298\t|\tloss: 0.38772\n",
      "Training Epoch 5  97.7% | batch:       291 of       298\t|\tloss: 0.255871\n",
      "Training Epoch 5  98.0% | batch:       292 of       298\t|\tloss: 0.370489\n",
      "Training Epoch 5  98.3% | batch:       293 of       298\t|\tloss: 0.137638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:37:07,090 | INFO : Epoch 5 Training Summary: epoch: 5.000000 | loss: 0.346913 | \n",
      "2023-06-22 14:37:07,091 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.276024103164673 seconds\n",
      "\n",
      "2023-06-22 14:37:07,092 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.319746065139771 seconds\n",
      "2023-06-22 14:37:07,093 | INFO : Avg batch train. time: 0.04134142974879118 seconds\n",
      "2023-06-22 14:37:07,094 | INFO : Avg sample train. time: 0.0012921906927983817 seconds\n",
      "Training Epoch:   1%|▏         | 5/400 [01:05<1:25:21, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 5  98.7% | batch:       294 of       298\t|\tloss: 0.231846\n",
      "Training Epoch 5  99.0% | batch:       295 of       298\t|\tloss: 0.20824\n",
      "Training Epoch 5  99.3% | batch:       296 of       298\t|\tloss: 0.252757\n",
      "Training Epoch 5  99.7% | batch:       297 of       298\t|\tloss: 0.261742\n",
      "\n",
      "Training Epoch 6   0.0% | batch:         0 of       298\t|\tloss: 0.218621\n",
      "Training Epoch 6   0.3% | batch:         1 of       298\t|\tloss: 0.196486\n",
      "Training Epoch 6   0.7% | batch:         2 of       298\t|\tloss: 0.241971\n",
      "Training Epoch 6   1.0% | batch:         3 of       298\t|\tloss: 0.160663\n",
      "Training Epoch 6   1.3% | batch:         4 of       298\t|\tloss: 0.230506\n",
      "Training Epoch 6   1.7% | batch:         5 of       298\t|\tloss: 0.221683\n",
      "Training Epoch 6   2.0% | batch:         6 of       298\t|\tloss: 0.181086\n",
      "Training Epoch 6   2.3% | batch:         7 of       298\t|\tloss: 0.234168\n",
      "Training Epoch 6   2.7% | batch:         8 of       298\t|\tloss: 0.265864\n",
      "Training Epoch 6   3.0% | batch:         9 of       298\t|\tloss: 0.23937\n",
      "Training Epoch 6   3.4% | batch:        10 of       298\t|\tloss: 0.236739\n",
      "Training Epoch 6   3.7% | batch:        11 of       298\t|\tloss: 0.282604\n",
      "Training Epoch 6   4.0% | batch:        12 of       298\t|\tloss: 0.189909\n",
      "Training Epoch 6   4.4% | batch:        13 of       298\t|\tloss: 0.201913\n",
      "Training Epoch 6   4.7% | batch:        14 of       298\t|\tloss: 0.338664\n",
      "Training Epoch 6   5.0% | batch:        15 of       298\t|\tloss: 0.184423\n",
      "Training Epoch 6   5.4% | batch:        16 of       298\t|\tloss: 0.231428\n",
      "Training Epoch 6   5.7% | batch:        17 of       298\t|\tloss: 0.231076\n",
      "Training Epoch 6   6.0% | batch:        18 of       298\t|\tloss: 0.241971\n",
      "Training Epoch 6   6.4% | batch:        19 of       298\t|\tloss: 0.284343\n",
      "Training Epoch 6   6.7% | batch:        20 of       298\t|\tloss: 0.455994\n",
      "Training Epoch 6   7.0% | batch:        21 of       298\t|\tloss: 0.379216\n",
      "Training Epoch 6   7.4% | batch:        22 of       298\t|\tloss: 0.228842\n",
      "Training Epoch 6   7.7% | batch:        23 of       298\t|\tloss: 0.461458\n",
      "Training Epoch 6   8.1% | batch:        24 of       298\t|\tloss: 0.16783\n",
      "Training Epoch 6   8.4% | batch:        25 of       298\t|\tloss: 0.22872\n",
      "Training Epoch 6   8.7% | batch:        26 of       298\t|\tloss: 0.181933\n",
      "Training Epoch 6   9.1% | batch:        27 of       298\t|\tloss: 0.192304\n",
      "Training Epoch 6   9.4% | batch:        28 of       298\t|\tloss: 0.173009\n",
      "Training Epoch 6   9.7% | batch:        29 of       298\t|\tloss: 0.231692\n",
      "Training Epoch 6  10.1% | batch:        30 of       298\t|\tloss: 0.219443\n",
      "Training Epoch 6  10.4% | batch:        31 of       298\t|\tloss: 0.149504\n",
      "Training Epoch 6  10.7% | batch:        32 of       298\t|\tloss: 0.195476\n",
      "Training Epoch 6  11.1% | batch:        33 of       298\t|\tloss: 0.185325\n",
      "Training Epoch 6  11.4% | batch:        34 of       298\t|\tloss: 0.18836\n",
      "Training Epoch 6  11.7% | batch:        35 of       298\t|\tloss: 0.212766\n",
      "Training Epoch 6  12.1% | batch:        36 of       298\t|\tloss: 0.15842\n",
      "Training Epoch 6  12.4% | batch:        37 of       298\t|\tloss: 0.146967\n",
      "Training Epoch 6  12.8% | batch:        38 of       298\t|\tloss: 0.179351\n",
      "Training Epoch 6  13.1% | batch:        39 of       298\t|\tloss: 0.204913\n",
      "Training Epoch 6  13.4% | batch:        40 of       298\t|\tloss: 0.264139\n",
      "Training Epoch 6  13.8% | batch:        41 of       298\t|\tloss: 0.17725\n",
      "Training Epoch 6  14.1% | batch:        42 of       298\t|\tloss: 0.537223\n",
      "Training Epoch 6  14.4% | batch:        43 of       298\t|\tloss: 0.191954\n",
      "Training Epoch 6  14.8% | batch:        44 of       298\t|\tloss: 0.335804\n",
      "Training Epoch 6  15.1% | batch:        45 of       298\t|\tloss: 1.45172\n",
      "Training Epoch 6  15.4% | batch:        46 of       298\t|\tloss: 0.235057\n",
      "Training Epoch 6  15.8% | batch:        47 of       298\t|\tloss: 0.23928\n",
      "Training Epoch 6  16.1% | batch:        48 of       298\t|\tloss: 0.243269\n",
      "Training Epoch 6  16.4% | batch:        49 of       298\t|\tloss: 0.251927\n",
      "Training Epoch 6  16.8% | batch:        50 of       298\t|\tloss: 0.276865\n",
      "Training Epoch 6  17.1% | batch:        51 of       298\t|\tloss: 0.368959\n",
      "Training Epoch 6  17.4% | batch:        52 of       298\t|\tloss: 0.308152\n",
      "Training Epoch 6  17.8% | batch:        53 of       298\t|\tloss: 0.33192\n",
      "Training Epoch 6  18.1% | batch:        54 of       298\t|\tloss: 0.153909\n",
      "Training Epoch 6  18.5% | batch:        55 of       298\t|\tloss: 0.239527\n",
      "Training Epoch 6  18.8% | batch:        56 of       298\t|\tloss: 0.287822\n",
      "Training Epoch 6  19.1% | batch:        57 of       298\t|\tloss: 0.228699\n",
      "Training Epoch 6  19.5% | batch:        58 of       298\t|\tloss: 0.195287\n",
      "Training Epoch 6  19.8% | batch:        59 of       298\t|\tloss: 0.242674\n",
      "Training Epoch 6  20.1% | batch:        60 of       298\t|\tloss: 0.314015\n",
      "Training Epoch 6  20.5% | batch:        61 of       298\t|\tloss: 0.185815\n",
      "Training Epoch 6  20.8% | batch:        62 of       298\t|\tloss: 0.24437\n",
      "Training Epoch 6  21.1% | batch:        63 of       298\t|\tloss: 0.217547\n",
      "Training Epoch 6  21.5% | batch:        64 of       298\t|\tloss: 0.180903\n",
      "Training Epoch 6  21.8% | batch:        65 of       298\t|\tloss: 0.220255\n",
      "Training Epoch 6  22.1% | batch:        66 of       298\t|\tloss: 0.218853\n",
      "Training Epoch 6  22.5% | batch:        67 of       298\t|\tloss: 0.189494\n",
      "Training Epoch 6  22.8% | batch:        68 of       298\t|\tloss: 0.213467\n",
      "Training Epoch 6  23.2% | batch:        69 of       298\t|\tloss: 0.203604\n",
      "Training Epoch 6  23.5% | batch:        70 of       298\t|\tloss: 0.342002\n",
      "Training Epoch 6  23.8% | batch:        71 of       298\t|\tloss: 0.212611\n",
      "Training Epoch 6  24.2% | batch:        72 of       298\t|\tloss: 0.170883\n",
      "Training Epoch 6  24.5% | batch:        73 of       298\t|\tloss: 0.205987\n",
      "Training Epoch 6  24.8% | batch:        74 of       298\t|\tloss: 0.223823\n",
      "Training Epoch 6  25.2% | batch:        75 of       298\t|\tloss: 0.242062\n",
      "Training Epoch 6  25.5% | batch:        76 of       298\t|\tloss: 0.194062\n",
      "Training Epoch 6  25.8% | batch:        77 of       298\t|\tloss: 0.928913\n",
      "Training Epoch 6  26.2% | batch:        78 of       298\t|\tloss: 1.02481\n",
      "Training Epoch 6  26.5% | batch:        79 of       298\t|\tloss: 0.312963\n",
      "Training Epoch 6  26.8% | batch:        80 of       298\t|\tloss: 0.165427\n",
      "Training Epoch 6  27.2% | batch:        81 of       298\t|\tloss: 0.804717\n",
      "Training Epoch 6  27.5% | batch:        82 of       298\t|\tloss: 0.157511\n",
      "Training Epoch 6  27.9% | batch:        83 of       298\t|\tloss: 0.27773\n",
      "Training Epoch 6  28.2% | batch:        84 of       298\t|\tloss: 0.22022\n",
      "Training Epoch 6  28.5% | batch:        85 of       298\t|\tloss: 0.170521\n",
      "Training Epoch 6  28.9% | batch:        86 of       298\t|\tloss: 0.197835\n",
      "Training Epoch 6  29.2% | batch:        87 of       298\t|\tloss: 0.240695\n",
      "Training Epoch 6  29.5% | batch:        88 of       298\t|\tloss: 0.151404\n",
      "Training Epoch 6  29.9% | batch:        89 of       298\t|\tloss: 0.16582\n",
      "Training Epoch 6  30.2% | batch:        90 of       298\t|\tloss: 0.187549\n",
      "Training Epoch 6  30.5% | batch:        91 of       298\t|\tloss: 0.17922\n",
      "Training Epoch 6  30.9% | batch:        92 of       298\t|\tloss: 0.238665\n",
      "Training Epoch 6  31.2% | batch:        93 of       298\t|\tloss: 0.583712\n",
      "Training Epoch 6  31.5% | batch:        94 of       298\t|\tloss: 0.281715\n",
      "Training Epoch 6  31.9% | batch:        95 of       298\t|\tloss: 0.299472\n",
      "Training Epoch 6  32.2% | batch:        96 of       298\t|\tloss: 0.199123\n",
      "Training Epoch 6  32.6% | batch:        97 of       298\t|\tloss: 0.228717\n",
      "Training Epoch 6  32.9% | batch:        98 of       298\t|\tloss: 0.238935\n",
      "Training Epoch 6  33.2% | batch:        99 of       298\t|\tloss: 0.19039\n",
      "Training Epoch 6  33.6% | batch:       100 of       298\t|\tloss: 0.540687\n",
      "Training Epoch 6  33.9% | batch:       101 of       298\t|\tloss: 0.201587\n",
      "Training Epoch 6  34.2% | batch:       102 of       298\t|\tloss: 0.181598\n",
      "Training Epoch 6  34.6% | batch:       103 of       298\t|\tloss: 0.173968\n",
      "Training Epoch 6  34.9% | batch:       104 of       298\t|\tloss: 0.289411\n",
      "Training Epoch 6  35.2% | batch:       105 of       298\t|\tloss: 0.426331\n",
      "Training Epoch 6  35.6% | batch:       106 of       298\t|\tloss: 0.301894\n",
      "Training Epoch 6  35.9% | batch:       107 of       298\t|\tloss: 0.188917\n",
      "Training Epoch 6  36.2% | batch:       108 of       298\t|\tloss: 0.206351\n",
      "Training Epoch 6  36.6% | batch:       109 of       298\t|\tloss: 0.200926\n",
      "Training Epoch 6  36.9% | batch:       110 of       298\t|\tloss: 0.277479\n",
      "Training Epoch 6  37.2% | batch:       111 of       298\t|\tloss: 0.200724\n",
      "Training Epoch 6  37.6% | batch:       112 of       298\t|\tloss: 0.164321\n",
      "Training Epoch 6  37.9% | batch:       113 of       298\t|\tloss: 0.215446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  38.3% | batch:       114 of       298\t|\tloss: 0.347293\n",
      "Training Epoch 6  38.6% | batch:       115 of       298\t|\tloss: 0.217038\n",
      "Training Epoch 6  38.9% | batch:       116 of       298\t|\tloss: 0.194251\n",
      "Training Epoch 6  39.3% | batch:       117 of       298\t|\tloss: 0.293132\n",
      "Training Epoch 6  39.6% | batch:       118 of       298\t|\tloss: 0.163807\n",
      "Training Epoch 6  39.9% | batch:       119 of       298\t|\tloss: 0.239559\n",
      "Training Epoch 6  40.3% | batch:       120 of       298\t|\tloss: 0.217992\n",
      "Training Epoch 6  40.6% | batch:       121 of       298\t|\tloss: 0.225045\n",
      "Training Epoch 6  40.9% | batch:       122 of       298\t|\tloss: 0.217467\n",
      "Training Epoch 6  41.3% | batch:       123 of       298\t|\tloss: 0.198244\n",
      "Training Epoch 6  41.6% | batch:       124 of       298\t|\tloss: 0.20565\n",
      "Training Epoch 6  41.9% | batch:       125 of       298\t|\tloss: 0.164773\n",
      "Training Epoch 6  42.3% | batch:       126 of       298\t|\tloss: 0.193942\n",
      "Training Epoch 6  42.6% | batch:       127 of       298\t|\tloss: 0.339107\n",
      "Training Epoch 6  43.0% | batch:       128 of       298\t|\tloss: 0.282919\n",
      "Training Epoch 6  43.3% | batch:       129 of       298\t|\tloss: 0.211857\n",
      "Training Epoch 6  43.6% | batch:       130 of       298\t|\tloss: 0.268629\n",
      "Training Epoch 6  44.0% | batch:       131 of       298\t|\tloss: 0.231897\n",
      "Training Epoch 6  44.3% | batch:       132 of       298\t|\tloss: 0.201829\n",
      "Training Epoch 6  44.6% | batch:       133 of       298\t|\tloss: 0.207838\n",
      "Training Epoch 6  45.0% | batch:       134 of       298\t|\tloss: 0.730487\n",
      "Training Epoch 6  45.3% | batch:       135 of       298\t|\tloss: 0.243602\n",
      "Training Epoch 6  45.6% | batch:       136 of       298\t|\tloss: 0.178224\n",
      "Training Epoch 6  46.0% | batch:       137 of       298\t|\tloss: 0.42552\n",
      "Training Epoch 6  46.3% | batch:       138 of       298\t|\tloss: 2.09107\n",
      "Training Epoch 6  46.6% | batch:       139 of       298\t|\tloss: 0.649401\n",
      "Training Epoch 6  47.0% | batch:       140 of       298\t|\tloss: 0.231523\n",
      "Training Epoch 6  47.3% | batch:       141 of       298\t|\tloss: 0.178742\n",
      "Training Epoch 6  47.7% | batch:       142 of       298\t|\tloss: 0.2009\n",
      "Training Epoch 6  48.0% | batch:       143 of       298\t|\tloss: 0.261424\n",
      "Training Epoch 6  48.3% | batch:       144 of       298\t|\tloss: 1.34785\n",
      "Training Epoch 6  48.7% | batch:       145 of       298\t|\tloss: 0.208351\n",
      "Training Epoch 6  49.0% | batch:       146 of       298\t|\tloss: 0.236564\n",
      "Training Epoch 6  49.3% | batch:       147 of       298\t|\tloss: 0.238016\n",
      "Training Epoch 6  49.7% | batch:       148 of       298\t|\tloss: 0.254264\n",
      "Training Epoch 6  50.0% | batch:       149 of       298\t|\tloss: 0.389302\n",
      "Training Epoch 6  50.3% | batch:       150 of       298\t|\tloss: 0.299999\n",
      "Training Epoch 6  50.7% | batch:       151 of       298\t|\tloss: 0.183389\n",
      "Training Epoch 6  51.0% | batch:       152 of       298\t|\tloss: 1.52403\n",
      "Training Epoch 6  51.3% | batch:       153 of       298\t|\tloss: 0.316969\n",
      "Training Epoch 6  51.7% | batch:       154 of       298\t|\tloss: 0.347359\n",
      "Training Epoch 6  52.0% | batch:       155 of       298\t|\tloss: 0.165323\n",
      "Training Epoch 6  52.3% | batch:       156 of       298\t|\tloss: 0.217944\n",
      "Training Epoch 6  52.7% | batch:       157 of       298\t|\tloss: 0.210729\n",
      "Training Epoch 6  53.0% | batch:       158 of       298\t|\tloss: 0.177763\n",
      "Training Epoch 6  53.4% | batch:       159 of       298\t|\tloss: 0.343515\n",
      "Training Epoch 6  53.7% | batch:       160 of       298\t|\tloss: 0.283209\n",
      "Training Epoch 6  54.0% | batch:       161 of       298\t|\tloss: 0.182162\n",
      "Training Epoch 6  54.4% | batch:       162 of       298\t|\tloss: 0.19543\n",
      "Training Epoch 6  54.7% | batch:       163 of       298\t|\tloss: 0.219137\n",
      "Training Epoch 6  55.0% | batch:       164 of       298\t|\tloss: 0.24886\n",
      "Training Epoch 6  55.4% | batch:       165 of       298\t|\tloss: 0.151956\n",
      "Training Epoch 6  55.7% | batch:       166 of       298\t|\tloss: 3.02438\n",
      "Training Epoch 6  56.0% | batch:       167 of       298\t|\tloss: 0.207628\n",
      "Training Epoch 6  56.4% | batch:       168 of       298\t|\tloss: 0.164957\n",
      "Training Epoch 6  56.7% | batch:       169 of       298\t|\tloss: 0.229603\n",
      "Training Epoch 6  57.0% | batch:       170 of       298\t|\tloss: 0.628144\n",
      "Training Epoch 6  57.4% | batch:       171 of       298\t|\tloss: 0.181666\n",
      "Training Epoch 6  57.7% | batch:       172 of       298\t|\tloss: 0.222775\n",
      "Training Epoch 6  58.1% | batch:       173 of       298\t|\tloss: 0.219849\n",
      "Training Epoch 6  58.4% | batch:       174 of       298\t|\tloss: 0.205572\n",
      "Training Epoch 6  58.7% | batch:       175 of       298\t|\tloss: 0.196731\n",
      "Training Epoch 6  59.1% | batch:       176 of       298\t|\tloss: 0.193381\n",
      "Training Epoch 6  59.4% | batch:       177 of       298\t|\tloss: 0.206927\n",
      "Training Epoch 6  59.7% | batch:       178 of       298\t|\tloss: 0.214528\n",
      "Training Epoch 6  60.1% | batch:       179 of       298\t|\tloss: 0.190531\n",
      "Training Epoch 6  60.4% | batch:       180 of       298\t|\tloss: 0.213435\n",
      "Training Epoch 6  60.7% | batch:       181 of       298\t|\tloss: 0.260044\n",
      "Training Epoch 6  61.1% | batch:       182 of       298\t|\tloss: 0.156188\n",
      "Training Epoch 6  61.4% | batch:       183 of       298\t|\tloss: 0.282012\n",
      "Training Epoch 6  61.7% | batch:       184 of       298\t|\tloss: 0.210638\n",
      "Training Epoch 6  62.1% | batch:       185 of       298\t|\tloss: 0.244772\n",
      "Training Epoch 6  62.4% | batch:       186 of       298\t|\tloss: 0.202784\n",
      "Training Epoch 6  62.8% | batch:       187 of       298\t|\tloss: 1.83981\n",
      "Training Epoch 6  63.1% | batch:       188 of       298\t|\tloss: 0.337286\n",
      "Training Epoch 6  63.4% | batch:       189 of       298\t|\tloss: 0.265023\n",
      "Training Epoch 6  63.8% | batch:       190 of       298\t|\tloss: 0.263267\n",
      "Training Epoch 6  64.1% | batch:       191 of       298\t|\tloss: 0.368589\n",
      "Training Epoch 6  64.4% | batch:       192 of       298\t|\tloss: 0.288197\n",
      "Training Epoch 6  64.8% | batch:       193 of       298\t|\tloss: 0.218629\n",
      "Training Epoch 6  65.1% | batch:       194 of       298\t|\tloss: 0.250656\n",
      "Training Epoch 6  65.4% | batch:       195 of       298\t|\tloss: 0.17223\n",
      "Training Epoch 6  65.8% | batch:       196 of       298\t|\tloss: 0.347506\n",
      "Training Epoch 6  66.1% | batch:       197 of       298\t|\tloss: 0.376033\n",
      "Training Epoch 6  66.4% | batch:       198 of       298\t|\tloss: 0.139715\n",
      "Training Epoch 6  66.8% | batch:       199 of       298\t|\tloss: 0.215327\n",
      "Training Epoch 6  67.1% | batch:       200 of       298\t|\tloss: 0.182888\n",
      "Training Epoch 6  67.4% | batch:       201 of       298\t|\tloss: 0.247611\n",
      "Training Epoch 6  67.8% | batch:       202 of       298\t|\tloss: 0.236051\n",
      "Training Epoch 6  68.1% | batch:       203 of       298\t|\tloss: 0.231916\n",
      "Training Epoch 6  68.5% | batch:       204 of       298\t|\tloss: 0.387962\n",
      "Training Epoch 6  68.8% | batch:       205 of       298\t|\tloss: 0.27192\n",
      "Training Epoch 6  69.1% | batch:       206 of       298\t|\tloss: 0.199629\n",
      "Training Epoch 6  69.5% | batch:       207 of       298\t|\tloss: 0.210087\n",
      "Training Epoch 6  69.8% | batch:       208 of       298\t|\tloss: 0.334996\n",
      "Training Epoch 6  70.1% | batch:       209 of       298\t|\tloss: 0.201454\n",
      "Training Epoch 6  70.5% | batch:       210 of       298\t|\tloss: 0.291669\n",
      "Training Epoch 6  70.8% | batch:       211 of       298\t|\tloss: 0.205979\n",
      "Training Epoch 6  71.1% | batch:       212 of       298\t|\tloss: 0.244366\n",
      "Training Epoch 6  71.5% | batch:       213 of       298\t|\tloss: 0.150945\n",
      "Training Epoch 6  71.8% | batch:       214 of       298\t|\tloss: 0.237939\n",
      "Training Epoch 6  72.1% | batch:       215 of       298\t|\tloss: 0.303282\n",
      "Training Epoch 6  72.5% | batch:       216 of       298\t|\tloss: 0.316617\n",
      "Training Epoch 6  72.8% | batch:       217 of       298\t|\tloss: 0.89254\n",
      "Training Epoch 6  73.2% | batch:       218 of       298\t|\tloss: 0.501942\n",
      "Training Epoch 6  73.5% | batch:       219 of       298\t|\tloss: 0.220877\n",
      "Training Epoch 6  73.8% | batch:       220 of       298\t|\tloss: 0.162507\n",
      "Training Epoch 6  74.2% | batch:       221 of       298\t|\tloss: 0.40341\n",
      "Training Epoch 6  74.5% | batch:       222 of       298\t|\tloss: 0.344528\n",
      "Training Epoch 6  74.8% | batch:       223 of       298\t|\tloss: 0.212361\n",
      "Training Epoch 6  75.2% | batch:       224 of       298\t|\tloss: 2.01318\n",
      "Training Epoch 6  75.5% | batch:       225 of       298\t|\tloss: 0.232518\n",
      "Training Epoch 6  75.8% | batch:       226 of       298\t|\tloss: 0.2599\n",
      "Training Epoch 6  76.2% | batch:       227 of       298\t|\tloss: 3.19256\n",
      "Training Epoch 6  76.5% | batch:       228 of       298\t|\tloss: 0.498575\n",
      "Training Epoch 6  76.8% | batch:       229 of       298\t|\tloss: 3.44034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  77.2% | batch:       230 of       298\t|\tloss: 0.344699\n",
      "Training Epoch 6  77.5% | batch:       231 of       298\t|\tloss: 0.457438\n",
      "Training Epoch 6  77.9% | batch:       232 of       298\t|\tloss: 0.190649\n",
      "Training Epoch 6  78.2% | batch:       233 of       298\t|\tloss: 0.225969\n",
      "Training Epoch 6  78.5% | batch:       234 of       298\t|\tloss: 0.253651\n",
      "Training Epoch 6  78.9% | batch:       235 of       298\t|\tloss: 0.208365\n",
      "Training Epoch 6  79.2% | batch:       236 of       298\t|\tloss: 0.253597\n",
      "Training Epoch 6  79.5% | batch:       237 of       298\t|\tloss: 0.170661\n",
      "Training Epoch 6  79.9% | batch:       238 of       298\t|\tloss: 0.239916\n",
      "Training Epoch 6  80.2% | batch:       239 of       298\t|\tloss: 0.217956\n",
      "Training Epoch 6  80.5% | batch:       240 of       298\t|\tloss: 0.221312\n",
      "Training Epoch 6  80.9% | batch:       241 of       298\t|\tloss: 0.417191\n",
      "Training Epoch 6  81.2% | batch:       242 of       298\t|\tloss: 0.211567\n",
      "Training Epoch 6  81.5% | batch:       243 of       298\t|\tloss: 0.255684\n",
      "Training Epoch 6  81.9% | batch:       244 of       298\t|\tloss: 0.247384\n",
      "Training Epoch 6  82.2% | batch:       245 of       298\t|\tloss: 0.176328\n",
      "Training Epoch 6  82.6% | batch:       246 of       298\t|\tloss: 0.441475\n",
      "Training Epoch 6  82.9% | batch:       247 of       298\t|\tloss: 0.216331\n",
      "Training Epoch 6  83.2% | batch:       248 of       298\t|\tloss: 0.222847\n",
      "Training Epoch 6  83.6% | batch:       249 of       298\t|\tloss: 0.248964\n",
      "Training Epoch 6  83.9% | batch:       250 of       298\t|\tloss: 0.285509\n",
      "Training Epoch 6  84.2% | batch:       251 of       298\t|\tloss: 0.211378\n",
      "Training Epoch 6  84.6% | batch:       252 of       298\t|\tloss: 0.179664\n",
      "Training Epoch 6  84.9% | batch:       253 of       298\t|\tloss: 0.215775\n",
      "Training Epoch 6  85.2% | batch:       254 of       298\t|\tloss: 0.24358\n",
      "Training Epoch 6  85.6% | batch:       255 of       298\t|\tloss: 0.164117\n",
      "Training Epoch 6  85.9% | batch:       256 of       298\t|\tloss: 0.157163\n",
      "Training Epoch 6  86.2% | batch:       257 of       298\t|\tloss: 0.248492\n",
      "Training Epoch 6  86.6% | batch:       258 of       298\t|\tloss: 0.195275\n",
      "Training Epoch 6  86.9% | batch:       259 of       298\t|\tloss: 0.305361\n",
      "Training Epoch 6  87.2% | batch:       260 of       298\t|\tloss: 0.309071\n",
      "Training Epoch 6  87.6% | batch:       261 of       298\t|\tloss: 0.191374\n",
      "Training Epoch 6  87.9% | batch:       262 of       298\t|\tloss: 0.404011\n",
      "Training Epoch 6  88.3% | batch:       263 of       298\t|\tloss: 0.205871\n",
      "Training Epoch 6  88.6% | batch:       264 of       298\t|\tloss: 0.262678\n",
      "Training Epoch 6  88.9% | batch:       265 of       298\t|\tloss: 0.182959\n",
      "Training Epoch 6  89.3% | batch:       266 of       298\t|\tloss: 0.258084\n",
      "Training Epoch 6  89.6% | batch:       267 of       298\t|\tloss: 0.240417\n",
      "Training Epoch 6  89.9% | batch:       268 of       298\t|\tloss: 0.216027\n",
      "Training Epoch 6  90.3% | batch:       269 of       298\t|\tloss: 0.364895\n",
      "Training Epoch 6  90.6% | batch:       270 of       298\t|\tloss: 0.167442\n",
      "Training Epoch 6  90.9% | batch:       271 of       298\t|\tloss: 0.229483\n",
      "Training Epoch 6  91.3% | batch:       272 of       298\t|\tloss: 0.371733\n",
      "Training Epoch 6  91.6% | batch:       273 of       298\t|\tloss: 0.197206\n",
      "Training Epoch 6  91.9% | batch:       274 of       298\t|\tloss: 0.361824\n",
      "Training Epoch 6  92.3% | batch:       275 of       298\t|\tloss: 0.270208\n",
      "Training Epoch 6  92.6% | batch:       276 of       298\t|\tloss: 0.208245\n",
      "Training Epoch 6  93.0% | batch:       277 of       298\t|\tloss: 0.218793\n",
      "Training Epoch 6  93.3% | batch:       278 of       298\t|\tloss: 0.362901\n",
      "Training Epoch 6  93.6% | batch:       279 of       298\t|\tloss: 0.219396\n",
      "Training Epoch 6  94.0% | batch:       280 of       298\t|\tloss: 0.221174\n",
      "Training Epoch 6  94.3% | batch:       281 of       298\t|\tloss: 0.294153\n",
      "Training Epoch 6  94.6% | batch:       282 of       298\t|\tloss: 0.285958\n",
      "Training Epoch 6  95.0% | batch:       283 of       298\t|\tloss: 0.173061\n",
      "Training Epoch 6  95.3% | batch:       284 of       298\t|\tloss: 0.162084\n",
      "Training Epoch 6  95.6% | batch:       285 of       298\t|\tloss: 0.335541\n",
      "Training Epoch 6  96.0% | batch:       286 of       298\t|\tloss: 0.352016\n",
      "Training Epoch 6  96.3% | batch:       287 of       298\t|\tloss: 0.285753\n",
      "Training Epoch 6  96.6% | batch:       288 of       298\t|\tloss: 0.213133\n",
      "Training Epoch 6  97.0% | batch:       289 of       298\t|\tloss: 0.266331\n",
      "Training Epoch 6  97.3% | batch:       290 of       298\t|\tloss: 0.265286\n",
      "Training Epoch 6  97.7% | batch:       291 of       298\t|\tloss: 0.204811\n",
      "Training Epoch 6  98.0% | batch:       292 of       298\t|\tloss: 0.186905\n",
      "Training Epoch 6  98.3% | batch:       293 of       298\t|\tloss: 0.246107\n",
      "Training Epoch 6  98.7% | batch:       294 of       298\t|\tloss: 0.13369\n",
      "Training Epoch 6  99.0% | batch:       295 of       298\t|\tloss: 0.379902\n",
      "Training Epoch 6  99.3% | batch:       296 of       298\t|\tloss: 0.378563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:37:19,334 | INFO : Epoch 6 Training Summary: epoch: 6.000000 | loss: 0.318625 | \n",
      "2023-06-22 14:37:19,336 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.221484899520874 seconds\n",
      "\n",
      "2023-06-22 14:37:19,337 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.303369204203287 seconds\n",
      "2023-06-22 14:37:19,338 | INFO : Avg batch train. time: 0.041286473839608345 seconds\n",
      "2023-06-22 14:37:19,339 | INFO : Avg sample train. time: 0.0012904729603737453 seconds\n",
      "2023-06-22 14:37:19,339 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 6  99.7% | batch:       297 of       298\t|\tloss: 0.196057\n",
      "\n",
      "Evaluating Epoch 6   0.0% | batch:         0 of        75\t|\tloss: 0.190291\n",
      "Evaluating Epoch 6   1.3% | batch:         1 of        75\t|\tloss: 0.158777\n",
      "Evaluating Epoch 6   2.7% | batch:         2 of        75\t|\tloss: 0.137948\n",
      "Evaluating Epoch 6   4.0% | batch:         3 of        75\t|\tloss: 0.134823\n",
      "Evaluating Epoch 6   5.3% | batch:         4 of        75\t|\tloss: 0.16636\n",
      "Evaluating Epoch 6   6.7% | batch:         5 of        75\t|\tloss: 0.163638\n",
      "Evaluating Epoch 6   8.0% | batch:         6 of        75\t|\tloss: 0.201189\n",
      "Evaluating Epoch 6   9.3% | batch:         7 of        75\t|\tloss: 0.169551\n",
      "Evaluating Epoch 6  10.7% | batch:         8 of        75\t|\tloss: 0.135026\n",
      "Evaluating Epoch 6  12.0% | batch:         9 of        75\t|\tloss: 0.1724\n",
      "Evaluating Epoch 6  13.3% | batch:        10 of        75\t|\tloss: 0.210595\n",
      "Evaluating Epoch 6  14.7% | batch:        11 of        75\t|\tloss: 0.142871\n",
      "Evaluating Epoch 6  16.0% | batch:        12 of        75\t|\tloss: 0.189964\n",
      "Evaluating Epoch 6  17.3% | batch:        13 of        75\t|\tloss: 0.169795\n",
      "Evaluating Epoch 6  18.7% | batch:        14 of        75\t|\tloss: 0.169969\n",
      "Evaluating Epoch 6  20.0% | batch:        15 of        75\t|\tloss: 0.286076\n",
      "Evaluating Epoch 6  21.3% | batch:        16 of        75\t|\tloss: 0.197348\n",
      "Evaluating Epoch 6  22.7% | batch:        17 of        75\t|\tloss: 0.162598\n",
      "Evaluating Epoch 6  24.0% | batch:        18 of        75\t|\tloss: 0.147053\n",
      "Evaluating Epoch 6  25.3% | batch:        19 of        75\t|\tloss: 0.570888\n",
      "Evaluating Epoch 6  26.7% | batch:        20 of        75\t|\tloss: 0.217591\n",
      "Evaluating Epoch 6  28.0% | batch:        21 of        75\t|\tloss: 0.203101\n",
      "Evaluating Epoch 6  29.3% | batch:        22 of        75\t|\tloss: 0.216314\n",
      "Evaluating Epoch 6  30.7% | batch:        23 of        75\t|\tloss: 0.169284\n",
      "Evaluating Epoch 6  32.0% | batch:        24 of        75\t|\tloss: 0.158705\n",
      "Evaluating Epoch 6  33.3% | batch:        25 of        75\t|\tloss: 0.173725\n",
      "Evaluating Epoch 6  34.7% | batch:        26 of        75\t|\tloss: 0.160148\n",
      "Evaluating Epoch 6  36.0% | batch:        27 of        75\t|\tloss: 0.174367\n",
      "Evaluating Epoch 6  37.3% | batch:        28 of        75\t|\tloss: 0.205401\n",
      "Evaluating Epoch 6  38.7% | batch:        29 of        75\t|\tloss: 0.257914\n",
      "Evaluating Epoch 6  40.0% | batch:        30 of        75\t|\tloss: 0.337608\n",
      "Evaluating Epoch 6  41.3% | batch:        31 of        75\t|\tloss: 0.228942\n",
      "Evaluating Epoch 6  42.7% | batch:        32 of        75\t|\tloss: 0.695783\n",
      "Evaluating Epoch 6  44.0% | batch:        33 of        75\t|\tloss: 0.189867\n",
      "Evaluating Epoch 6  45.3% | batch:        34 of        75\t|\tloss: 0.195753\n",
      "Evaluating Epoch 6  46.7% | batch:        35 of        75\t|\tloss: 0.566214\n",
      "Evaluating Epoch 6  48.0% | batch:        36 of        75\t|\tloss: 0.147637\n",
      "Evaluating Epoch 6  49.3% | batch:        37 of        75\t|\tloss: 1.20542\n",
      "Evaluating Epoch 6  50.7% | batch:        38 of        75\t|\tloss: 0.24653\n",
      "Evaluating Epoch 6  52.0% | batch:        39 of        75\t|\tloss: 0.200797\n",
      "Evaluating Epoch 6  53.3% | batch:        40 of        75\t|\tloss: 1.1853\n",
      "Evaluating Epoch 6  54.7% | batch:        41 of        75\t|\tloss: 0.125704\n",
      "Evaluating Epoch 6  56.0% | batch:        42 of        75\t|\tloss: 2.26776\n",
      "Evaluating Epoch 6  57.3% | batch:        43 of        75\t|\tloss: 0.19703\n",
      "Evaluating Epoch 6  58.7% | batch:        44 of        75\t|\tloss: 0.214425\n",
      "Evaluating Epoch 6  60.0% | batch:        45 of        75\t|\tloss: 0.203875\n",
      "Evaluating Epoch 6  61.3% | batch:        46 of        75\t|\tloss: 0.233568\n",
      "Evaluating Epoch 6  62.7% | batch:        47 of        75\t|\tloss: 0.172183\n",
      "Evaluating Epoch 6  64.0% | batch:        48 of        75\t|\tloss: 0.225918\n",
      "Evaluating Epoch 6  65.3% | batch:        49 of        75\t|\tloss: 0.226806\n",
      "Evaluating Epoch 6  66.7% | batch:        50 of        75\t|\tloss: 0.204282\n",
      "Evaluating Epoch 6  68.0% | batch:        51 of        75\t|\tloss: 0.147007\n",
      "Evaluating Epoch 6  69.3% | batch:        52 of        75\t|\tloss: 0.132569\n",
      "Evaluating Epoch 6  70.7% | batch:        53 of        75\t|\tloss: 0.165768\n",
      "Evaluating Epoch 6  72.0% | batch:        54 of        75\t|\tloss: 0.166387\n",
      "Evaluating Epoch 6  73.3% | batch:        55 of        75\t|\tloss: 0.893906\n",
      "Evaluating Epoch 6  74.7% | batch:        56 of        75\t|\tloss: 0.169851\n",
      "Evaluating Epoch 6  76.0% | batch:        57 of        75\t|\tloss: 0.195209\n",
      "Evaluating Epoch 6  77.3% | batch:        58 of        75\t|\tloss: 0.183477\n",
      "Evaluating Epoch 6  78.7% | batch:        59 of        75\t|\tloss: 7.09679\n",
      "Evaluating Epoch 6  80.0% | batch:        60 of        75\t|\tloss: 0.134003\n",
      "Evaluating Epoch 6  81.3% | batch:        61 of        75\t|\tloss: 0.159494\n",
      "Evaluating Epoch 6  82.7% | batch:        62 of        75\t|\tloss: 0.156264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:37:20,544 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.203620433807373 seconds\n",
      "\n",
      "2023-06-22 14:37:20,545 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.2327805995941161 seconds\n",
      "2023-06-22 14:37:20,546 | INFO : Avg batch val. time: 0.01643707466125488 seconds\n",
      "2023-06-22 14:37:20,546 | INFO : Avg sample val. time: 0.0005171059562055856 seconds\n",
      "2023-06-22 14:37:20,547 | INFO : Epoch 6 Validation Summary: epoch: 6.000000 | loss: 0.369616 | \n",
      "Training Epoch:   2%|▏         | 6/400 [01:19<1:26:13, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 6  84.0% | batch:        63 of        75\t|\tloss: 0.229898\n",
      "Evaluating Epoch 6  85.3% | batch:        64 of        75\t|\tloss: 0.243926\n",
      "Evaluating Epoch 6  86.7% | batch:        65 of        75\t|\tloss: 0.245487\n",
      "Evaluating Epoch 6  88.0% | batch:        66 of        75\t|\tloss: 0.172253\n",
      "Evaluating Epoch 6  89.3% | batch:        67 of        75\t|\tloss: 0.17313\n",
      "Evaluating Epoch 6  90.7% | batch:        68 of        75\t|\tloss: 0.160164\n",
      "Evaluating Epoch 6  92.0% | batch:        69 of        75\t|\tloss: 0.171194\n",
      "Evaluating Epoch 6  93.3% | batch:        70 of        75\t|\tloss: 0.219519\n",
      "Evaluating Epoch 6  94.7% | batch:        71 of        75\t|\tloss: 0.18957\n",
      "Evaluating Epoch 6  96.0% | batch:        72 of        75\t|\tloss: 0.168218\n",
      "Evaluating Epoch 6  97.3% | batch:        73 of        75\t|\tloss: 0.382789\n",
      "Evaluating Epoch 6  98.7% | batch:        74 of        75\t|\tloss: 0.371724\n",
      "\n",
      "Training Epoch 7   0.0% | batch:         0 of       298\t|\tloss: 0.203903\n",
      "Training Epoch 7   0.3% | batch:         1 of       298\t|\tloss: 0.210465\n",
      "Training Epoch 7   0.7% | batch:         2 of       298\t|\tloss: 0.184414\n",
      "Training Epoch 7   1.0% | batch:         3 of       298\t|\tloss: 0.238873\n",
      "Training Epoch 7   1.3% | batch:         4 of       298\t|\tloss: 0.154048\n",
      "Training Epoch 7   1.7% | batch:         5 of       298\t|\tloss: 0.150461\n",
      "Training Epoch 7   2.0% | batch:         6 of       298\t|\tloss: 0.281451\n",
      "Training Epoch 7   2.3% | batch:         7 of       298\t|\tloss: 0.613666\n",
      "Training Epoch 7   2.7% | batch:         8 of       298\t|\tloss: 0.242642\n",
      "Training Epoch 7   3.0% | batch:         9 of       298\t|\tloss: 0.177149\n",
      "Training Epoch 7   3.4% | batch:        10 of       298\t|\tloss: 0.17992\n",
      "Training Epoch 7   3.7% | batch:        11 of       298\t|\tloss: 0.288803\n",
      "Training Epoch 7   4.0% | batch:        12 of       298\t|\tloss: 0.23618\n",
      "Training Epoch 7   4.4% | batch:        13 of       298\t|\tloss: 0.206576\n",
      "Training Epoch 7   4.7% | batch:        14 of       298\t|\tloss: 0.545827\n",
      "Training Epoch 7   5.0% | batch:        15 of       298\t|\tloss: 0.202824\n",
      "Training Epoch 7   5.4% | batch:        16 of       298\t|\tloss: 0.184698\n",
      "Training Epoch 7   5.7% | batch:        17 of       298\t|\tloss: 0.142164\n",
      "Training Epoch 7   6.0% | batch:        18 of       298\t|\tloss: 0.22478\n",
      "Training Epoch 7   6.4% | batch:        19 of       298\t|\tloss: 0.243943\n",
      "Training Epoch 7   6.7% | batch:        20 of       298\t|\tloss: 0.491873\n",
      "Training Epoch 7   7.0% | batch:        21 of       298\t|\tloss: 0.194275\n",
      "Training Epoch 7   7.4% | batch:        22 of       298\t|\tloss: 0.181254\n",
      "Training Epoch 7   7.7% | batch:        23 of       298\t|\tloss: 0.216841\n",
      "Training Epoch 7   8.1% | batch:        24 of       298\t|\tloss: 0.2485\n",
      "Training Epoch 7   8.4% | batch:        25 of       298\t|\tloss: 0.237339\n",
      "Training Epoch 7   8.7% | batch:        26 of       298\t|\tloss: 0.211558\n",
      "Training Epoch 7   9.1% | batch:        27 of       298\t|\tloss: 0.342342\n",
      "Training Epoch 7   9.4% | batch:        28 of       298\t|\tloss: 0.235457\n",
      "Training Epoch 7   9.7% | batch:        29 of       298\t|\tloss: 0.41365\n",
      "Training Epoch 7  10.1% | batch:        30 of       298\t|\tloss: 0.348818\n",
      "Training Epoch 7  10.4% | batch:        31 of       298\t|\tloss: 0.316866\n",
      "Training Epoch 7  10.7% | batch:        32 of       298\t|\tloss: 0.290892\n",
      "Training Epoch 7  11.1% | batch:        33 of       298\t|\tloss: 0.581749\n",
      "Training Epoch 7  11.4% | batch:        34 of       298\t|\tloss: 0.157109\n",
      "Training Epoch 7  11.7% | batch:        35 of       298\t|\tloss: 0.17404\n",
      "Training Epoch 7  12.1% | batch:        36 of       298\t|\tloss: 0.220156\n",
      "Training Epoch 7  12.4% | batch:        37 of       298\t|\tloss: 0.198257\n",
      "Training Epoch 7  12.8% | batch:        38 of       298\t|\tloss: 0.173317\n",
      "Training Epoch 7  13.1% | batch:        39 of       298\t|\tloss: 0.187478\n",
      "Training Epoch 7  13.4% | batch:        40 of       298\t|\tloss: 0.25013\n",
      "Training Epoch 7  13.8% | batch:        41 of       298\t|\tloss: 0.205976\n",
      "Training Epoch 7  14.1% | batch:        42 of       298\t|\tloss: 0.178417\n",
      "Training Epoch 7  14.4% | batch:        43 of       298\t|\tloss: 0.149023\n",
      "Training Epoch 7  14.8% | batch:        44 of       298\t|\tloss: 0.159298\n",
      "Training Epoch 7  15.1% | batch:        45 of       298\t|\tloss: 0.236493\n",
      "Training Epoch 7  15.4% | batch:        46 of       298\t|\tloss: 0.27406\n",
      "Training Epoch 7  15.8% | batch:        47 of       298\t|\tloss: 0.225025\n",
      "Training Epoch 7  16.1% | batch:        48 of       298\t|\tloss: 0.629618\n",
      "Training Epoch 7  16.4% | batch:        49 of       298\t|\tloss: 0.375629\n",
      "Training Epoch 7  16.8% | batch:        50 of       298\t|\tloss: 0.231807\n",
      "Training Epoch 7  17.1% | batch:        51 of       298\t|\tloss: 0.203783\n",
      "Training Epoch 7  17.4% | batch:        52 of       298\t|\tloss: 0.455193\n",
      "Training Epoch 7  17.8% | batch:        53 of       298\t|\tloss: 7.91389\n",
      "Training Epoch 7  18.1% | batch:        54 of       298\t|\tloss: 0.17544\n",
      "Training Epoch 7  18.5% | batch:        55 of       298\t|\tloss: 0.2291\n",
      "Training Epoch 7  18.8% | batch:        56 of       298\t|\tloss: 0.291004\n",
      "Training Epoch 7  19.1% | batch:        57 of       298\t|\tloss: 0.184338\n",
      "Training Epoch 7  19.5% | batch:        58 of       298\t|\tloss: 0.159173\n",
      "Training Epoch 7  19.8% | batch:        59 of       298\t|\tloss: 0.203395\n",
      "Training Epoch 7  20.1% | batch:        60 of       298\t|\tloss: 1.13164\n",
      "Training Epoch 7  20.5% | batch:        61 of       298\t|\tloss: 0.247279\n",
      "Training Epoch 7  20.8% | batch:        62 of       298\t|\tloss: 1.94254\n",
      "Training Epoch 7  21.1% | batch:        63 of       298\t|\tloss: 0.151889\n",
      "Training Epoch 7  21.5% | batch:        64 of       298\t|\tloss: 0.220892\n",
      "Training Epoch 7  21.8% | batch:        65 of       298\t|\tloss: 0.33413\n",
      "Training Epoch 7  22.1% | batch:        66 of       298\t|\tloss: 0.286717\n",
      "Training Epoch 7  22.5% | batch:        67 of       298\t|\tloss: 1.90224\n",
      "Training Epoch 7  22.8% | batch:        68 of       298\t|\tloss: 0.203767\n",
      "Training Epoch 7  23.2% | batch:        69 of       298\t|\tloss: 0.166805\n",
      "Training Epoch 7  23.5% | batch:        70 of       298\t|\tloss: 0.45598\n",
      "Training Epoch 7  23.8% | batch:        71 of       298\t|\tloss: 0.188067\n",
      "Training Epoch 7  24.2% | batch:        72 of       298\t|\tloss: 0.157768\n",
      "Training Epoch 7  24.5% | batch:        73 of       298\t|\tloss: 0.262106\n",
      "Training Epoch 7  24.8% | batch:        74 of       298\t|\tloss: 0.340192\n",
      "Training Epoch 7  25.2% | batch:        75 of       298\t|\tloss: 0.212703\n",
      "Training Epoch 7  25.5% | batch:        76 of       298\t|\tloss: 0.226256\n",
      "Training Epoch 7  25.8% | batch:        77 of       298\t|\tloss: 0.253245\n",
      "Training Epoch 7  26.2% | batch:        78 of       298\t|\tloss: 0.237871\n",
      "Training Epoch 7  26.5% | batch:        79 of       298\t|\tloss: 0.226364\n",
      "Training Epoch 7  26.8% | batch:        80 of       298\t|\tloss: 0.268818\n",
      "Training Epoch 7  27.2% | batch:        81 of       298\t|\tloss: 0.182672\n",
      "Training Epoch 7  27.5% | batch:        82 of       298\t|\tloss: 0.204798\n",
      "Training Epoch 7  27.9% | batch:        83 of       298\t|\tloss: 0.259607\n",
      "Training Epoch 7  28.2% | batch:        84 of       298\t|\tloss: 0.263338\n",
      "Training Epoch 7  28.5% | batch:        85 of       298\t|\tloss: 0.264834\n",
      "Training Epoch 7  28.9% | batch:        86 of       298\t|\tloss: 0.192396\n",
      "Training Epoch 7  29.2% | batch:        87 of       298\t|\tloss: 0.199294\n",
      "Training Epoch 7  29.5% | batch:        88 of       298\t|\tloss: 0.206973\n",
      "Training Epoch 7  29.9% | batch:        89 of       298\t|\tloss: 0.230734\n",
      "Training Epoch 7  30.2% | batch:        90 of       298\t|\tloss: 0.204435\n",
      "Training Epoch 7  30.5% | batch:        91 of       298\t|\tloss: 0.338416\n",
      "Training Epoch 7  30.9% | batch:        92 of       298\t|\tloss: 0.255953\n",
      "Training Epoch 7  31.2% | batch:        93 of       298\t|\tloss: 0.234698\n",
      "Training Epoch 7  31.5% | batch:        94 of       298\t|\tloss: 0.228841\n",
      "Training Epoch 7  31.9% | batch:        95 of       298\t|\tloss: 0.169136\n",
      "Training Epoch 7  32.2% | batch:        96 of       298\t|\tloss: 0.285514\n",
      "Training Epoch 7  32.6% | batch:        97 of       298\t|\tloss: 0.215121\n",
      "Training Epoch 7  32.9% | batch:        98 of       298\t|\tloss: 0.223095\n",
      "Training Epoch 7  33.2% | batch:        99 of       298\t|\tloss: 0.308066\n",
      "Training Epoch 7  33.6% | batch:       100 of       298\t|\tloss: 0.229003\n",
      "Training Epoch 7  33.9% | batch:       101 of       298\t|\tloss: 0.212256\n",
      "Training Epoch 7  34.2% | batch:       102 of       298\t|\tloss: 0.244675\n",
      "Training Epoch 7  34.6% | batch:       103 of       298\t|\tloss: 0.208716\n",
      "Training Epoch 7  34.9% | batch:       104 of       298\t|\tloss: 0.228559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  35.2% | batch:       105 of       298\t|\tloss: 0.198161\n",
      "Training Epoch 7  35.6% | batch:       106 of       298\t|\tloss: 0.207965\n",
      "Training Epoch 7  35.9% | batch:       107 of       298\t|\tloss: 0.242525\n",
      "Training Epoch 7  36.2% | batch:       108 of       298\t|\tloss: 0.193\n",
      "Training Epoch 7  36.6% | batch:       109 of       298\t|\tloss: 0.251719\n",
      "Training Epoch 7  36.9% | batch:       110 of       298\t|\tloss: 0.202894\n",
      "Training Epoch 7  37.2% | batch:       111 of       298\t|\tloss: 0.218785\n",
      "Training Epoch 7  37.6% | batch:       112 of       298\t|\tloss: 0.256165\n",
      "Training Epoch 7  37.9% | batch:       113 of       298\t|\tloss: 0.24058\n",
      "Training Epoch 7  38.3% | batch:       114 of       298\t|\tloss: 0.368052\n",
      "Training Epoch 7  38.6% | batch:       115 of       298\t|\tloss: 0.269781\n",
      "Training Epoch 7  38.9% | batch:       116 of       298\t|\tloss: 0.247392\n",
      "Training Epoch 7  39.3% | batch:       117 of       298\t|\tloss: 0.161102\n",
      "Training Epoch 7  39.6% | batch:       118 of       298\t|\tloss: 0.899082\n",
      "Training Epoch 7  39.9% | batch:       119 of       298\t|\tloss: 0.20799\n",
      "Training Epoch 7  40.3% | batch:       120 of       298\t|\tloss: 0.216897\n",
      "Training Epoch 7  40.6% | batch:       121 of       298\t|\tloss: 0.192039\n",
      "Training Epoch 7  40.9% | batch:       122 of       298\t|\tloss: 0.234547\n",
      "Training Epoch 7  41.3% | batch:       123 of       298\t|\tloss: 0.298157\n",
      "Training Epoch 7  41.6% | batch:       124 of       298\t|\tloss: 0.249968\n",
      "Training Epoch 7  41.9% | batch:       125 of       298\t|\tloss: 0.236122\n",
      "Training Epoch 7  42.3% | batch:       126 of       298\t|\tloss: 0.152073\n",
      "Training Epoch 7  42.6% | batch:       127 of       298\t|\tloss: 0.185254\n",
      "Training Epoch 7  43.0% | batch:       128 of       298\t|\tloss: 0.254921\n",
      "Training Epoch 7  43.3% | batch:       129 of       298\t|\tloss: 0.314669\n",
      "Training Epoch 7  43.6% | batch:       130 of       298\t|\tloss: 0.178577\n",
      "Training Epoch 7  44.0% | batch:       131 of       298\t|\tloss: 0.197662\n",
      "Training Epoch 7  44.3% | batch:       132 of       298\t|\tloss: 0.228329\n",
      "Training Epoch 7  44.6% | batch:       133 of       298\t|\tloss: 0.248963\n",
      "Training Epoch 7  45.0% | batch:       134 of       298\t|\tloss: 0.417333\n",
      "Training Epoch 7  45.3% | batch:       135 of       298\t|\tloss: 0.208769\n",
      "Training Epoch 7  45.6% | batch:       136 of       298\t|\tloss: 0.198244\n",
      "Training Epoch 7  46.0% | batch:       137 of       298\t|\tloss: 0.174519\n",
      "Training Epoch 7  46.3% | batch:       138 of       298\t|\tloss: 1.2123\n",
      "Training Epoch 7  46.6% | batch:       139 of       298\t|\tloss: 0.24152\n",
      "Training Epoch 7  47.0% | batch:       140 of       298\t|\tloss: 0.191327\n",
      "Training Epoch 7  47.3% | batch:       141 of       298\t|\tloss: 0.169785\n",
      "Training Epoch 7  47.7% | batch:       142 of       298\t|\tloss: 1.05936\n",
      "Training Epoch 7  48.0% | batch:       143 of       298\t|\tloss: 2.23785\n",
      "Training Epoch 7  48.3% | batch:       144 of       298\t|\tloss: 0.433177\n",
      "Training Epoch 7  48.7% | batch:       145 of       298\t|\tloss: 0.204967\n",
      "Training Epoch 7  49.0% | batch:       146 of       298\t|\tloss: 0.20607\n",
      "Training Epoch 7  49.3% | batch:       147 of       298\t|\tloss: 0.237795\n",
      "Training Epoch 7  49.7% | batch:       148 of       298\t|\tloss: 0.212854\n",
      "Training Epoch 7  50.0% | batch:       149 of       298\t|\tloss: 0.19701\n",
      "Training Epoch 7  50.3% | batch:       150 of       298\t|\tloss: 0.166065\n",
      "Training Epoch 7  50.7% | batch:       151 of       298\t|\tloss: 0.484896\n",
      "Training Epoch 7  51.0% | batch:       152 of       298\t|\tloss: 0.155839\n",
      "Training Epoch 7  51.3% | batch:       153 of       298\t|\tloss: 0.531749\n",
      "Training Epoch 7  51.7% | batch:       154 of       298\t|\tloss: 0.2931\n",
      "Training Epoch 7  52.0% | batch:       155 of       298\t|\tloss: 0.200495\n",
      "Training Epoch 7  52.3% | batch:       156 of       298\t|\tloss: 0.177976\n",
      "Training Epoch 7  52.7% | batch:       157 of       298\t|\tloss: 0.183676\n",
      "Training Epoch 7  53.0% | batch:       158 of       298\t|\tloss: 0.212222\n",
      "Training Epoch 7  53.4% | batch:       159 of       298\t|\tloss: 0.225124\n",
      "Training Epoch 7  53.7% | batch:       160 of       298\t|\tloss: 0.242137\n",
      "Training Epoch 7  54.0% | batch:       161 of       298\t|\tloss: 0.196208\n",
      "Training Epoch 7  54.4% | batch:       162 of       298\t|\tloss: 0.279507\n",
      "Training Epoch 7  54.7% | batch:       163 of       298\t|\tloss: 0.413481\n",
      "Training Epoch 7  55.0% | batch:       164 of       298\t|\tloss: 0.339977\n",
      "Training Epoch 7  55.4% | batch:       165 of       298\t|\tloss: 0.220213\n",
      "Training Epoch 7  55.7% | batch:       166 of       298\t|\tloss: 0.280805\n",
      "Training Epoch 7  56.0% | batch:       167 of       298\t|\tloss: 0.427786\n",
      "Training Epoch 7  56.4% | batch:       168 of       298\t|\tloss: 0.179358\n",
      "Training Epoch 7  56.7% | batch:       169 of       298\t|\tloss: 0.234591\n",
      "Training Epoch 7  57.0% | batch:       170 of       298\t|\tloss: 0.283981\n",
      "Training Epoch 7  57.4% | batch:       171 of       298\t|\tloss: 3.18551\n",
      "Training Epoch 7  57.7% | batch:       172 of       298\t|\tloss: 0.427565\n",
      "Training Epoch 7  58.1% | batch:       173 of       298\t|\tloss: 0.247255\n",
      "Training Epoch 7  58.4% | batch:       174 of       298\t|\tloss: 0.170545\n",
      "Training Epoch 7  58.7% | batch:       175 of       298\t|\tloss: 0.191328\n",
      "Training Epoch 7  59.1% | batch:       176 of       298\t|\tloss: 0.263249\n",
      "Training Epoch 7  59.4% | batch:       177 of       298\t|\tloss: 0.191137\n",
      "Training Epoch 7  59.7% | batch:       178 of       298\t|\tloss: 0.281736\n",
      "Training Epoch 7  60.1% | batch:       179 of       298\t|\tloss: 0.214263\n",
      "Training Epoch 7  60.4% | batch:       180 of       298\t|\tloss: 0.177088\n",
      "Training Epoch 7  60.7% | batch:       181 of       298\t|\tloss: 0.179426\n",
      "Training Epoch 7  61.1% | batch:       182 of       298\t|\tloss: 0.789155\n",
      "Training Epoch 7  61.4% | batch:       183 of       298\t|\tloss: 0.238767\n",
      "Training Epoch 7  61.7% | batch:       184 of       298\t|\tloss: 0.174875\n",
      "Training Epoch 7  62.1% | batch:       185 of       298\t|\tloss: 0.226906\n",
      "Training Epoch 7  62.4% | batch:       186 of       298\t|\tloss: 0.395424\n",
      "Training Epoch 7  62.8% | batch:       187 of       298\t|\tloss: 0.223396\n",
      "Training Epoch 7  63.1% | batch:       188 of       298\t|\tloss: 0.293323\n",
      "Training Epoch 7  63.4% | batch:       189 of       298\t|\tloss: 0.253212\n",
      "Training Epoch 7  63.8% | batch:       190 of       298\t|\tloss: 0.174124\n",
      "Training Epoch 7  64.1% | batch:       191 of       298\t|\tloss: 0.21713\n",
      "Training Epoch 7  64.4% | batch:       192 of       298\t|\tloss: 0.221208\n",
      "Training Epoch 7  64.8% | batch:       193 of       298\t|\tloss: 0.173105\n",
      "Training Epoch 7  65.1% | batch:       194 of       298\t|\tloss: 0.526979\n",
      "Training Epoch 7  65.4% | batch:       195 of       298\t|\tloss: 0.179203\n",
      "Training Epoch 7  65.8% | batch:       196 of       298\t|\tloss: 0.375802\n",
      "Training Epoch 7  66.1% | batch:       197 of       298\t|\tloss: 0.30727\n",
      "Training Epoch 7  66.4% | batch:       198 of       298\t|\tloss: 0.197562\n",
      "Training Epoch 7  66.8% | batch:       199 of       298\t|\tloss: 0.259224\n",
      "Training Epoch 7  67.1% | batch:       200 of       298\t|\tloss: 0.264868\n",
      "Training Epoch 7  67.4% | batch:       201 of       298\t|\tloss: 0.294061\n",
      "Training Epoch 7  67.8% | batch:       202 of       298\t|\tloss: 0.17484\n",
      "Training Epoch 7  68.1% | batch:       203 of       298\t|\tloss: 0.245601\n",
      "Training Epoch 7  68.5% | batch:       204 of       298\t|\tloss: 0.274592\n",
      "Training Epoch 7  68.8% | batch:       205 of       298\t|\tloss: 0.369168\n",
      "Training Epoch 7  69.1% | batch:       206 of       298\t|\tloss: 0.14784\n",
      "Training Epoch 7  69.5% | batch:       207 of       298\t|\tloss: 0.340901\n",
      "Training Epoch 7  69.8% | batch:       208 of       298\t|\tloss: 0.236465\n",
      "Training Epoch 7  70.1% | batch:       209 of       298\t|\tloss: 0.507636\n",
      "Training Epoch 7  70.5% | batch:       210 of       298\t|\tloss: 0.334314\n",
      "Training Epoch 7  70.8% | batch:       211 of       298\t|\tloss: 0.174575\n",
      "Training Epoch 7  71.1% | batch:       212 of       298\t|\tloss: 0.175077\n",
      "Training Epoch 7  71.5% | batch:       213 of       298\t|\tloss: 0.270352\n",
      "Training Epoch 7  71.8% | batch:       214 of       298\t|\tloss: 0.183209\n",
      "Training Epoch 7  72.1% | batch:       215 of       298\t|\tloss: 0.225075\n",
      "Training Epoch 7  72.5% | batch:       216 of       298\t|\tloss: 0.229605\n",
      "Training Epoch 7  72.8% | batch:       217 of       298\t|\tloss: 0.454641\n",
      "Training Epoch 7  73.2% | batch:       218 of       298\t|\tloss: 0.434574\n",
      "Training Epoch 7  73.5% | batch:       219 of       298\t|\tloss: 0.38092\n",
      "Training Epoch 7  73.8% | batch:       220 of       298\t|\tloss: 0.203351\n",
      "Training Epoch 7  74.2% | batch:       221 of       298\t|\tloss: 0.160892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  74.5% | batch:       222 of       298\t|\tloss: 0.246164\n",
      "Training Epoch 7  74.8% | batch:       223 of       298\t|\tloss: 0.242352\n",
      "Training Epoch 7  75.2% | batch:       224 of       298\t|\tloss: 0.161504\n",
      "Training Epoch 7  75.5% | batch:       225 of       298\t|\tloss: 0.396263\n",
      "Training Epoch 7  75.8% | batch:       226 of       298\t|\tloss: 0.201643\n",
      "Training Epoch 7  76.2% | batch:       227 of       298\t|\tloss: 0.140625\n",
      "Training Epoch 7  76.5% | batch:       228 of       298\t|\tloss: 0.1981\n",
      "Training Epoch 7  76.8% | batch:       229 of       298\t|\tloss: 0.245804\n",
      "Training Epoch 7  77.2% | batch:       230 of       298\t|\tloss: 0.218342\n",
      "Training Epoch 7  77.5% | batch:       231 of       298\t|\tloss: 0.17114\n",
      "Training Epoch 7  77.9% | batch:       232 of       298\t|\tloss: 0.196355\n",
      "Training Epoch 7  78.2% | batch:       233 of       298\t|\tloss: 0.162693\n",
      "Training Epoch 7  78.5% | batch:       234 of       298\t|\tloss: 0.194713\n",
      "Training Epoch 7  78.9% | batch:       235 of       298\t|\tloss: 0.250741\n",
      "Training Epoch 7  79.2% | batch:       236 of       298\t|\tloss: 0.218493\n",
      "Training Epoch 7  79.5% | batch:       237 of       298\t|\tloss: 0.232792\n",
      "Training Epoch 7  79.9% | batch:       238 of       298\t|\tloss: 0.181075\n",
      "Training Epoch 7  80.2% | batch:       239 of       298\t|\tloss: 0.251206\n",
      "Training Epoch 7  80.5% | batch:       240 of       298\t|\tloss: 0.253365\n",
      "Training Epoch 7  80.9% | batch:       241 of       298\t|\tloss: 0.22588\n",
      "Training Epoch 7  81.2% | batch:       242 of       298\t|\tloss: 0.571061\n",
      "Training Epoch 7  81.5% | batch:       243 of       298\t|\tloss: 0.224575\n",
      "Training Epoch 7  81.9% | batch:       244 of       298\t|\tloss: 0.177653\n",
      "Training Epoch 7  82.2% | batch:       245 of       298\t|\tloss: 0.205203\n",
      "Training Epoch 7  82.6% | batch:       246 of       298\t|\tloss: 0.413194\n",
      "Training Epoch 7  82.9% | batch:       247 of       298\t|\tloss: 0.516535\n",
      "Training Epoch 7  83.2% | batch:       248 of       298\t|\tloss: 0.207217\n",
      "Training Epoch 7  83.6% | batch:       249 of       298\t|\tloss: 0.216295\n",
      "Training Epoch 7  83.9% | batch:       250 of       298\t|\tloss: 0.256749\n",
      "Training Epoch 7  84.2% | batch:       251 of       298\t|\tloss: 0.187118\n",
      "Training Epoch 7  84.6% | batch:       252 of       298\t|\tloss: 0.191111\n",
      "Training Epoch 7  84.9% | batch:       253 of       298\t|\tloss: 0.175472\n",
      "Training Epoch 7  85.2% | batch:       254 of       298\t|\tloss: 0.287094\n",
      "Training Epoch 7  85.6% | batch:       255 of       298\t|\tloss: 0.219413\n",
      "Training Epoch 7  85.9% | batch:       256 of       298\t|\tloss: 0.194671\n",
      "Training Epoch 7  86.2% | batch:       257 of       298\t|\tloss: 0.203013\n",
      "Training Epoch 7  86.6% | batch:       258 of       298\t|\tloss: 0.21746\n",
      "Training Epoch 7  86.9% | batch:       259 of       298\t|\tloss: 0.270981\n",
      "Training Epoch 7  87.2% | batch:       260 of       298\t|\tloss: 0.155211\n",
      "Training Epoch 7  87.6% | batch:       261 of       298\t|\tloss: 0.216757\n",
      "Training Epoch 7  87.9% | batch:       262 of       298\t|\tloss: 0.20666\n",
      "Training Epoch 7  88.3% | batch:       263 of       298\t|\tloss: 0.318893\n",
      "Training Epoch 7  88.6% | batch:       264 of       298\t|\tloss: 0.322524\n",
      "Training Epoch 7  88.9% | batch:       265 of       298\t|\tloss: 1.29583\n",
      "Training Epoch 7  89.3% | batch:       266 of       298\t|\tloss: 0.315218\n",
      "Training Epoch 7  89.6% | batch:       267 of       298\t|\tloss: 0.342005\n",
      "Training Epoch 7  89.9% | batch:       268 of       298\t|\tloss: 0.162571\n",
      "Training Epoch 7  90.3% | batch:       269 of       298\t|\tloss: 0.227229\n",
      "Training Epoch 7  90.6% | batch:       270 of       298\t|\tloss: 0.487613\n",
      "Training Epoch 7  90.9% | batch:       271 of       298\t|\tloss: 0.266348\n",
      "Training Epoch 7  91.3% | batch:       272 of       298\t|\tloss: 0.194296\n",
      "Training Epoch 7  91.6% | batch:       273 of       298\t|\tloss: 0.180618\n",
      "Training Epoch 7  91.9% | batch:       274 of       298\t|\tloss: 0.19742\n",
      "Training Epoch 7  92.3% | batch:       275 of       298\t|\tloss: 1.46187\n",
      "Training Epoch 7  92.6% | batch:       276 of       298\t|\tloss: 0.24168\n",
      "Training Epoch 7  93.0% | batch:       277 of       298\t|\tloss: 0.223504\n",
      "Training Epoch 7  93.3% | batch:       278 of       298\t|\tloss: 0.284797\n",
      "Training Epoch 7  93.6% | batch:       279 of       298\t|\tloss: 0.504498\n",
      "Training Epoch 7  94.0% | batch:       280 of       298\t|\tloss: 0.185827\n",
      "Training Epoch 7  94.3% | batch:       281 of       298\t|\tloss: 0.271228\n",
      "Training Epoch 7  94.6% | batch:       282 of       298\t|\tloss: 0.31534\n",
      "Training Epoch 7  95.0% | batch:       283 of       298\t|\tloss: 0.166414\n",
      "Training Epoch 7  95.3% | batch:       284 of       298\t|\tloss: 0.303885\n",
      "Training Epoch 7  95.6% | batch:       285 of       298\t|\tloss: 0.351547\n",
      "Training Epoch 7  96.0% | batch:       286 of       298\t|\tloss: 0.193139\n",
      "Training Epoch 7  96.3% | batch:       287 of       298\t|\tloss: 0.201396\n",
      "Training Epoch 7  96.6% | batch:       288 of       298\t|\tloss: 0.485224\n",
      "Training Epoch 7  97.0% | batch:       289 of       298\t|\tloss: 0.34851\n",
      "Training Epoch 7  97.3% | batch:       290 of       298\t|\tloss: 0.197574\n",
      "Training Epoch 7  97.7% | batch:       291 of       298\t|\tloss: 0.283498\n",
      "Training Epoch 7  98.0% | batch:       292 of       298\t|\tloss: 0.194284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:37:32,618 | INFO : Epoch 7 Training Summary: epoch: 7.000000 | loss: 0.324510 | \n",
      "2023-06-22 14:37:32,619 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.054327249526978 seconds\n",
      "\n",
      "2023-06-22 14:37:32,620 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.267791782106672 seconds\n",
      "2023-06-22 14:37:32,621 | INFO : Avg batch train. time: 0.041167086517136485 seconds\n",
      "2023-06-22 14:37:32,621 | INFO : Avg sample train. time: 0.0012867413239046226 seconds\n",
      "Training Epoch:   2%|▏         | 7/400 [01:31<1:23:46, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 7  98.3% | batch:       293 of       298\t|\tloss: 0.218895\n",
      "Training Epoch 7  98.7% | batch:       294 of       298\t|\tloss: 0.248616\n",
      "Training Epoch 7  99.0% | batch:       295 of       298\t|\tloss: 0.188016\n",
      "Training Epoch 7  99.3% | batch:       296 of       298\t|\tloss: 0.266405\n",
      "Training Epoch 7  99.7% | batch:       297 of       298\t|\tloss: 0.142636\n",
      "\n",
      "Training Epoch 8   0.0% | batch:         0 of       298\t|\tloss: 0.230752\n",
      "Training Epoch 8   0.3% | batch:         1 of       298\t|\tloss: 0.207936\n",
      "Training Epoch 8   0.7% | batch:         2 of       298\t|\tloss: 0.182118\n",
      "Training Epoch 8   1.0% | batch:         3 of       298\t|\tloss: 0.19014\n",
      "Training Epoch 8   1.3% | batch:         4 of       298\t|\tloss: 0.25244\n",
      "Training Epoch 8   1.7% | batch:         5 of       298\t|\tloss: 0.16706\n",
      "Training Epoch 8   2.0% | batch:         6 of       298\t|\tloss: 0.275529\n",
      "Training Epoch 8   2.3% | batch:         7 of       298\t|\tloss: 0.185189\n",
      "Training Epoch 8   2.7% | batch:         8 of       298\t|\tloss: 0.221818\n",
      "Training Epoch 8   3.0% | batch:         9 of       298\t|\tloss: 0.187228\n",
      "Training Epoch 8   3.4% | batch:        10 of       298\t|\tloss: 0.615067\n",
      "Training Epoch 8   3.7% | batch:        11 of       298\t|\tloss: 0.200618\n",
      "Training Epoch 8   4.0% | batch:        12 of       298\t|\tloss: 0.188616\n",
      "Training Epoch 8   4.4% | batch:        13 of       298\t|\tloss: 0.189366\n",
      "Training Epoch 8   4.7% | batch:        14 of       298\t|\tloss: 0.237557\n",
      "Training Epoch 8   5.0% | batch:        15 of       298\t|\tloss: 0.248418\n",
      "Training Epoch 8   5.4% | batch:        16 of       298\t|\tloss: 0.144496\n",
      "Training Epoch 8   5.7% | batch:        17 of       298\t|\tloss: 0.171699\n",
      "Training Epoch 8   6.0% | batch:        18 of       298\t|\tloss: 0.292137\n",
      "Training Epoch 8   6.4% | batch:        19 of       298\t|\tloss: 0.202355\n",
      "Training Epoch 8   6.7% | batch:        20 of       298\t|\tloss: 0.43072\n",
      "Training Epoch 8   7.0% | batch:        21 of       298\t|\tloss: 0.200818\n",
      "Training Epoch 8   7.4% | batch:        22 of       298\t|\tloss: 1.01689\n",
      "Training Epoch 8   7.7% | batch:        23 of       298\t|\tloss: 0.206137\n",
      "Training Epoch 8   8.1% | batch:        24 of       298\t|\tloss: 0.224119\n",
      "Training Epoch 8   8.4% | batch:        25 of       298\t|\tloss: 0.654111\n",
      "Training Epoch 8   8.7% | batch:        26 of       298\t|\tloss: 0.20503\n",
      "Training Epoch 8   9.1% | batch:        27 of       298\t|\tloss: 0.246663\n",
      "Training Epoch 8   9.4% | batch:        28 of       298\t|\tloss: 0.218182\n",
      "Training Epoch 8   9.7% | batch:        29 of       298\t|\tloss: 0.437809\n",
      "Training Epoch 8  10.1% | batch:        30 of       298\t|\tloss: 0.209713\n",
      "Training Epoch 8  10.4% | batch:        31 of       298\t|\tloss: 0.171341\n",
      "Training Epoch 8  10.7% | batch:        32 of       298\t|\tloss: 0.161874\n",
      "Training Epoch 8  11.1% | batch:        33 of       298\t|\tloss: 0.46754\n",
      "Training Epoch 8  11.4% | batch:        34 of       298\t|\tloss: 0.214443\n",
      "Training Epoch 8  11.7% | batch:        35 of       298\t|\tloss: 0.221706\n",
      "Training Epoch 8  12.1% | batch:        36 of       298\t|\tloss: 0.192\n",
      "Training Epoch 8  12.4% | batch:        37 of       298\t|\tloss: 0.160925\n",
      "Training Epoch 8  12.8% | batch:        38 of       298\t|\tloss: 0.50848\n",
      "Training Epoch 8  13.1% | batch:        39 of       298\t|\tloss: 0.228488\n",
      "Training Epoch 8  13.4% | batch:        40 of       298\t|\tloss: 0.17685\n",
      "Training Epoch 8  13.8% | batch:        41 of       298\t|\tloss: 0.257067\n",
      "Training Epoch 8  14.1% | batch:        42 of       298\t|\tloss: 2.95091\n",
      "Training Epoch 8  14.4% | batch:        43 of       298\t|\tloss: 0.224077\n",
      "Training Epoch 8  14.8% | batch:        44 of       298\t|\tloss: 0.214945\n",
      "Training Epoch 8  15.1% | batch:        45 of       298\t|\tloss: 0.279793\n",
      "Training Epoch 8  15.4% | batch:        46 of       298\t|\tloss: 0.197449\n",
      "Training Epoch 8  15.8% | batch:        47 of       298\t|\tloss: 0.244787\n",
      "Training Epoch 8  16.1% | batch:        48 of       298\t|\tloss: 0.206883\n",
      "Training Epoch 8  16.4% | batch:        49 of       298\t|\tloss: 0.244254\n",
      "Training Epoch 8  16.8% | batch:        50 of       298\t|\tloss: 0.199676\n",
      "Training Epoch 8  17.1% | batch:        51 of       298\t|\tloss: 0.227118\n",
      "Training Epoch 8  17.4% | batch:        52 of       298\t|\tloss: 0.207184\n",
      "Training Epoch 8  17.8% | batch:        53 of       298\t|\tloss: 0.241425\n",
      "Training Epoch 8  18.1% | batch:        54 of       298\t|\tloss: 0.220562\n",
      "Training Epoch 8  18.5% | batch:        55 of       298\t|\tloss: 0.167499\n",
      "Training Epoch 8  18.8% | batch:        56 of       298\t|\tloss: 0.216495\n",
      "Training Epoch 8  19.1% | batch:        57 of       298\t|\tloss: 0.458016\n",
      "Training Epoch 8  19.5% | batch:        58 of       298\t|\tloss: 0.360744\n",
      "Training Epoch 8  19.8% | batch:        59 of       298\t|\tloss: 0.205322\n",
      "Training Epoch 8  20.1% | batch:        60 of       298\t|\tloss: 0.185754\n",
      "Training Epoch 8  20.5% | batch:        61 of       298\t|\tloss: 0.221578\n",
      "Training Epoch 8  20.8% | batch:        62 of       298\t|\tloss: 0.223241\n",
      "Training Epoch 8  21.1% | batch:        63 of       298\t|\tloss: 0.334784\n",
      "Training Epoch 8  21.5% | batch:        64 of       298\t|\tloss: 0.309767\n",
      "Training Epoch 8  21.8% | batch:        65 of       298\t|\tloss: 0.307948\n",
      "Training Epoch 8  22.1% | batch:        66 of       298\t|\tloss: 0.254282\n",
      "Training Epoch 8  22.5% | batch:        67 of       298\t|\tloss: 1.0639\n",
      "Training Epoch 8  22.8% | batch:        68 of       298\t|\tloss: 0.278523\n",
      "Training Epoch 8  23.2% | batch:        69 of       298\t|\tloss: 1.9889\n",
      "Training Epoch 8  23.5% | batch:        70 of       298\t|\tloss: 0.203723\n",
      "Training Epoch 8  23.8% | batch:        71 of       298\t|\tloss: 0.243912\n",
      "Training Epoch 8  24.2% | batch:        72 of       298\t|\tloss: 0.492745\n",
      "Training Epoch 8  24.5% | batch:        73 of       298\t|\tloss: 0.197763\n",
      "Training Epoch 8  24.8% | batch:        74 of       298\t|\tloss: 1.40864\n",
      "Training Epoch 8  25.2% | batch:        75 of       298\t|\tloss: 0.18328\n",
      "Training Epoch 8  25.5% | batch:        76 of       298\t|\tloss: 0.174986\n",
      "Training Epoch 8  25.8% | batch:        77 of       298\t|\tloss: 0.240496\n",
      "Training Epoch 8  26.2% | batch:        78 of       298\t|\tloss: 0.201365\n",
      "Training Epoch 8  26.5% | batch:        79 of       298\t|\tloss: 0.208376\n",
      "Training Epoch 8  26.8% | batch:        80 of       298\t|\tloss: 0.226013\n",
      "Training Epoch 8  27.2% | batch:        81 of       298\t|\tloss: 0.267839\n",
      "Training Epoch 8  27.5% | batch:        82 of       298\t|\tloss: 0.234466\n",
      "Training Epoch 8  27.9% | batch:        83 of       298\t|\tloss: 0.280864\n",
      "Training Epoch 8  28.2% | batch:        84 of       298\t|\tloss: 0.254578\n",
      "Training Epoch 8  28.5% | batch:        85 of       298\t|\tloss: 0.578769\n",
      "Training Epoch 8  28.9% | batch:        86 of       298\t|\tloss: 0.258391\n",
      "Training Epoch 8  29.2% | batch:        87 of       298\t|\tloss: 0.223261\n",
      "Training Epoch 8  29.5% | batch:        88 of       298\t|\tloss: 0.575046\n",
      "Training Epoch 8  29.9% | batch:        89 of       298\t|\tloss: 0.248138\n",
      "Training Epoch 8  30.2% | batch:        90 of       298\t|\tloss: 0.17617\n",
      "Training Epoch 8  30.5% | batch:        91 of       298\t|\tloss: 0.216194\n",
      "Training Epoch 8  30.9% | batch:        92 of       298\t|\tloss: 0.424836\n",
      "Training Epoch 8  31.2% | batch:        93 of       298\t|\tloss: 0.141165\n",
      "Training Epoch 8  31.5% | batch:        94 of       298\t|\tloss: 0.256233\n",
      "Training Epoch 8  31.9% | batch:        95 of       298\t|\tloss: 0.296453\n",
      "Training Epoch 8  32.2% | batch:        96 of       298\t|\tloss: 1.56247\n",
      "Training Epoch 8  32.6% | batch:        97 of       298\t|\tloss: 0.208211\n",
      "Training Epoch 8  32.9% | batch:        98 of       298\t|\tloss: 0.202454\n",
      "Training Epoch 8  33.2% | batch:        99 of       298\t|\tloss: 0.21016\n",
      "Training Epoch 8  33.6% | batch:       100 of       298\t|\tloss: 0.176149\n",
      "Training Epoch 8  33.9% | batch:       101 of       298\t|\tloss: 0.442952\n",
      "Training Epoch 8  34.2% | batch:       102 of       298\t|\tloss: 0.350708\n",
      "Training Epoch 8  34.6% | batch:       103 of       298\t|\tloss: 0.574064\n",
      "Training Epoch 8  34.9% | batch:       104 of       298\t|\tloss: 0.222569\n",
      "Training Epoch 8  35.2% | batch:       105 of       298\t|\tloss: 0.377549\n",
      "Training Epoch 8  35.6% | batch:       106 of       298\t|\tloss: 0.162804\n",
      "Training Epoch 8  35.9% | batch:       107 of       298\t|\tloss: 0.168814\n",
      "Training Epoch 8  36.2% | batch:       108 of       298\t|\tloss: 0.24538\n",
      "Training Epoch 8  36.6% | batch:       109 of       298\t|\tloss: 0.173485\n",
      "Training Epoch 8  36.9% | batch:       110 of       298\t|\tloss: 0.263463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  37.2% | batch:       111 of       298\t|\tloss: 0.243911\n",
      "Training Epoch 8  37.6% | batch:       112 of       298\t|\tloss: 0.396552\n",
      "Training Epoch 8  37.9% | batch:       113 of       298\t|\tloss: 0.192669\n",
      "Training Epoch 8  38.3% | batch:       114 of       298\t|\tloss: 1.67212\n",
      "Training Epoch 8  38.6% | batch:       115 of       298\t|\tloss: 0.223709\n",
      "Training Epoch 8  38.9% | batch:       116 of       298\t|\tloss: 0.515805\n",
      "Training Epoch 8  39.3% | batch:       117 of       298\t|\tloss: 0.185379\n",
      "Training Epoch 8  39.6% | batch:       118 of       298\t|\tloss: 0.233983\n",
      "Training Epoch 8  39.9% | batch:       119 of       298\t|\tloss: 1.115\n",
      "Training Epoch 8  40.3% | batch:       120 of       298\t|\tloss: 0.255725\n",
      "Training Epoch 8  40.6% | batch:       121 of       298\t|\tloss: 0.186913\n",
      "Training Epoch 8  40.9% | batch:       122 of       298\t|\tloss: 0.204739\n",
      "Training Epoch 8  41.3% | batch:       123 of       298\t|\tloss: 0.198818\n",
      "Training Epoch 8  41.6% | batch:       124 of       298\t|\tloss: 0.213077\n",
      "Training Epoch 8  41.9% | batch:       125 of       298\t|\tloss: 0.536312\n",
      "Training Epoch 8  42.3% | batch:       126 of       298\t|\tloss: 0.222162\n",
      "Training Epoch 8  42.6% | batch:       127 of       298\t|\tloss: 0.149958\n",
      "Training Epoch 8  43.0% | batch:       128 of       298\t|\tloss: 2.80183\n",
      "Training Epoch 8  43.3% | batch:       129 of       298\t|\tloss: 0.342021\n",
      "Training Epoch 8  43.6% | batch:       130 of       298\t|\tloss: 0.243756\n",
      "Training Epoch 8  44.0% | batch:       131 of       298\t|\tloss: 0.225016\n",
      "Training Epoch 8  44.3% | batch:       132 of       298\t|\tloss: 0.187046\n",
      "Training Epoch 8  44.6% | batch:       133 of       298\t|\tloss: 0.20052\n",
      "Training Epoch 8  45.0% | batch:       134 of       298\t|\tloss: 0.281627\n",
      "Training Epoch 8  45.3% | batch:       135 of       298\t|\tloss: 0.986399\n",
      "Training Epoch 8  45.6% | batch:       136 of       298\t|\tloss: 0.235571\n",
      "Training Epoch 8  46.0% | batch:       137 of       298\t|\tloss: 0.207091\n",
      "Training Epoch 8  46.3% | batch:       138 of       298\t|\tloss: 0.22865\n",
      "Training Epoch 8  46.6% | batch:       139 of       298\t|\tloss: 0.865918\n",
      "Training Epoch 8  47.0% | batch:       140 of       298\t|\tloss: 0.196659\n",
      "Training Epoch 8  47.3% | batch:       141 of       298\t|\tloss: 0.175562\n",
      "Training Epoch 8  47.7% | batch:       142 of       298\t|\tloss: 0.219238\n",
      "Training Epoch 8  48.0% | batch:       143 of       298\t|\tloss: 0.586538\n",
      "Training Epoch 8  48.3% | batch:       144 of       298\t|\tloss: 0.234714\n",
      "Training Epoch 8  48.7% | batch:       145 of       298\t|\tloss: 0.199274\n",
      "Training Epoch 8  49.0% | batch:       146 of       298\t|\tloss: 0.272922\n",
      "Training Epoch 8  49.3% | batch:       147 of       298\t|\tloss: 0.228218\n",
      "Training Epoch 8  49.7% | batch:       148 of       298\t|\tloss: 0.186387\n",
      "Training Epoch 8  50.0% | batch:       149 of       298\t|\tloss: 0.250909\n",
      "Training Epoch 8  50.3% | batch:       150 of       298\t|\tloss: 0.162494\n",
      "Training Epoch 8  50.7% | batch:       151 of       298\t|\tloss: 0.279892\n",
      "Training Epoch 8  51.0% | batch:       152 of       298\t|\tloss: 0.290363\n",
      "Training Epoch 8  51.3% | batch:       153 of       298\t|\tloss: 0.287127\n",
      "Training Epoch 8  51.7% | batch:       154 of       298\t|\tloss: 0.375632\n",
      "Training Epoch 8  52.0% | batch:       155 of       298\t|\tloss: 0.465665\n",
      "Training Epoch 8  52.3% | batch:       156 of       298\t|\tloss: 0.188\n",
      "Training Epoch 8  52.7% | batch:       157 of       298\t|\tloss: 0.174894\n",
      "Training Epoch 8  53.0% | batch:       158 of       298\t|\tloss: 0.202875\n",
      "Training Epoch 8  53.4% | batch:       159 of       298\t|\tloss: 0.204512\n",
      "Training Epoch 8  53.7% | batch:       160 of       298\t|\tloss: 0.179741\n",
      "Training Epoch 8  54.0% | batch:       161 of       298\t|\tloss: 0.668691\n",
      "Training Epoch 8  54.4% | batch:       162 of       298\t|\tloss: 0.354667\n",
      "Training Epoch 8  54.7% | batch:       163 of       298\t|\tloss: 0.149996\n",
      "Training Epoch 8  55.0% | batch:       164 of       298\t|\tloss: 0.249253\n",
      "Training Epoch 8  55.4% | batch:       165 of       298\t|\tloss: 0.278438\n",
      "Training Epoch 8  55.7% | batch:       166 of       298\t|\tloss: 0.209381\n",
      "Training Epoch 8  56.0% | batch:       167 of       298\t|\tloss: 0.236846\n",
      "Training Epoch 8  56.4% | batch:       168 of       298\t|\tloss: 0.163162\n",
      "Training Epoch 8  56.7% | batch:       169 of       298\t|\tloss: 0.15964\n",
      "Training Epoch 8  57.0% | batch:       170 of       298\t|\tloss: 0.183138\n",
      "Training Epoch 8  57.4% | batch:       171 of       298\t|\tloss: 1.19689\n",
      "Training Epoch 8  57.7% | batch:       172 of       298\t|\tloss: 0.262673\n",
      "Training Epoch 8  58.1% | batch:       173 of       298\t|\tloss: 0.176877\n",
      "Training Epoch 8  58.4% | batch:       174 of       298\t|\tloss: 0.213192\n",
      "Training Epoch 8  58.7% | batch:       175 of       298\t|\tloss: 0.163548\n",
      "Training Epoch 8  59.1% | batch:       176 of       298\t|\tloss: 0.223942\n",
      "Training Epoch 8  59.4% | batch:       177 of       298\t|\tloss: 0.255123\n",
      "Training Epoch 8  59.7% | batch:       178 of       298\t|\tloss: 0.215108\n",
      "Training Epoch 8  60.1% | batch:       179 of       298\t|\tloss: 0.23179\n",
      "Training Epoch 8  60.4% | batch:       180 of       298\t|\tloss: 0.345891\n",
      "Training Epoch 8  60.7% | batch:       181 of       298\t|\tloss: 0.229964\n",
      "Training Epoch 8  61.1% | batch:       182 of       298\t|\tloss: 0.249739\n",
      "Training Epoch 8  61.4% | batch:       183 of       298\t|\tloss: 0.200444\n",
      "Training Epoch 8  61.7% | batch:       184 of       298\t|\tloss: 0.218376\n",
      "Training Epoch 8  62.1% | batch:       185 of       298\t|\tloss: 0.303878\n",
      "Training Epoch 8  62.4% | batch:       186 of       298\t|\tloss: 0.165737\n",
      "Training Epoch 8  62.8% | batch:       187 of       298\t|\tloss: 0.242595\n",
      "Training Epoch 8  63.1% | batch:       188 of       298\t|\tloss: 0.221196\n",
      "Training Epoch 8  63.4% | batch:       189 of       298\t|\tloss: 0.156592\n",
      "Training Epoch 8  63.8% | batch:       190 of       298\t|\tloss: 0.171996\n",
      "Training Epoch 8  64.1% | batch:       191 of       298\t|\tloss: 0.181481\n",
      "Training Epoch 8  64.4% | batch:       192 of       298\t|\tloss: 0.14154\n",
      "Training Epoch 8  64.8% | batch:       193 of       298\t|\tloss: 0.183996\n",
      "Training Epoch 8  65.1% | batch:       194 of       298\t|\tloss: 0.159616\n",
      "Training Epoch 8  65.4% | batch:       195 of       298\t|\tloss: 0.182856\n",
      "Training Epoch 8  65.8% | batch:       196 of       298\t|\tloss: 0.201304\n",
      "Training Epoch 8  66.1% | batch:       197 of       298\t|\tloss: 0.238727\n",
      "Training Epoch 8  66.4% | batch:       198 of       298\t|\tloss: 0.240308\n",
      "Training Epoch 8  66.8% | batch:       199 of       298\t|\tloss: 0.274674\n",
      "Training Epoch 8  67.1% | batch:       200 of       298\t|\tloss: 2.1436\n",
      "Training Epoch 8  67.4% | batch:       201 of       298\t|\tloss: 0.355734\n",
      "Training Epoch 8  67.8% | batch:       202 of       298\t|\tloss: 0.154651\n",
      "Training Epoch 8  68.1% | batch:       203 of       298\t|\tloss: 0.3164\n",
      "Training Epoch 8  68.5% | batch:       204 of       298\t|\tloss: 0.315237\n",
      "Training Epoch 8  68.8% | batch:       205 of       298\t|\tloss: 0.163121\n",
      "Training Epoch 8  69.1% | batch:       206 of       298\t|\tloss: 0.166259\n",
      "Training Epoch 8  69.5% | batch:       207 of       298\t|\tloss: 0.170473\n",
      "Training Epoch 8  69.8% | batch:       208 of       298\t|\tloss: 0.31371\n",
      "Training Epoch 8  70.1% | batch:       209 of       298\t|\tloss: 0.171567\n",
      "Training Epoch 8  70.5% | batch:       210 of       298\t|\tloss: 0.251087\n",
      "Training Epoch 8  70.8% | batch:       211 of       298\t|\tloss: 0.233892\n",
      "Training Epoch 8  71.1% | batch:       212 of       298\t|\tloss: 0.283021\n",
      "Training Epoch 8  71.5% | batch:       213 of       298\t|\tloss: 0.188721\n",
      "Training Epoch 8  71.8% | batch:       214 of       298\t|\tloss: 0.199496\n",
      "Training Epoch 8  72.1% | batch:       215 of       298\t|\tloss: 0.212599\n",
      "Training Epoch 8  72.5% | batch:       216 of       298\t|\tloss: 0.209691\n",
      "Training Epoch 8  72.8% | batch:       217 of       298\t|\tloss: 0.191855\n",
      "Training Epoch 8  73.2% | batch:       218 of       298\t|\tloss: 0.325877\n",
      "Training Epoch 8  73.5% | batch:       219 of       298\t|\tloss: 0.167449\n",
      "Training Epoch 8  73.8% | batch:       220 of       298\t|\tloss: 0.169165\n",
      "Training Epoch 8  74.2% | batch:       221 of       298\t|\tloss: 0.270014\n",
      "Training Epoch 8  74.5% | batch:       222 of       298\t|\tloss: 0.414585\n",
      "Training Epoch 8  74.8% | batch:       223 of       298\t|\tloss: 0.596871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  75.2% | batch:       224 of       298\t|\tloss: 0.4319\n",
      "Training Epoch 8  75.5% | batch:       225 of       298\t|\tloss: 0.193783\n",
      "Training Epoch 8  75.8% | batch:       226 of       298\t|\tloss: 0.223585\n",
      "Training Epoch 8  76.2% | batch:       227 of       298\t|\tloss: 0.193688\n",
      "Training Epoch 8  76.5% | batch:       228 of       298\t|\tloss: 0.185033\n",
      "Training Epoch 8  76.8% | batch:       229 of       298\t|\tloss: 0.170312\n",
      "Training Epoch 8  77.2% | batch:       230 of       298\t|\tloss: 0.321624\n",
      "Training Epoch 8  77.5% | batch:       231 of       298\t|\tloss: 0.18507\n",
      "Training Epoch 8  77.9% | batch:       232 of       298\t|\tloss: 0.196522\n",
      "Training Epoch 8  78.2% | batch:       233 of       298\t|\tloss: 0.163041\n",
      "Training Epoch 8  78.5% | batch:       234 of       298\t|\tloss: 0.455762\n",
      "Training Epoch 8  78.9% | batch:       235 of       298\t|\tloss: 0.204564\n",
      "Training Epoch 8  79.2% | batch:       236 of       298\t|\tloss: 0.259189\n",
      "Training Epoch 8  79.5% | batch:       237 of       298\t|\tloss: 0.171717\n",
      "Training Epoch 8  79.9% | batch:       238 of       298\t|\tloss: 0.166801\n",
      "Training Epoch 8  80.2% | batch:       239 of       298\t|\tloss: 0.347924\n",
      "Training Epoch 8  80.5% | batch:       240 of       298\t|\tloss: 0.175361\n",
      "Training Epoch 8  80.9% | batch:       241 of       298\t|\tloss: 1.38203\n",
      "Training Epoch 8  81.2% | batch:       242 of       298\t|\tloss: 0.187018\n",
      "Training Epoch 8  81.5% | batch:       243 of       298\t|\tloss: 0.461294\n",
      "Training Epoch 8  81.9% | batch:       244 of       298\t|\tloss: 0.187883\n",
      "Training Epoch 8  82.2% | batch:       245 of       298\t|\tloss: 0.267418\n",
      "Training Epoch 8  82.6% | batch:       246 of       298\t|\tloss: 0.22315\n",
      "Training Epoch 8  82.9% | batch:       247 of       298\t|\tloss: 0.267155\n",
      "Training Epoch 8  83.2% | batch:       248 of       298\t|\tloss: 0.240776\n",
      "Training Epoch 8  83.6% | batch:       249 of       298\t|\tloss: 0.207057\n",
      "Training Epoch 8  83.9% | batch:       250 of       298\t|\tloss: 0.259941\n",
      "Training Epoch 8  84.2% | batch:       251 of       298\t|\tloss: 0.213391\n",
      "Training Epoch 8  84.6% | batch:       252 of       298\t|\tloss: 0.201397\n",
      "Training Epoch 8  84.9% | batch:       253 of       298\t|\tloss: 0.20488\n",
      "Training Epoch 8  85.2% | batch:       254 of       298\t|\tloss: 0.217623\n",
      "Training Epoch 8  85.6% | batch:       255 of       298\t|\tloss: 0.2398\n",
      "Training Epoch 8  85.9% | batch:       256 of       298\t|\tloss: 0.204481\n",
      "Training Epoch 8  86.2% | batch:       257 of       298\t|\tloss: 0.20525\n",
      "Training Epoch 8  86.6% | batch:       258 of       298\t|\tloss: 0.23995\n",
      "Training Epoch 8  86.9% | batch:       259 of       298\t|\tloss: 0.36382\n",
      "Training Epoch 8  87.2% | batch:       260 of       298\t|\tloss: 0.424005\n",
      "Training Epoch 8  87.6% | batch:       261 of       298\t|\tloss: 0.390776\n",
      "Training Epoch 8  87.9% | batch:       262 of       298\t|\tloss: 0.197038\n",
      "Training Epoch 8  88.3% | batch:       263 of       298\t|\tloss: 0.228596\n",
      "Training Epoch 8  88.6% | batch:       264 of       298\t|\tloss: 0.158205\n",
      "Training Epoch 8  88.9% | batch:       265 of       298\t|\tloss: 0.249733\n",
      "Training Epoch 8  89.3% | batch:       266 of       298\t|\tloss: 0.230927\n",
      "Training Epoch 8  89.6% | batch:       267 of       298\t|\tloss: 0.221416\n",
      "Training Epoch 8  89.9% | batch:       268 of       298\t|\tloss: 0.299705\n",
      "Training Epoch 8  90.3% | batch:       269 of       298\t|\tloss: 0.201177\n",
      "Training Epoch 8  90.6% | batch:       270 of       298\t|\tloss: 0.233778\n",
      "Training Epoch 8  90.9% | batch:       271 of       298\t|\tloss: 0.223329\n",
      "Training Epoch 8  91.3% | batch:       272 of       298\t|\tloss: 0.208209\n",
      "Training Epoch 8  91.6% | batch:       273 of       298\t|\tloss: 0.159446\n",
      "Training Epoch 8  91.9% | batch:       274 of       298\t|\tloss: 0.256184\n",
      "Training Epoch 8  92.3% | batch:       275 of       298\t|\tloss: 0.243282\n",
      "Training Epoch 8  92.6% | batch:       276 of       298\t|\tloss: 0.46425\n",
      "Training Epoch 8  93.0% | batch:       277 of       298\t|\tloss: 0.216843\n",
      "Training Epoch 8  93.3% | batch:       278 of       298\t|\tloss: 0.260386\n",
      "Training Epoch 8  93.6% | batch:       279 of       298\t|\tloss: 0.181979\n",
      "Training Epoch 8  94.0% | batch:       280 of       298\t|\tloss: 0.227926\n",
      "Training Epoch 8  94.3% | batch:       281 of       298\t|\tloss: 0.227551\n",
      "Training Epoch 8  94.6% | batch:       282 of       298\t|\tloss: 0.172185\n",
      "Training Epoch 8  95.0% | batch:       283 of       298\t|\tloss: 0.203478\n",
      "Training Epoch 8  95.3% | batch:       284 of       298\t|\tloss: 0.218887\n",
      "Training Epoch 8  95.6% | batch:       285 of       298\t|\tloss: 0.185512\n",
      "Training Epoch 8  96.0% | batch:       286 of       298\t|\tloss: 0.180487\n",
      "Training Epoch 8  96.3% | batch:       287 of       298\t|\tloss: 0.134759\n",
      "Training Epoch 8  96.6% | batch:       288 of       298\t|\tloss: 0.178196\n",
      "Training Epoch 8  97.0% | batch:       289 of       298\t|\tloss: 0.232393\n",
      "Training Epoch 8  97.3% | batch:       290 of       298\t|\tloss: 0.295704\n",
      "Training Epoch 8  97.7% | batch:       291 of       298\t|\tloss: 0.220685\n",
      "Training Epoch 8  98.0% | batch:       292 of       298\t|\tloss: 0.25427\n",
      "Training Epoch 8  98.3% | batch:       293 of       298\t|\tloss: 0.412911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:37:44,873 | INFO : Epoch 8 Training Summary: epoch: 8.000000 | loss: 0.312519 | \n",
      "2023-06-22 14:37:44,874 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.220973014831543 seconds\n",
      "\n",
      "2023-06-22 14:37:44,875 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.26193943619728 seconds\n",
      "2023-06-22 14:37:44,875 | INFO : Avg batch train. time: 0.041147447772474095 seconds\n",
      "2023-06-22 14:37:44,876 | INFO : Avg sample train. time: 0.0012861274843924146 seconds\n",
      "2023-06-22 14:37:44,877 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 8  98.7% | batch:       294 of       298\t|\tloss: 0.213928\n",
      "Training Epoch 8  99.0% | batch:       295 of       298\t|\tloss: 0.277559\n",
      "Training Epoch 8  99.3% | batch:       296 of       298\t|\tloss: 0.292778\n",
      "Training Epoch 8  99.7% | batch:       297 of       298\t|\tloss: 0.192449\n",
      "\n",
      "Evaluating Epoch 8   0.0% | batch:         0 of        75\t|\tloss: 0.24204\n",
      "Evaluating Epoch 8   1.3% | batch:         1 of        75\t|\tloss: 0.343431\n",
      "Evaluating Epoch 8   2.7% | batch:         2 of        75\t|\tloss: 0.23402\n",
      "Evaluating Epoch 8   4.0% | batch:         3 of        75\t|\tloss: 0.158374\n",
      "Evaluating Epoch 8   5.3% | batch:         4 of        75\t|\tloss: 0.137181\n",
      "Evaluating Epoch 8   6.7% | batch:         5 of        75\t|\tloss: 0.332998\n",
      "Evaluating Epoch 8   8.0% | batch:         6 of        75\t|\tloss: 0.312874\n",
      "Evaluating Epoch 8   9.3% | batch:         7 of        75\t|\tloss: 0.300691\n",
      "Evaluating Epoch 8  10.7% | batch:         8 of        75\t|\tloss: 0.163014\n",
      "Evaluating Epoch 8  12.0% | batch:         9 of        75\t|\tloss: 0.293438\n",
      "Evaluating Epoch 8  13.3% | batch:        10 of        75\t|\tloss: 0.147515\n",
      "Evaluating Epoch 8  14.7% | batch:        11 of        75\t|\tloss: 0.152912\n",
      "Evaluating Epoch 8  16.0% | batch:        12 of        75\t|\tloss: 0.211307\n",
      "Evaluating Epoch 8  17.3% | batch:        13 of        75\t|\tloss: 0.172028\n",
      "Evaluating Epoch 8  18.7% | batch:        14 of        75\t|\tloss: 0.198326\n",
      "Evaluating Epoch 8  20.0% | batch:        15 of        75\t|\tloss: 2.12667\n",
      "Evaluating Epoch 8  21.3% | batch:        16 of        75\t|\tloss: 0.150318\n",
      "Evaluating Epoch 8  22.7% | batch:        17 of        75\t|\tloss: 0.16719\n",
      "Evaluating Epoch 8  24.0% | batch:        18 of        75\t|\tloss: 0.220969\n",
      "Evaluating Epoch 8  25.3% | batch:        19 of        75\t|\tloss: 0.166174\n",
      "Evaluating Epoch 8  26.7% | batch:        20 of        75\t|\tloss: 0.224061\n",
      "Evaluating Epoch 8  28.0% | batch:        21 of        75\t|\tloss: 0.289068\n",
      "Evaluating Epoch 8  29.3% | batch:        22 of        75\t|\tloss: 0.225824\n",
      "Evaluating Epoch 8  30.7% | batch:        23 of        75\t|\tloss: 2.43333\n",
      "Evaluating Epoch 8  32.0% | batch:        24 of        75\t|\tloss: 0.766145\n",
      "Evaluating Epoch 8  33.3% | batch:        25 of        75\t|\tloss: 0.181438\n",
      "Evaluating Epoch 8  34.7% | batch:        26 of        75\t|\tloss: 0.227835\n",
      "Evaluating Epoch 8  36.0% | batch:        27 of        75\t|\tloss: 0.246507\n",
      "Evaluating Epoch 8  37.3% | batch:        28 of        75\t|\tloss: 0.182064\n",
      "Evaluating Epoch 8  38.7% | batch:        29 of        75\t|\tloss: 0.227032\n",
      "Evaluating Epoch 8  40.0% | batch:        30 of        75\t|\tloss: 0.121501\n",
      "Evaluating Epoch 8  41.3% | batch:        31 of        75\t|\tloss: 0.198485\n",
      "Evaluating Epoch 8  42.7% | batch:        32 of        75\t|\tloss: 0.97049\n",
      "Evaluating Epoch 8  44.0% | batch:        33 of        75\t|\tloss: 0.239495\n",
      "Evaluating Epoch 8  45.3% | batch:        34 of        75\t|\tloss: 0.19171\n",
      "Evaluating Epoch 8  46.7% | batch:        35 of        75\t|\tloss: 0.85084\n",
      "Evaluating Epoch 8  48.0% | batch:        36 of        75\t|\tloss: 0.148098\n",
      "Evaluating Epoch 8  49.3% | batch:        37 of        75\t|\tloss: 0.168037\n",
      "Evaluating Epoch 8  50.7% | batch:        38 of        75\t|\tloss: 0.227664\n",
      "Evaluating Epoch 8  52.0% | batch:        39 of        75\t|\tloss: 0.217318\n",
      "Evaluating Epoch 8  53.3% | batch:        40 of        75\t|\tloss: 1.1702\n",
      "Evaluating Epoch 8  54.7% | batch:        41 of        75\t|\tloss: 0.141161\n",
      "Evaluating Epoch 8  56.0% | batch:        42 of        75\t|\tloss: 0.159613\n",
      "Evaluating Epoch 8  57.3% | batch:        43 of        75\t|\tloss: 0.226091\n",
      "Evaluating Epoch 8  58.7% | batch:        44 of        75\t|\tloss: 0.153982\n",
      "Evaluating Epoch 8  60.0% | batch:        45 of        75\t|\tloss: 0.323268\n",
      "Evaluating Epoch 8  61.3% | batch:        46 of        75\t|\tloss: 0.234801\n",
      "Evaluating Epoch 8  62.7% | batch:        47 of        75\t|\tloss: 0.144136\n",
      "Evaluating Epoch 8  64.0% | batch:        48 of        75\t|\tloss: 0.21445\n",
      "Evaluating Epoch 8  65.3% | batch:        49 of        75\t|\tloss: 0.151098\n",
      "Evaluating Epoch 8  66.7% | batch:        50 of        75\t|\tloss: 0.86535\n",
      "Evaluating Epoch 8  68.0% | batch:        51 of        75\t|\tloss: 0.333503\n",
      "Evaluating Epoch 8  69.3% | batch:        52 of        75\t|\tloss: 0.213114\n",
      "Evaluating Epoch 8  70.7% | batch:        53 of        75\t|\tloss: 0.473848\n",
      "Evaluating Epoch 8  72.0% | batch:        54 of        75\t|\tloss: 0.247419\n",
      "Evaluating Epoch 8  73.3% | batch:        55 of        75\t|\tloss: 0.886755\n",
      "Evaluating Epoch 8  74.7% | batch:        56 of        75\t|\tloss: 0.117795\n",
      "Evaluating Epoch 8  76.0% | batch:        57 of        75\t|\tloss: 0.124128\n",
      "Evaluating Epoch 8  77.3% | batch:        58 of        75\t|\tloss: 0.225599\n",
      "Evaluating Epoch 8  78.7% | batch:        59 of        75\t|\tloss: 7.31017\n",
      "Evaluating Epoch 8  80.0% | batch:        60 of        75\t|\tloss: 0.177912\n",
      "Evaluating Epoch 8  81.3% | batch:        61 of        75\t|\tloss: 0.174533\n",
      "Evaluating Epoch 8  82.7% | batch:        62 of        75\t|\tloss: 0.155999\n",
      "Evaluating Epoch 8  84.0% | batch:        63 of        75\t|\tloss: 0.149409\n",
      "Evaluating Epoch 8  85.3% | batch:        64 of        75\t|\tloss: 0.154056\n",
      "Evaluating Epoch 8  86.7% | batch:        65 of        75\t|\tloss: 0.183834\n",
      "Evaluating Epoch 8  88.0% | batch:        66 of        75\t|\tloss: 0.154453\n",
      "Evaluating Epoch 8  89.3% | batch:        67 of        75\t|\tloss: 0.224894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:37:46,107 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.2296059131622314 seconds\n",
      "\n",
      "2023-06-22 14:37:46,108 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.232251485188802 seconds\n",
      "2023-06-22 14:37:46,109 | INFO : Avg batch val. time: 0.01643001980251736 seconds\n",
      "2023-06-22 14:37:46,110 | INFO : Avg sample val. time: 0.000516884012243625 seconds\n",
      "2023-06-22 14:37:46,111 | INFO : Epoch 8 Validation Summary: epoch: 8.000000 | loss: 0.421244 | \n",
      "Training Epoch:   2%|▏         | 8/400 [01:44<1:25:00, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 8  90.7% | batch:        68 of        75\t|\tloss: 0.187474\n",
      "Evaluating Epoch 8  92.0% | batch:        69 of        75\t|\tloss: 0.198219\n",
      "Evaluating Epoch 8  93.3% | batch:        70 of        75\t|\tloss: 0.219111\n",
      "Evaluating Epoch 8  94.7% | batch:        71 of        75\t|\tloss: 0.210799\n",
      "Evaluating Epoch 8  96.0% | batch:        72 of        75\t|\tloss: 0.114155\n",
      "Evaluating Epoch 8  97.3% | batch:        73 of        75\t|\tloss: 0.341037\n",
      "Evaluating Epoch 8  98.7% | batch:        74 of        75\t|\tloss: 0.17377\n",
      "\n",
      "Training Epoch 9   0.0% | batch:         0 of       298\t|\tloss: 0.46713\n",
      "Training Epoch 9   0.3% | batch:         1 of       298\t|\tloss: 0.547712\n",
      "Training Epoch 9   0.7% | batch:         2 of       298\t|\tloss: 0.218154\n",
      "Training Epoch 9   1.0% | batch:         3 of       298\t|\tloss: 0.216219\n",
      "Training Epoch 9   1.3% | batch:         4 of       298\t|\tloss: 0.234035\n",
      "Training Epoch 9   1.7% | batch:         5 of       298\t|\tloss: 0.319391\n",
      "Training Epoch 9   2.0% | batch:         6 of       298\t|\tloss: 0.18847\n",
      "Training Epoch 9   2.3% | batch:         7 of       298\t|\tloss: 0.560315\n",
      "Training Epoch 9   2.7% | batch:         8 of       298\t|\tloss: 0.579913\n",
      "Training Epoch 9   3.0% | batch:         9 of       298\t|\tloss: 0.238092\n",
      "Training Epoch 9   3.4% | batch:        10 of       298\t|\tloss: 0.420031\n",
      "Training Epoch 9   3.7% | batch:        11 of       298\t|\tloss: 0.214223\n",
      "Training Epoch 9   4.0% | batch:        12 of       298\t|\tloss: 0.177246\n",
      "Training Epoch 9   4.4% | batch:        13 of       298\t|\tloss: 0.204067\n",
      "Training Epoch 9   4.7% | batch:        14 of       298\t|\tloss: 0.222391\n",
      "Training Epoch 9   5.0% | batch:        15 of       298\t|\tloss: 0.159441\n",
      "Training Epoch 9   5.4% | batch:        16 of       298\t|\tloss: 0.191425\n",
      "Training Epoch 9   5.7% | batch:        17 of       298\t|\tloss: 0.385275\n",
      "Training Epoch 9   6.0% | batch:        18 of       298\t|\tloss: 0.219963\n",
      "Training Epoch 9   6.4% | batch:        19 of       298\t|\tloss: 0.216741\n",
      "Training Epoch 9   6.7% | batch:        20 of       298\t|\tloss: 0.204647\n",
      "Training Epoch 9   7.0% | batch:        21 of       298\t|\tloss: 0.239219\n",
      "Training Epoch 9   7.4% | batch:        22 of       298\t|\tloss: 0.279552\n",
      "Training Epoch 9   7.7% | batch:        23 of       298\t|\tloss: 0.194468\n",
      "Training Epoch 9   8.1% | batch:        24 of       298\t|\tloss: 0.346956\n",
      "Training Epoch 9   8.4% | batch:        25 of       298\t|\tloss: 0.319249\n",
      "Training Epoch 9   8.7% | batch:        26 of       298\t|\tloss: 0.226799\n",
      "Training Epoch 9   9.1% | batch:        27 of       298\t|\tloss: 0.189185\n",
      "Training Epoch 9   9.4% | batch:        28 of       298\t|\tloss: 0.755008\n",
      "Training Epoch 9   9.7% | batch:        29 of       298\t|\tloss: 0.190234\n",
      "Training Epoch 9  10.1% | batch:        30 of       298\t|\tloss: 0.161055\n",
      "Training Epoch 9  10.4% | batch:        31 of       298\t|\tloss: 0.201412\n",
      "Training Epoch 9  10.7% | batch:        32 of       298\t|\tloss: 0.197774\n",
      "Training Epoch 9  11.1% | batch:        33 of       298\t|\tloss: 0.16619\n",
      "Training Epoch 9  11.4% | batch:        34 of       298\t|\tloss: 0.15462\n",
      "Training Epoch 9  11.7% | batch:        35 of       298\t|\tloss: 0.237797\n",
      "Training Epoch 9  12.1% | batch:        36 of       298\t|\tloss: 0.189349\n",
      "Training Epoch 9  12.4% | batch:        37 of       298\t|\tloss: 0.254\n",
      "Training Epoch 9  12.8% | batch:        38 of       298\t|\tloss: 0.315657\n",
      "Training Epoch 9  13.1% | batch:        39 of       298\t|\tloss: 0.283332\n",
      "Training Epoch 9  13.4% | batch:        40 of       298\t|\tloss: 0.225631\n",
      "Training Epoch 9  13.8% | batch:        41 of       298\t|\tloss: 0.526769\n",
      "Training Epoch 9  14.1% | batch:        42 of       298\t|\tloss: 0.191879\n",
      "Training Epoch 9  14.4% | batch:        43 of       298\t|\tloss: 0.379619\n",
      "Training Epoch 9  14.8% | batch:        44 of       298\t|\tloss: 0.250036\n",
      "Training Epoch 9  15.1% | batch:        45 of       298\t|\tloss: 0.157382\n",
      "Training Epoch 9  15.4% | batch:        46 of       298\t|\tloss: 0.226076\n",
      "Training Epoch 9  15.8% | batch:        47 of       298\t|\tloss: 0.14801\n",
      "Training Epoch 9  16.1% | batch:        48 of       298\t|\tloss: 0.314976\n",
      "Training Epoch 9  16.4% | batch:        49 of       298\t|\tloss: 0.152463\n",
      "Training Epoch 9  16.8% | batch:        50 of       298\t|\tloss: 0.224366\n",
      "Training Epoch 9  17.1% | batch:        51 of       298\t|\tloss: 0.304442\n",
      "Training Epoch 9  17.4% | batch:        52 of       298\t|\tloss: 0.377798\n",
      "Training Epoch 9  17.8% | batch:        53 of       298\t|\tloss: 0.249138\n",
      "Training Epoch 9  18.1% | batch:        54 of       298\t|\tloss: 0.660782\n",
      "Training Epoch 9  18.5% | batch:        55 of       298\t|\tloss: 0.185083\n",
      "Training Epoch 9  18.8% | batch:        56 of       298\t|\tloss: 0.209853\n",
      "Training Epoch 9  19.1% | batch:        57 of       298\t|\tloss: 0.223661\n",
      "Training Epoch 9  19.5% | batch:        58 of       298\t|\tloss: 0.28933\n",
      "Training Epoch 9  19.8% | batch:        59 of       298\t|\tloss: 0.16957\n",
      "Training Epoch 9  20.1% | batch:        60 of       298\t|\tloss: 0.239177\n",
      "Training Epoch 9  20.5% | batch:        61 of       298\t|\tloss: 0.161394\n",
      "Training Epoch 9  20.8% | batch:        62 of       298\t|\tloss: 0.157189\n",
      "Training Epoch 9  21.1% | batch:        63 of       298\t|\tloss: 0.175101\n",
      "Training Epoch 9  21.5% | batch:        64 of       298\t|\tloss: 0.185919\n",
      "Training Epoch 9  21.8% | batch:        65 of       298\t|\tloss: 0.185478\n",
      "Training Epoch 9  22.1% | batch:        66 of       298\t|\tloss: 0.188283\n",
      "Training Epoch 9  22.5% | batch:        67 of       298\t|\tloss: 0.1799\n",
      "Training Epoch 9  22.8% | batch:        68 of       298\t|\tloss: 0.212551\n",
      "Training Epoch 9  23.2% | batch:        69 of       298\t|\tloss: 0.262491\n",
      "Training Epoch 9  23.5% | batch:        70 of       298\t|\tloss: 0.141273\n",
      "Training Epoch 9  23.8% | batch:        71 of       298\t|\tloss: 0.20347\n",
      "Training Epoch 9  24.2% | batch:        72 of       298\t|\tloss: 0.240503\n",
      "Training Epoch 9  24.5% | batch:        73 of       298\t|\tloss: 0.173708\n",
      "Training Epoch 9  24.8% | batch:        74 of       298\t|\tloss: 0.163448\n",
      "Training Epoch 9  25.2% | batch:        75 of       298\t|\tloss: 0.172238\n",
      "Training Epoch 9  25.5% | batch:        76 of       298\t|\tloss: 0.244639\n",
      "Training Epoch 9  25.8% | batch:        77 of       298\t|\tloss: 0.338507\n",
      "Training Epoch 9  26.2% | batch:        78 of       298\t|\tloss: 0.186233\n",
      "Training Epoch 9  26.5% | batch:        79 of       298\t|\tloss: 0.227054\n",
      "Training Epoch 9  26.8% | batch:        80 of       298\t|\tloss: 0.28159\n",
      "Training Epoch 9  27.2% | batch:        81 of       298\t|\tloss: 0.199484\n",
      "Training Epoch 9  27.5% | batch:        82 of       298\t|\tloss: 0.239483\n",
      "Training Epoch 9  27.9% | batch:        83 of       298\t|\tloss: 0.178355\n",
      "Training Epoch 9  28.2% | batch:        84 of       298\t|\tloss: 1.36512\n",
      "Training Epoch 9  28.5% | batch:        85 of       298\t|\tloss: 0.396974\n",
      "Training Epoch 9  28.9% | batch:        86 of       298\t|\tloss: 0.186003\n",
      "Training Epoch 9  29.2% | batch:        87 of       298\t|\tloss: 0.192187\n",
      "Training Epoch 9  29.5% | batch:        88 of       298\t|\tloss: 0.210071\n",
      "Training Epoch 9  29.9% | batch:        89 of       298\t|\tloss: 0.171772\n",
      "Training Epoch 9  30.2% | batch:        90 of       298\t|\tloss: 0.207476\n",
      "Training Epoch 9  30.5% | batch:        91 of       298\t|\tloss: 0.461871\n",
      "Training Epoch 9  30.9% | batch:        92 of       298\t|\tloss: 0.227703\n",
      "Training Epoch 9  31.2% | batch:        93 of       298\t|\tloss: 0.204754\n",
      "Training Epoch 9  31.5% | batch:        94 of       298\t|\tloss: 0.195679\n",
      "Training Epoch 9  31.9% | batch:        95 of       298\t|\tloss: 0.463532\n",
      "Training Epoch 9  32.2% | batch:        96 of       298\t|\tloss: 0.234076\n",
      "Training Epoch 9  32.6% | batch:        97 of       298\t|\tloss: 0.20082\n",
      "Training Epoch 9  32.9% | batch:        98 of       298\t|\tloss: 0.366903\n",
      "Training Epoch 9  33.2% | batch:        99 of       298\t|\tloss: 0.197296\n",
      "Training Epoch 9  33.6% | batch:       100 of       298\t|\tloss: 0.169747\n",
      "Training Epoch 9  33.9% | batch:       101 of       298\t|\tloss: 0.170953\n",
      "Training Epoch 9  34.2% | batch:       102 of       298\t|\tloss: 1.12814\n",
      "Training Epoch 9  34.6% | batch:       103 of       298\t|\tloss: 0.381612\n",
      "Training Epoch 9  34.9% | batch:       104 of       298\t|\tloss: 0.178403\n",
      "Training Epoch 9  35.2% | batch:       105 of       298\t|\tloss: 0.141091\n",
      "Training Epoch 9  35.6% | batch:       106 of       298\t|\tloss: 0.287464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  35.9% | batch:       107 of       298\t|\tloss: 0.201196\n",
      "Training Epoch 9  36.2% | batch:       108 of       298\t|\tloss: 0.167067\n",
      "Training Epoch 9  36.6% | batch:       109 of       298\t|\tloss: 0.16983\n",
      "Training Epoch 9  36.9% | batch:       110 of       298\t|\tloss: 0.244748\n",
      "Training Epoch 9  37.2% | batch:       111 of       298\t|\tloss: 0.198581\n",
      "Training Epoch 9  37.6% | batch:       112 of       298\t|\tloss: 0.208022\n",
      "Training Epoch 9  37.9% | batch:       113 of       298\t|\tloss: 0.255789\n",
      "Training Epoch 9  38.3% | batch:       114 of       298\t|\tloss: 0.195802\n",
      "Training Epoch 9  38.6% | batch:       115 of       298\t|\tloss: 2.00121\n",
      "Training Epoch 9  38.9% | batch:       116 of       298\t|\tloss: 2.17276\n",
      "Training Epoch 9  39.3% | batch:       117 of       298\t|\tloss: 7.94597\n",
      "Training Epoch 9  39.6% | batch:       118 of       298\t|\tloss: 0.185745\n",
      "Training Epoch 9  39.9% | batch:       119 of       298\t|\tloss: 0.201486\n",
      "Training Epoch 9  40.3% | batch:       120 of       298\t|\tloss: 0.294586\n",
      "Training Epoch 9  40.6% | batch:       121 of       298\t|\tloss: 0.439747\n",
      "Training Epoch 9  40.9% | batch:       122 of       298\t|\tloss: 0.178154\n",
      "Training Epoch 9  41.3% | batch:       123 of       298\t|\tloss: 0.193447\n",
      "Training Epoch 9  41.6% | batch:       124 of       298\t|\tloss: 0.205558\n",
      "Training Epoch 9  41.9% | batch:       125 of       298\t|\tloss: 0.373482\n",
      "Training Epoch 9  42.3% | batch:       126 of       298\t|\tloss: 0.17755\n",
      "Training Epoch 9  42.6% | batch:       127 of       298\t|\tloss: 0.236826\n",
      "Training Epoch 9  43.0% | batch:       128 of       298\t|\tloss: 0.371861\n",
      "Training Epoch 9  43.3% | batch:       129 of       298\t|\tloss: 0.224121\n",
      "Training Epoch 9  43.6% | batch:       130 of       298\t|\tloss: 0.213506\n",
      "Training Epoch 9  44.0% | batch:       131 of       298\t|\tloss: 0.19266\n",
      "Training Epoch 9  44.3% | batch:       132 of       298\t|\tloss: 0.167312\n",
      "Training Epoch 9  44.6% | batch:       133 of       298\t|\tloss: 0.370662\n",
      "Training Epoch 9  45.0% | batch:       134 of       298\t|\tloss: 0.217106\n",
      "Training Epoch 9  45.3% | batch:       135 of       298\t|\tloss: 0.256992\n",
      "Training Epoch 9  45.6% | batch:       136 of       298\t|\tloss: 0.18458\n",
      "Training Epoch 9  46.0% | batch:       137 of       298\t|\tloss: 0.190206\n",
      "Training Epoch 9  46.3% | batch:       138 of       298\t|\tloss: 0.326709\n",
      "Training Epoch 9  46.6% | batch:       139 of       298\t|\tloss: 0.156917\n",
      "Training Epoch 9  47.0% | batch:       140 of       298\t|\tloss: 0.499568\n",
      "Training Epoch 9  47.3% | batch:       141 of       298\t|\tloss: 1.66921\n",
      "Training Epoch 9  47.7% | batch:       142 of       298\t|\tloss: 0.183885\n",
      "Training Epoch 9  48.0% | batch:       143 of       298\t|\tloss: 0.196482\n",
      "Training Epoch 9  48.3% | batch:       144 of       298\t|\tloss: 0.18506\n",
      "Training Epoch 9  48.7% | batch:       145 of       298\t|\tloss: 0.433921\n",
      "Training Epoch 9  49.0% | batch:       146 of       298\t|\tloss: 0.290396\n",
      "Training Epoch 9  49.3% | batch:       147 of       298\t|\tloss: 0.30353\n",
      "Training Epoch 9  49.7% | batch:       148 of       298\t|\tloss: 0.199196\n",
      "Training Epoch 9  50.0% | batch:       149 of       298\t|\tloss: 0.363091\n",
      "Training Epoch 9  50.3% | batch:       150 of       298\t|\tloss: 0.196917\n",
      "Training Epoch 9  50.7% | batch:       151 of       298\t|\tloss: 0.234258\n",
      "Training Epoch 9  51.0% | batch:       152 of       298\t|\tloss: 0.404292\n",
      "Training Epoch 9  51.3% | batch:       153 of       298\t|\tloss: 0.402986\n",
      "Training Epoch 9  51.7% | batch:       154 of       298\t|\tloss: 0.225968\n",
      "Training Epoch 9  52.0% | batch:       155 of       298\t|\tloss: 0.196829\n",
      "Training Epoch 9  52.3% | batch:       156 of       298\t|\tloss: 0.20182\n",
      "Training Epoch 9  52.7% | batch:       157 of       298\t|\tloss: 0.170577\n",
      "Training Epoch 9  53.0% | batch:       158 of       298\t|\tloss: 0.208081\n",
      "Training Epoch 9  53.4% | batch:       159 of       298\t|\tloss: 0.352421\n",
      "Training Epoch 9  53.7% | batch:       160 of       298\t|\tloss: 0.307087\n",
      "Training Epoch 9  54.0% | batch:       161 of       298\t|\tloss: 1.9238\n",
      "Training Epoch 9  54.4% | batch:       162 of       298\t|\tloss: 0.2026\n",
      "Training Epoch 9  54.7% | batch:       163 of       298\t|\tloss: 0.208148\n",
      "Training Epoch 9  55.0% | batch:       164 of       298\t|\tloss: 0.184678\n",
      "Training Epoch 9  55.4% | batch:       165 of       298\t|\tloss: 0.168339\n",
      "Training Epoch 9  55.7% | batch:       166 of       298\t|\tloss: 0.258196\n",
      "Training Epoch 9  56.0% | batch:       167 of       298\t|\tloss: 0.294172\n",
      "Training Epoch 9  56.4% | batch:       168 of       298\t|\tloss: 0.156993\n",
      "Training Epoch 9  56.7% | batch:       169 of       298\t|\tloss: 0.270849\n",
      "Training Epoch 9  57.0% | batch:       170 of       298\t|\tloss: 0.183668\n",
      "Training Epoch 9  57.4% | batch:       171 of       298\t|\tloss: 0.189692\n",
      "Training Epoch 9  57.7% | batch:       172 of       298\t|\tloss: 0.131116\n",
      "Training Epoch 9  58.1% | batch:       173 of       298\t|\tloss: 0.225239\n",
      "Training Epoch 9  58.4% | batch:       174 of       298\t|\tloss: 0.197174\n",
      "Training Epoch 9  58.7% | batch:       175 of       298\t|\tloss: 0.214163\n",
      "Training Epoch 9  59.1% | batch:       176 of       298\t|\tloss: 0.297037\n",
      "Training Epoch 9  59.4% | batch:       177 of       298\t|\tloss: 0.174868\n",
      "Training Epoch 9  59.7% | batch:       178 of       298\t|\tloss: 0.223973\n",
      "Training Epoch 9  60.1% | batch:       179 of       298\t|\tloss: 0.179577\n",
      "Training Epoch 9  60.4% | batch:       180 of       298\t|\tloss: 2.3703\n",
      "Training Epoch 9  60.7% | batch:       181 of       298\t|\tloss: 0.162614\n",
      "Training Epoch 9  61.1% | batch:       182 of       298\t|\tloss: 3.04947\n",
      "Training Epoch 9  61.4% | batch:       183 of       298\t|\tloss: 0.258163\n",
      "Training Epoch 9  61.7% | batch:       184 of       298\t|\tloss: 0.375888\n",
      "Training Epoch 9  62.1% | batch:       185 of       298\t|\tloss: 0.163612\n",
      "Training Epoch 9  62.4% | batch:       186 of       298\t|\tloss: 0.176943\n",
      "Training Epoch 9  62.8% | batch:       187 of       298\t|\tloss: 0.262955\n",
      "Training Epoch 9  63.1% | batch:       188 of       298\t|\tloss: 0.202042\n",
      "Training Epoch 9  63.4% | batch:       189 of       298\t|\tloss: 0.208203\n",
      "Training Epoch 9  63.8% | batch:       190 of       298\t|\tloss: 0.138473\n",
      "Training Epoch 9  64.1% | batch:       191 of       298\t|\tloss: 0.15403\n",
      "Training Epoch 9  64.4% | batch:       192 of       298\t|\tloss: 0.16009\n",
      "Training Epoch 9  64.8% | batch:       193 of       298\t|\tloss: 0.302069\n",
      "Training Epoch 9  65.1% | batch:       194 of       298\t|\tloss: 0.259674\n",
      "Training Epoch 9  65.4% | batch:       195 of       298\t|\tloss: 0.267242\n",
      "Training Epoch 9  65.8% | batch:       196 of       298\t|\tloss: 0.311073\n",
      "Training Epoch 9  66.1% | batch:       197 of       298\t|\tloss: 0.623583\n",
      "Training Epoch 9  66.4% | batch:       198 of       298\t|\tloss: 0.168861\n",
      "Training Epoch 9  66.8% | batch:       199 of       298\t|\tloss: 0.201138\n",
      "Training Epoch 9  67.1% | batch:       200 of       298\t|\tloss: 0.267531\n",
      "Training Epoch 9  67.4% | batch:       201 of       298\t|\tloss: 0.286354\n",
      "Training Epoch 9  67.8% | batch:       202 of       298\t|\tloss: 0.243589\n",
      "Training Epoch 9  68.1% | batch:       203 of       298\t|\tloss: 0.334922\n",
      "Training Epoch 9  68.5% | batch:       204 of       298\t|\tloss: 0.15405\n",
      "Training Epoch 9  68.8% | batch:       205 of       298\t|\tloss: 0.162569\n",
      "Training Epoch 9  69.1% | batch:       206 of       298\t|\tloss: 0.282273\n",
      "Training Epoch 9  69.5% | batch:       207 of       298\t|\tloss: 0.220354\n",
      "Training Epoch 9  69.8% | batch:       208 of       298\t|\tloss: 0.206311\n",
      "Training Epoch 9  70.1% | batch:       209 of       298\t|\tloss: 0.699563\n",
      "Training Epoch 9  70.5% | batch:       210 of       298\t|\tloss: 0.212903\n",
      "Training Epoch 9  70.8% | batch:       211 of       298\t|\tloss: 0.20223\n",
      "Training Epoch 9  71.1% | batch:       212 of       298\t|\tloss: 0.190334\n",
      "Training Epoch 9  71.5% | batch:       213 of       298\t|\tloss: 0.18784\n",
      "Training Epoch 9  71.8% | batch:       214 of       298\t|\tloss: 0.332659\n",
      "Training Epoch 9  72.1% | batch:       215 of       298\t|\tloss: 0.195992\n",
      "Training Epoch 9  72.5% | batch:       216 of       298\t|\tloss: 0.18172\n",
      "Training Epoch 9  72.8% | batch:       217 of       298\t|\tloss: 0.230036\n",
      "Training Epoch 9  73.2% | batch:       218 of       298\t|\tloss: 0.278571\n",
      "Training Epoch 9  73.5% | batch:       219 of       298\t|\tloss: 0.410961\n",
      "Training Epoch 9  73.8% | batch:       220 of       298\t|\tloss: 0.295901\n",
      "Training Epoch 9  74.2% | batch:       221 of       298\t|\tloss: 0.282975\n",
      "Training Epoch 9  74.5% | batch:       222 of       298\t|\tloss: 0.257719\n",
      "Training Epoch 9  74.8% | batch:       223 of       298\t|\tloss: 0.167943\n",
      "Training Epoch 9  75.2% | batch:       224 of       298\t|\tloss: 0.158544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  75.5% | batch:       225 of       298\t|\tloss: 0.163196\n",
      "Training Epoch 9  75.8% | batch:       226 of       298\t|\tloss: 0.275672\n",
      "Training Epoch 9  76.2% | batch:       227 of       298\t|\tloss: 0.308228\n",
      "Training Epoch 9  76.5% | batch:       228 of       298\t|\tloss: 0.209477\n",
      "Training Epoch 9  76.8% | batch:       229 of       298\t|\tloss: 0.365727\n",
      "Training Epoch 9  77.2% | batch:       230 of       298\t|\tloss: 0.211901\n",
      "Training Epoch 9  77.5% | batch:       231 of       298\t|\tloss: 0.201749\n",
      "Training Epoch 9  77.9% | batch:       232 of       298\t|\tloss: 0.206119\n",
      "Training Epoch 9  78.2% | batch:       233 of       298\t|\tloss: 0.737889\n",
      "Training Epoch 9  78.5% | batch:       234 of       298\t|\tloss: 0.284569\n",
      "Training Epoch 9  78.9% | batch:       235 of       298\t|\tloss: 0.219803\n",
      "Training Epoch 9  79.2% | batch:       236 of       298\t|\tloss: 0.234753\n",
      "Training Epoch 9  79.5% | batch:       237 of       298\t|\tloss: 0.409397\n",
      "Training Epoch 9  79.9% | batch:       238 of       298\t|\tloss: 0.253204\n",
      "Training Epoch 9  80.2% | batch:       239 of       298\t|\tloss: 0.207651\n",
      "Training Epoch 9  80.5% | batch:       240 of       298\t|\tloss: 0.537801\n",
      "Training Epoch 9  80.9% | batch:       241 of       298\t|\tloss: 0.263576\n",
      "Training Epoch 9  81.2% | batch:       242 of       298\t|\tloss: 0.254836\n",
      "Training Epoch 9  81.5% | batch:       243 of       298\t|\tloss: 0.169487\n",
      "Training Epoch 9  81.9% | batch:       244 of       298\t|\tloss: 0.177499\n",
      "Training Epoch 9  82.2% | batch:       245 of       298\t|\tloss: 0.269144\n",
      "Training Epoch 9  82.6% | batch:       246 of       298\t|\tloss: 0.191828\n",
      "Training Epoch 9  82.9% | batch:       247 of       298\t|\tloss: 0.2363\n",
      "Training Epoch 9  83.2% | batch:       248 of       298\t|\tloss: 0.237096\n",
      "Training Epoch 9  83.6% | batch:       249 of       298\t|\tloss: 0.270559\n",
      "Training Epoch 9  83.9% | batch:       250 of       298\t|\tloss: 0.318315\n",
      "Training Epoch 9  84.2% | batch:       251 of       298\t|\tloss: 0.164989\n",
      "Training Epoch 9  84.6% | batch:       252 of       298\t|\tloss: 0.281763\n",
      "Training Epoch 9  84.9% | batch:       253 of       298\t|\tloss: 0.171868\n",
      "Training Epoch 9  85.2% | batch:       254 of       298\t|\tloss: 0.192746\n",
      "Training Epoch 9  85.6% | batch:       255 of       298\t|\tloss: 2.16905\n",
      "Training Epoch 9  85.9% | batch:       256 of       298\t|\tloss: 0.227594\n",
      "Training Epoch 9  86.2% | batch:       257 of       298\t|\tloss: 0.17748\n",
      "Training Epoch 9  86.6% | batch:       258 of       298\t|\tloss: 0.419619\n",
      "Training Epoch 9  86.9% | batch:       259 of       298\t|\tloss: 0.185863\n",
      "Training Epoch 9  87.2% | batch:       260 of       298\t|\tloss: 0.201243\n",
      "Training Epoch 9  87.6% | batch:       261 of       298\t|\tloss: 0.163971\n",
      "Training Epoch 9  87.9% | batch:       262 of       298\t|\tloss: 0.21075\n",
      "Training Epoch 9  88.3% | batch:       263 of       298\t|\tloss: 0.204075\n",
      "Training Epoch 9  88.6% | batch:       264 of       298\t|\tloss: 0.174606\n",
      "Training Epoch 9  88.9% | batch:       265 of       298\t|\tloss: 0.187129\n",
      "Training Epoch 9  89.3% | batch:       266 of       298\t|\tloss: 0.180347\n",
      "Training Epoch 9  89.6% | batch:       267 of       298\t|\tloss: 0.177311\n",
      "Training Epoch 9  89.9% | batch:       268 of       298\t|\tloss: 0.375921\n",
      "Training Epoch 9  90.3% | batch:       269 of       298\t|\tloss: 0.182553\n",
      "Training Epoch 9  90.6% | batch:       270 of       298\t|\tloss: 0.237721\n",
      "Training Epoch 9  90.9% | batch:       271 of       298\t|\tloss: 0.251852\n",
      "Training Epoch 9  91.3% | batch:       272 of       298\t|\tloss: 0.268403\n",
      "Training Epoch 9  91.6% | batch:       273 of       298\t|\tloss: 0.29212\n",
      "Training Epoch 9  91.9% | batch:       274 of       298\t|\tloss: 0.263419\n",
      "Training Epoch 9  92.3% | batch:       275 of       298\t|\tloss: 0.190437\n",
      "Training Epoch 9  92.6% | batch:       276 of       298\t|\tloss: 0.196089\n",
      "Training Epoch 9  93.0% | batch:       277 of       298\t|\tloss: 0.258958\n",
      "Training Epoch 9  93.3% | batch:       278 of       298\t|\tloss: 0.209192\n",
      "Training Epoch 9  93.6% | batch:       279 of       298\t|\tloss: 0.17707\n",
      "Training Epoch 9  94.0% | batch:       280 of       298\t|\tloss: 0.238229\n",
      "Training Epoch 9  94.3% | batch:       281 of       298\t|\tloss: 0.128891\n",
      "Training Epoch 9  94.6% | batch:       282 of       298\t|\tloss: 0.172219\n",
      "Training Epoch 9  95.0% | batch:       283 of       298\t|\tloss: 0.247532\n",
      "Training Epoch 9  95.3% | batch:       284 of       298\t|\tloss: 0.181771\n",
      "Training Epoch 9  95.6% | batch:       285 of       298\t|\tloss: 0.282792\n",
      "Training Epoch 9  96.0% | batch:       286 of       298\t|\tloss: 0.245743\n",
      "Training Epoch 9  96.3% | batch:       287 of       298\t|\tloss: 0.2448\n",
      "Training Epoch 9  96.6% | batch:       288 of       298\t|\tloss: 0.229255\n",
      "Training Epoch 9  97.0% | batch:       289 of       298\t|\tloss: 0.216423\n",
      "Training Epoch 9  97.3% | batch:       290 of       298\t|\tloss: 0.417678\n",
      "Training Epoch 9  97.7% | batch:       291 of       298\t|\tloss: 0.221165\n",
      "Training Epoch 9  98.0% | batch:       292 of       298\t|\tloss: 0.355195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:37:58,597 | INFO : Epoch 9 Training Summary: epoch: 9.000000 | loss: 0.326952 | \n",
      "2023-06-22 14:37:58,598 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.455394983291626 seconds\n",
      "\n",
      "2023-06-22 14:37:58,599 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.283434496985542 seconds\n",
      "2023-06-22 14:37:58,600 | INFO : Avg batch train. time: 0.041219578848944775 seconds\n",
      "2023-06-22 14:37:58,601 | INFO : Avg sample train. time: 0.0012883820533863585 seconds\n",
      "Training Epoch:   2%|▏         | 9/400 [01:57<1:23:43, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 9  98.3% | batch:       293 of       298\t|\tloss: 0.276695\n",
      "Training Epoch 9  98.7% | batch:       294 of       298\t|\tloss: 0.149899\n",
      "Training Epoch 9  99.0% | batch:       295 of       298\t|\tloss: 0.529277\n",
      "Training Epoch 9  99.3% | batch:       296 of       298\t|\tloss: 0.186953\n",
      "Training Epoch 9  99.7% | batch:       297 of       298\t|\tloss: 0.774705\n",
      "\n",
      "Training Epoch 10   0.0% | batch:         0 of       298\t|\tloss: 0.174564\n",
      "Training Epoch 10   0.3% | batch:         1 of       298\t|\tloss: 0.218953\n",
      "Training Epoch 10   0.7% | batch:         2 of       298\t|\tloss: 0.193251\n",
      "Training Epoch 10   1.0% | batch:         3 of       298\t|\tloss: 0.197531\n",
      "Training Epoch 10   1.3% | batch:         4 of       298\t|\tloss: 0.130351\n",
      "Training Epoch 10   1.7% | batch:         5 of       298\t|\tloss: 0.355452\n",
      "Training Epoch 10   2.0% | batch:         6 of       298\t|\tloss: 0.233636\n",
      "Training Epoch 10   2.3% | batch:         7 of       298\t|\tloss: 0.202525\n",
      "Training Epoch 10   2.7% | batch:         8 of       298\t|\tloss: 0.223706\n",
      "Training Epoch 10   3.0% | batch:         9 of       298\t|\tloss: 0.311097\n",
      "Training Epoch 10   3.4% | batch:        10 of       298\t|\tloss: 0.197078\n",
      "Training Epoch 10   3.7% | batch:        11 of       298\t|\tloss: 0.283143\n",
      "Training Epoch 10   4.0% | batch:        12 of       298\t|\tloss: 0.195165\n",
      "Training Epoch 10   4.4% | batch:        13 of       298\t|\tloss: 0.287228\n",
      "Training Epoch 10   4.7% | batch:        14 of       298\t|\tloss: 0.186297\n",
      "Training Epoch 10   5.0% | batch:        15 of       298\t|\tloss: 0.213046\n",
      "Training Epoch 10   5.4% | batch:        16 of       298\t|\tloss: 0.187563\n",
      "Training Epoch 10   5.7% | batch:        17 of       298\t|\tloss: 0.154152\n",
      "Training Epoch 10   6.0% | batch:        18 of       298\t|\tloss: 0.179222\n",
      "Training Epoch 10   6.4% | batch:        19 of       298\t|\tloss: 0.21392\n",
      "Training Epoch 10   6.7% | batch:        20 of       298\t|\tloss: 0.59957\n",
      "Training Epoch 10   7.0% | batch:        21 of       298\t|\tloss: 0.225104\n",
      "Training Epoch 10   7.4% | batch:        22 of       298\t|\tloss: 0.20878\n",
      "Training Epoch 10   7.7% | batch:        23 of       298\t|\tloss: 0.17156\n",
      "Training Epoch 10   8.1% | batch:        24 of       298\t|\tloss: 0.299563\n",
      "Training Epoch 10   8.4% | batch:        25 of       298\t|\tloss: 0.192721\n",
      "Training Epoch 10   8.7% | batch:        26 of       298\t|\tloss: 0.134639\n",
      "Training Epoch 10   9.1% | batch:        27 of       298\t|\tloss: 0.39616\n",
      "Training Epoch 10   9.4% | batch:        28 of       298\t|\tloss: 0.452525\n",
      "Training Epoch 10   9.7% | batch:        29 of       298\t|\tloss: 1.67917\n",
      "Training Epoch 10  10.1% | batch:        30 of       298\t|\tloss: 0.169649\n",
      "Training Epoch 10  10.4% | batch:        31 of       298\t|\tloss: 0.205641\n",
      "Training Epoch 10  10.7% | batch:        32 of       298\t|\tloss: 0.183513\n",
      "Training Epoch 10  11.1% | batch:        33 of       298\t|\tloss: 0.226145\n",
      "Training Epoch 10  11.4% | batch:        34 of       298\t|\tloss: 0.252582\n",
      "Training Epoch 10  11.7% | batch:        35 of       298\t|\tloss: 0.163177\n",
      "Training Epoch 10  12.1% | batch:        36 of       298\t|\tloss: 0.176259\n",
      "Training Epoch 10  12.4% | batch:        37 of       298\t|\tloss: 1.40917\n",
      "Training Epoch 10  12.8% | batch:        38 of       298\t|\tloss: 0.180487\n",
      "Training Epoch 10  13.1% | batch:        39 of       298\t|\tloss: 0.205285\n",
      "Training Epoch 10  13.4% | batch:        40 of       298\t|\tloss: 0.261248\n",
      "Training Epoch 10  13.8% | batch:        41 of       298\t|\tloss: 0.15431\n",
      "Training Epoch 10  14.1% | batch:        42 of       298\t|\tloss: 0.190416\n",
      "Training Epoch 10  14.4% | batch:        43 of       298\t|\tloss: 0.263107\n",
      "Training Epoch 10  14.8% | batch:        44 of       298\t|\tloss: 0.212453\n",
      "Training Epoch 10  15.1% | batch:        45 of       298\t|\tloss: 0.161811\n",
      "Training Epoch 10  15.4% | batch:        46 of       298\t|\tloss: 0.161573\n",
      "Training Epoch 10  15.8% | batch:        47 of       298\t|\tloss: 0.204606\n",
      "Training Epoch 10  16.1% | batch:        48 of       298\t|\tloss: 0.174817\n",
      "Training Epoch 10  16.4% | batch:        49 of       298\t|\tloss: 0.217588\n",
      "Training Epoch 10  16.8% | batch:        50 of       298\t|\tloss: 0.181657\n",
      "Training Epoch 10  17.1% | batch:        51 of       298\t|\tloss: 0.158282\n",
      "Training Epoch 10  17.4% | batch:        52 of       298\t|\tloss: 0.292471\n",
      "Training Epoch 10  17.8% | batch:        53 of       298\t|\tloss: 0.23991\n",
      "Training Epoch 10  18.1% | batch:        54 of       298\t|\tloss: 0.215087\n",
      "Training Epoch 10  18.5% | batch:        55 of       298\t|\tloss: 0.23429\n",
      "Training Epoch 10  18.8% | batch:        56 of       298\t|\tloss: 0.205051\n",
      "Training Epoch 10  19.1% | batch:        57 of       298\t|\tloss: 0.524025\n",
      "Training Epoch 10  19.5% | batch:        58 of       298\t|\tloss: 0.27674\n",
      "Training Epoch 10  19.8% | batch:        59 of       298\t|\tloss: 0.345807\n",
      "Training Epoch 10  20.1% | batch:        60 of       298\t|\tloss: 0.265476\n",
      "Training Epoch 10  20.5% | batch:        61 of       298\t|\tloss: 0.207596\n",
      "Training Epoch 10  20.8% | batch:        62 of       298\t|\tloss: 0.205154\n",
      "Training Epoch 10  21.1% | batch:        63 of       298\t|\tloss: 0.934331\n",
      "Training Epoch 10  21.5% | batch:        64 of       298\t|\tloss: 0.169746\n",
      "Training Epoch 10  21.8% | batch:        65 of       298\t|\tloss: 0.149804\n",
      "Training Epoch 10  22.1% | batch:        66 of       298\t|\tloss: 0.23825\n",
      "Training Epoch 10  22.5% | batch:        67 of       298\t|\tloss: 0.35977\n",
      "Training Epoch 10  22.8% | batch:        68 of       298\t|\tloss: 0.912695\n",
      "Training Epoch 10  23.2% | batch:        69 of       298\t|\tloss: 0.217229\n",
      "Training Epoch 10  23.5% | batch:        70 of       298\t|\tloss: 0.244503\n",
      "Training Epoch 10  23.8% | batch:        71 of       298\t|\tloss: 0.201973\n",
      "Training Epoch 10  24.2% | batch:        72 of       298\t|\tloss: 0.163395\n",
      "Training Epoch 10  24.5% | batch:        73 of       298\t|\tloss: 0.213785\n",
      "Training Epoch 10  24.8% | batch:        74 of       298\t|\tloss: 0.240894\n",
      "Training Epoch 10  25.2% | batch:        75 of       298\t|\tloss: 0.273208\n",
      "Training Epoch 10  25.5% | batch:        76 of       298\t|\tloss: 0.197403\n",
      "Training Epoch 10  25.8% | batch:        77 of       298\t|\tloss: 0.152287\n",
      "Training Epoch 10  26.2% | batch:        78 of       298\t|\tloss: 0.241125\n",
      "Training Epoch 10  26.5% | batch:        79 of       298\t|\tloss: 0.204154\n",
      "Training Epoch 10  26.8% | batch:        80 of       298\t|\tloss: 0.543362\n",
      "Training Epoch 10  27.2% | batch:        81 of       298\t|\tloss: 0.188589\n",
      "Training Epoch 10  27.5% | batch:        82 of       298\t|\tloss: 0.26205\n",
      "Training Epoch 10  27.9% | batch:        83 of       298\t|\tloss: 0.281708\n",
      "Training Epoch 10  28.2% | batch:        84 of       298\t|\tloss: 0.204811\n",
      "Training Epoch 10  28.5% | batch:        85 of       298\t|\tloss: 0.201825\n",
      "Training Epoch 10  28.9% | batch:        86 of       298\t|\tloss: 0.244333\n",
      "Training Epoch 10  29.2% | batch:        87 of       298\t|\tloss: 0.380044\n",
      "Training Epoch 10  29.5% | batch:        88 of       298\t|\tloss: 0.147752\n",
      "Training Epoch 10  29.9% | batch:        89 of       298\t|\tloss: 0.189089\n",
      "Training Epoch 10  30.2% | batch:        90 of       298\t|\tloss: 0.206967\n",
      "Training Epoch 10  30.5% | batch:        91 of       298\t|\tloss: 0.374514\n",
      "Training Epoch 10  30.9% | batch:        92 of       298\t|\tloss: 0.151879\n",
      "Training Epoch 10  31.2% | batch:        93 of       298\t|\tloss: 0.183451\n",
      "Training Epoch 10  31.5% | batch:        94 of       298\t|\tloss: 0.266945\n",
      "Training Epoch 10  31.9% | batch:        95 of       298\t|\tloss: 0.202828\n",
      "Training Epoch 10  32.2% | batch:        96 of       298\t|\tloss: 0.19781\n",
      "Training Epoch 10  32.6% | batch:        97 of       298\t|\tloss: 0.648274\n",
      "Training Epoch 10  32.9% | batch:        98 of       298\t|\tloss: 0.211435\n",
      "Training Epoch 10  33.2% | batch:        99 of       298\t|\tloss: 0.249827\n",
      "Training Epoch 10  33.6% | batch:       100 of       298\t|\tloss: 1.45341\n",
      "Training Epoch 10  33.9% | batch:       101 of       298\t|\tloss: 0.232078\n",
      "Training Epoch 10  34.2% | batch:       102 of       298\t|\tloss: 0.204929\n",
      "Training Epoch 10  34.6% | batch:       103 of       298\t|\tloss: 0.222917\n",
      "Training Epoch 10  34.9% | batch:       104 of       298\t|\tloss: 0.172995\n",
      "Training Epoch 10  35.2% | batch:       105 of       298\t|\tloss: 0.225119\n",
      "Training Epoch 10  35.6% | batch:       106 of       298\t|\tloss: 0.412547\n",
      "Training Epoch 10  35.9% | batch:       107 of       298\t|\tloss: 0.218957\n",
      "Training Epoch 10  36.2% | batch:       108 of       298\t|\tloss: 0.257531\n",
      "Training Epoch 10  36.6% | batch:       109 of       298\t|\tloss: 0.194876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  36.9% | batch:       110 of       298\t|\tloss: 0.171441\n",
      "Training Epoch 10  37.2% | batch:       111 of       298\t|\tloss: 2.08625\n",
      "Training Epoch 10  37.6% | batch:       112 of       298\t|\tloss: 0.320077\n",
      "Training Epoch 10  37.9% | batch:       113 of       298\t|\tloss: 0.238955\n",
      "Training Epoch 10  38.3% | batch:       114 of       298\t|\tloss: 0.261396\n",
      "Training Epoch 10  38.6% | batch:       115 of       298\t|\tloss: 0.161828\n",
      "Training Epoch 10  38.9% | batch:       116 of       298\t|\tloss: 0.240074\n",
      "Training Epoch 10  39.3% | batch:       117 of       298\t|\tloss: 0.357485\n",
      "Training Epoch 10  39.6% | batch:       118 of       298\t|\tloss: 0.277759\n",
      "Training Epoch 10  39.9% | batch:       119 of       298\t|\tloss: 0.186901\n",
      "Training Epoch 10  40.3% | batch:       120 of       298\t|\tloss: 0.187827\n",
      "Training Epoch 10  40.6% | batch:       121 of       298\t|\tloss: 0.182721\n",
      "Training Epoch 10  40.9% | batch:       122 of       298\t|\tloss: 0.192557\n",
      "Training Epoch 10  41.3% | batch:       123 of       298\t|\tloss: 0.197183\n",
      "Training Epoch 10  41.6% | batch:       124 of       298\t|\tloss: 0.174436\n",
      "Training Epoch 10  41.9% | batch:       125 of       298\t|\tloss: 0.213194\n",
      "Training Epoch 10  42.3% | batch:       126 of       298\t|\tloss: 0.212509\n",
      "Training Epoch 10  42.6% | batch:       127 of       298\t|\tloss: 0.203879\n",
      "Training Epoch 10  43.0% | batch:       128 of       298\t|\tloss: 0.616508\n",
      "Training Epoch 10  43.3% | batch:       129 of       298\t|\tloss: 0.183398\n",
      "Training Epoch 10  43.6% | batch:       130 of       298\t|\tloss: 0.186451\n",
      "Training Epoch 10  44.0% | batch:       131 of       298\t|\tloss: 0.20553\n",
      "Training Epoch 10  44.3% | batch:       132 of       298\t|\tloss: 0.179321\n",
      "Training Epoch 10  44.6% | batch:       133 of       298\t|\tloss: 0.179559\n",
      "Training Epoch 10  45.0% | batch:       134 of       298\t|\tloss: 0.5328\n",
      "Training Epoch 10  45.3% | batch:       135 of       298\t|\tloss: 0.167747\n",
      "Training Epoch 10  45.6% | batch:       136 of       298\t|\tloss: 0.18718\n",
      "Training Epoch 10  46.0% | batch:       137 of       298\t|\tloss: 1.54647\n",
      "Training Epoch 10  46.3% | batch:       138 of       298\t|\tloss: 0.160836\n",
      "Training Epoch 10  46.6% | batch:       139 of       298\t|\tloss: 7.24426\n",
      "Training Epoch 10  47.0% | batch:       140 of       298\t|\tloss: 0.222165\n",
      "Training Epoch 10  47.3% | batch:       141 of       298\t|\tloss: 0.275955\n",
      "Training Epoch 10  47.7% | batch:       142 of       298\t|\tloss: 0.210168\n",
      "Training Epoch 10  48.0% | batch:       143 of       298\t|\tloss: 0.166698\n",
      "Training Epoch 10  48.3% | batch:       144 of       298\t|\tloss: 0.176838\n",
      "Training Epoch 10  48.7% | batch:       145 of       298\t|\tloss: 0.365672\n",
      "Training Epoch 10  49.0% | batch:       146 of       298\t|\tloss: 0.259778\n",
      "Training Epoch 10  49.3% | batch:       147 of       298\t|\tloss: 0.266707\n",
      "Training Epoch 10  49.7% | batch:       148 of       298\t|\tloss: 0.20472\n",
      "Training Epoch 10  50.0% | batch:       149 of       298\t|\tloss: 0.158004\n",
      "Training Epoch 10  50.3% | batch:       150 of       298\t|\tloss: 0.235763\n",
      "Training Epoch 10  50.7% | batch:       151 of       298\t|\tloss: 0.196092\n",
      "Training Epoch 10  51.0% | batch:       152 of       298\t|\tloss: 0.291935\n",
      "Training Epoch 10  51.3% | batch:       153 of       298\t|\tloss: 0.483751\n",
      "Training Epoch 10  51.7% | batch:       154 of       298\t|\tloss: 0.140081\n",
      "Training Epoch 10  52.0% | batch:       155 of       298\t|\tloss: 0.206106\n",
      "Training Epoch 10  52.3% | batch:       156 of       298\t|\tloss: 0.24526\n",
      "Training Epoch 10  52.7% | batch:       157 of       298\t|\tloss: 1.37051\n",
      "Training Epoch 10  53.0% | batch:       158 of       298\t|\tloss: 0.181325\n",
      "Training Epoch 10  53.4% | batch:       159 of       298\t|\tloss: 0.367355\n",
      "Training Epoch 10  53.7% | batch:       160 of       298\t|\tloss: 0.146335\n",
      "Training Epoch 10  54.0% | batch:       161 of       298\t|\tloss: 0.19674\n",
      "Training Epoch 10  54.4% | batch:       162 of       298\t|\tloss: 0.177937\n",
      "Training Epoch 10  54.7% | batch:       163 of       298\t|\tloss: 0.187242\n",
      "Training Epoch 10  55.0% | batch:       164 of       298\t|\tloss: 0.231177\n",
      "Training Epoch 10  55.4% | batch:       165 of       298\t|\tloss: 0.269074\n",
      "Training Epoch 10  55.7% | batch:       166 of       298\t|\tloss: 0.242239\n",
      "Training Epoch 10  56.0% | batch:       167 of       298\t|\tloss: 0.204988\n",
      "Training Epoch 10  56.4% | batch:       168 of       298\t|\tloss: 0.239407\n",
      "Training Epoch 10  56.7% | batch:       169 of       298\t|\tloss: 0.199261\n",
      "Training Epoch 10  57.0% | batch:       170 of       298\t|\tloss: 0.294392\n",
      "Training Epoch 10  57.4% | batch:       171 of       298\t|\tloss: 0.187619\n",
      "Training Epoch 10  57.7% | batch:       172 of       298\t|\tloss: 0.17486\n",
      "Training Epoch 10  58.1% | batch:       173 of       298\t|\tloss: 0.193633\n",
      "Training Epoch 10  58.4% | batch:       174 of       298\t|\tloss: 0.229741\n",
      "Training Epoch 10  58.7% | batch:       175 of       298\t|\tloss: 0.279839\n",
      "Training Epoch 10  59.1% | batch:       176 of       298\t|\tloss: 0.202013\n",
      "Training Epoch 10  59.4% | batch:       177 of       298\t|\tloss: 0.200416\n",
      "Training Epoch 10  59.7% | batch:       178 of       298\t|\tloss: 0.189182\n",
      "Training Epoch 10  60.1% | batch:       179 of       298\t|\tloss: 3.13683\n",
      "Training Epoch 10  60.4% | batch:       180 of       298\t|\tloss: 0.14804\n",
      "Training Epoch 10  60.7% | batch:       181 of       298\t|\tloss: 0.21891\n",
      "Training Epoch 10  61.1% | batch:       182 of       298\t|\tloss: 0.288142\n",
      "Training Epoch 10  61.4% | batch:       183 of       298\t|\tloss: 0.176842\n",
      "Training Epoch 10  61.7% | batch:       184 of       298\t|\tloss: 0.296354\n",
      "Training Epoch 10  62.1% | batch:       185 of       298\t|\tloss: 0.233781\n",
      "Training Epoch 10  62.4% | batch:       186 of       298\t|\tloss: 0.187578\n",
      "Training Epoch 10  62.8% | batch:       187 of       298\t|\tloss: 0.182043\n",
      "Training Epoch 10  63.1% | batch:       188 of       298\t|\tloss: 0.15773\n",
      "Training Epoch 10  63.4% | batch:       189 of       298\t|\tloss: 0.380392\n",
      "Training Epoch 10  63.8% | batch:       190 of       298\t|\tloss: 0.190897\n",
      "Training Epoch 10  64.1% | batch:       191 of       298\t|\tloss: 0.227027\n",
      "Training Epoch 10  64.4% | batch:       192 of       298\t|\tloss: 0.258516\n",
      "Training Epoch 10  64.8% | batch:       193 of       298\t|\tloss: 0.233216\n",
      "Training Epoch 10  65.1% | batch:       194 of       298\t|\tloss: 0.226348\n",
      "Training Epoch 10  65.4% | batch:       195 of       298\t|\tloss: 0.165745\n",
      "Training Epoch 10  65.8% | batch:       196 of       298\t|\tloss: 0.427544\n",
      "Training Epoch 10  66.1% | batch:       197 of       298\t|\tloss: 0.19711\n",
      "Training Epoch 10  66.4% | batch:       198 of       298\t|\tloss: 0.119196\n",
      "Training Epoch 10  66.8% | batch:       199 of       298\t|\tloss: 0.221843\n",
      "Training Epoch 10  67.1% | batch:       200 of       298\t|\tloss: 0.213253\n",
      "Training Epoch 10  67.4% | batch:       201 of       298\t|\tloss: 0.145502\n",
      "Training Epoch 10  67.8% | batch:       202 of       298\t|\tloss: 0.183176\n",
      "Training Epoch 10  68.1% | batch:       203 of       298\t|\tloss: 0.50712\n",
      "Training Epoch 10  68.5% | batch:       204 of       298\t|\tloss: 0.197355\n",
      "Training Epoch 10  68.8% | batch:       205 of       298\t|\tloss: 0.230408\n",
      "Training Epoch 10  69.1% | batch:       206 of       298\t|\tloss: 0.164153\n",
      "Training Epoch 10  69.5% | batch:       207 of       298\t|\tloss: 0.180793\n",
      "Training Epoch 10  69.8% | batch:       208 of       298\t|\tloss: 0.320061\n",
      "Training Epoch 10  70.1% | batch:       209 of       298\t|\tloss: 1.16513\n",
      "Training Epoch 10  70.5% | batch:       210 of       298\t|\tloss: 0.271646\n",
      "Training Epoch 10  70.8% | batch:       211 of       298\t|\tloss: 0.147823\n",
      "Training Epoch 10  71.1% | batch:       212 of       298\t|\tloss: 2.02119\n",
      "Training Epoch 10  71.5% | batch:       213 of       298\t|\tloss: 0.159489\n",
      "Training Epoch 10  71.8% | batch:       214 of       298\t|\tloss: 0.141449\n",
      "Training Epoch 10  72.1% | batch:       215 of       298\t|\tloss: 0.259562\n",
      "Training Epoch 10  72.5% | batch:       216 of       298\t|\tloss: 0.275203\n",
      "Training Epoch 10  72.8% | batch:       217 of       298\t|\tloss: 0.240437\n",
      "Training Epoch 10  73.2% | batch:       218 of       298\t|\tloss: 0.221653\n",
      "Training Epoch 10  73.5% | batch:       219 of       298\t|\tloss: 0.234183\n",
      "Training Epoch 10  73.8% | batch:       220 of       298\t|\tloss: 0.176413\n",
      "Training Epoch 10  74.2% | batch:       221 of       298\t|\tloss: 1.71557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  74.5% | batch:       222 of       298\t|\tloss: 0.195155\n",
      "Training Epoch 10  74.8% | batch:       223 of       298\t|\tloss: 0.210635\n",
      "Training Epoch 10  75.2% | batch:       224 of       298\t|\tloss: 1.09334\n",
      "Training Epoch 10  75.5% | batch:       225 of       298\t|\tloss: 0.173561\n",
      "Training Epoch 10  75.8% | batch:       226 of       298\t|\tloss: 0.197644\n",
      "Training Epoch 10  76.2% | batch:       227 of       298\t|\tloss: 1.06373\n",
      "Training Epoch 10  76.5% | batch:       228 of       298\t|\tloss: 0.142216\n",
      "Training Epoch 10  76.8% | batch:       229 of       298\t|\tloss: 0.268287\n",
      "Training Epoch 10  77.2% | batch:       230 of       298\t|\tloss: 0.240734\n",
      "Training Epoch 10  77.5% | batch:       231 of       298\t|\tloss: 0.270219\n",
      "Training Epoch 10  77.9% | batch:       232 of       298\t|\tloss: 0.223687\n",
      "Training Epoch 10  78.2% | batch:       233 of       298\t|\tloss: 0.253427\n",
      "Training Epoch 10  78.5% | batch:       234 of       298\t|\tloss: 0.210582\n",
      "Training Epoch 10  78.9% | batch:       235 of       298\t|\tloss: 0.919829\n",
      "Training Epoch 10  79.2% | batch:       236 of       298\t|\tloss: 0.296772\n",
      "Training Epoch 10  79.5% | batch:       237 of       298\t|\tloss: 0.290275\n",
      "Training Epoch 10  79.9% | batch:       238 of       298\t|\tloss: 0.152011\n",
      "Training Epoch 10  80.2% | batch:       239 of       298\t|\tloss: 0.273625\n",
      "Training Epoch 10  80.5% | batch:       240 of       298\t|\tloss: 0.23366\n",
      "Training Epoch 10  80.9% | batch:       241 of       298\t|\tloss: 0.225309\n",
      "Training Epoch 10  81.2% | batch:       242 of       298\t|\tloss: 0.515869\n",
      "Training Epoch 10  81.5% | batch:       243 of       298\t|\tloss: 0.209468\n",
      "Training Epoch 10  81.9% | batch:       244 of       298\t|\tloss: 0.260478\n",
      "Training Epoch 10  82.2% | batch:       245 of       298\t|\tloss: 0.18977\n",
      "Training Epoch 10  82.6% | batch:       246 of       298\t|\tloss: 0.360083\n",
      "Training Epoch 10  82.9% | batch:       247 of       298\t|\tloss: 0.162609\n",
      "Training Epoch 10  83.2% | batch:       248 of       298\t|\tloss: 0.201574\n",
      "Training Epoch 10  83.6% | batch:       249 of       298\t|\tloss: 0.225602\n",
      "Training Epoch 10  83.9% | batch:       250 of       298\t|\tloss: 0.282588\n",
      "Training Epoch 10  84.2% | batch:       251 of       298\t|\tloss: 0.21447\n",
      "Training Epoch 10  84.6% | batch:       252 of       298\t|\tloss: 0.254499\n",
      "Training Epoch 10  84.9% | batch:       253 of       298\t|\tloss: 0.167824\n",
      "Training Epoch 10  85.2% | batch:       254 of       298\t|\tloss: 0.247125\n",
      "Training Epoch 10  85.6% | batch:       255 of       298\t|\tloss: 0.171063\n",
      "Training Epoch 10  85.9% | batch:       256 of       298\t|\tloss: 0.215158\n",
      "Training Epoch 10  86.2% | batch:       257 of       298\t|\tloss: 0.219867\n",
      "Training Epoch 10  86.6% | batch:       258 of       298\t|\tloss: 0.169109\n",
      "Training Epoch 10  86.9% | batch:       259 of       298\t|\tloss: 0.173068\n",
      "Training Epoch 10  87.2% | batch:       260 of       298\t|\tloss: 0.20771\n",
      "Training Epoch 10  87.6% | batch:       261 of       298\t|\tloss: 0.200671\n",
      "Training Epoch 10  87.9% | batch:       262 of       298\t|\tloss: 0.164916\n",
      "Training Epoch 10  88.3% | batch:       263 of       298\t|\tloss: 0.321628\n",
      "Training Epoch 10  88.6% | batch:       264 of       298\t|\tloss: 0.169992\n",
      "Training Epoch 10  88.9% | batch:       265 of       298\t|\tloss: 0.210406\n",
      "Training Epoch 10  89.3% | batch:       266 of       298\t|\tloss: 0.279644\n",
      "Training Epoch 10  89.6% | batch:       267 of       298\t|\tloss: 0.183226\n",
      "Training Epoch 10  89.9% | batch:       268 of       298\t|\tloss: 2.1704\n",
      "Training Epoch 10  90.3% | batch:       269 of       298\t|\tloss: 0.211954\n",
      "Training Epoch 10  90.6% | batch:       270 of       298\t|\tloss: 0.223161\n",
      "Training Epoch 10  90.9% | batch:       271 of       298\t|\tloss: 0.193488\n",
      "Training Epoch 10  91.3% | batch:       272 of       298\t|\tloss: 0.183591\n",
      "Training Epoch 10  91.6% | batch:       273 of       298\t|\tloss: 0.177098\n",
      "Training Epoch 10  91.9% | batch:       274 of       298\t|\tloss: 0.187038\n",
      "Training Epoch 10  92.3% | batch:       275 of       298\t|\tloss: 0.252872\n",
      "Training Epoch 10  92.6% | batch:       276 of       298\t|\tloss: 0.207238\n",
      "Training Epoch 10  93.0% | batch:       277 of       298\t|\tloss: 0.127828\n",
      "Training Epoch 10  93.3% | batch:       278 of       298\t|\tloss: 0.328772\n",
      "Training Epoch 10  93.6% | batch:       279 of       298\t|\tloss: 0.501604\n",
      "Training Epoch 10  94.0% | batch:       280 of       298\t|\tloss: 0.184484\n",
      "Training Epoch 10  94.3% | batch:       281 of       298\t|\tloss: 0.121615\n",
      "Training Epoch 10  94.6% | batch:       282 of       298\t|\tloss: 0.15873\n",
      "Training Epoch 10  95.0% | batch:       283 of       298\t|\tloss: 0.252906\n",
      "Training Epoch 10  95.3% | batch:       284 of       298\t|\tloss: 0.178808\n",
      "Training Epoch 10  95.6% | batch:       285 of       298\t|\tloss: 0.398889\n",
      "Training Epoch 10  96.0% | batch:       286 of       298\t|\tloss: 0.179432\n",
      "Training Epoch 10  96.3% | batch:       287 of       298\t|\tloss: 0.179559\n",
      "Training Epoch 10  96.6% | batch:       288 of       298\t|\tloss: 0.281451\n",
      "Training Epoch 10  97.0% | batch:       289 of       298\t|\tloss: 0.17357\n",
      "Training Epoch 10  97.3% | batch:       290 of       298\t|\tloss: 0.202217\n",
      "Training Epoch 10  97.7% | batch:       291 of       298\t|\tloss: 0.187194\n",
      "Training Epoch 10  98.0% | batch:       292 of       298\t|\tloss: 0.258358\n",
      "Training Epoch 10  98.3% | batch:       293 of       298\t|\tloss: 0.813202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:38:10,869 | INFO : Epoch 10 Training Summary: epoch: 10.000000 | loss: 0.329171 | \n",
      "2023-06-22 14:38:10,871 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.242393255233765 seconds\n",
      "\n",
      "2023-06-22 14:38:10,872 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.279330372810364 seconds\n",
      "2023-06-22 14:38:10,873 | INFO : Avg batch train. time: 0.041205806620169004 seconds\n",
      "2023-06-22 14:38:10,873 | INFO : Avg sample train. time: 0.001287951580953468 seconds\n",
      "2023-06-22 14:38:10,874 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 10  98.7% | batch:       294 of       298\t|\tloss: 0.239367\n",
      "Training Epoch 10  99.0% | batch:       295 of       298\t|\tloss: 0.190274\n",
      "Training Epoch 10  99.3% | batch:       296 of       298\t|\tloss: 0.167482\n",
      "Training Epoch 10  99.7% | batch:       297 of       298\t|\tloss: 0.235509\n",
      "\n",
      "Evaluating Epoch 10   0.0% | batch:         0 of        75\t|\tloss: 0.223632\n",
      "Evaluating Epoch 10   1.3% | batch:         1 of        75\t|\tloss: 0.244392\n",
      "Evaluating Epoch 10   2.7% | batch:         2 of        75\t|\tloss: 0.226972\n",
      "Evaluating Epoch 10   4.0% | batch:         3 of        75\t|\tloss: 0.199473\n",
      "Evaluating Epoch 10   5.3% | batch:         4 of        75\t|\tloss: 0.251148\n",
      "Evaluating Epoch 10   6.7% | batch:         5 of        75\t|\tloss: 0.179265\n",
      "Evaluating Epoch 10   8.0% | batch:         6 of        75\t|\tloss: 0.380925\n",
      "Evaluating Epoch 10   9.3% | batch:         7 of        75\t|\tloss: 0.168283\n",
      "Evaluating Epoch 10  10.7% | batch:         8 of        75\t|\tloss: 0.167238\n",
      "Evaluating Epoch 10  12.0% | batch:         9 of        75\t|\tloss: 0.271245\n",
      "Evaluating Epoch 10  13.3% | batch:        10 of        75\t|\tloss: 0.145247\n",
      "Evaluating Epoch 10  14.7% | batch:        11 of        75\t|\tloss: 0.195424\n",
      "Evaluating Epoch 10  16.0% | batch:        12 of        75\t|\tloss: 0.470992\n",
      "Evaluating Epoch 10  17.3% | batch:        13 of        75\t|\tloss: 0.238981\n",
      "Evaluating Epoch 10  18.7% | batch:        14 of        75\t|\tloss: 0.139429\n",
      "Evaluating Epoch 10  20.0% | batch:        15 of        75\t|\tloss: 0.229441\n",
      "Evaluating Epoch 10  21.3% | batch:        16 of        75\t|\tloss: 0.217249\n",
      "Evaluating Epoch 10  22.7% | batch:        17 of        75\t|\tloss: 1.41229\n",
      "Evaluating Epoch 10  24.0% | batch:        18 of        75\t|\tloss: 0.200168\n",
      "Evaluating Epoch 10  25.3% | batch:        19 of        75\t|\tloss: 0.187328\n",
      "Evaluating Epoch 10  26.7% | batch:        20 of        75\t|\tloss: 0.172922\n",
      "Evaluating Epoch 10  28.0% | batch:        21 of        75\t|\tloss: 0.246663\n",
      "Evaluating Epoch 10  29.3% | batch:        22 of        75\t|\tloss: 0.200911\n",
      "Evaluating Epoch 10  30.7% | batch:        23 of        75\t|\tloss: 0.178958\n",
      "Evaluating Epoch 10  32.0% | batch:        24 of        75\t|\tloss: 0.170093\n",
      "Evaluating Epoch 10  33.3% | batch:        25 of        75\t|\tloss: 0.193978\n",
      "Evaluating Epoch 10  34.7% | batch:        26 of        75\t|\tloss: 0.145042\n",
      "Evaluating Epoch 10  36.0% | batch:        27 of        75\t|\tloss: 0.2037\n",
      "Evaluating Epoch 10  37.3% | batch:        28 of        75\t|\tloss: 0.361852\n",
      "Evaluating Epoch 10  38.7% | batch:        29 of        75\t|\tloss: 0.189515\n",
      "Evaluating Epoch 10  40.0% | batch:        30 of        75\t|\tloss: 0.512577\n",
      "Evaluating Epoch 10  41.3% | batch:        31 of        75\t|\tloss: 0.169564\n",
      "Evaluating Epoch 10  42.7% | batch:        32 of        75\t|\tloss: 0.180678\n",
      "Evaluating Epoch 10  44.0% | batch:        33 of        75\t|\tloss: 0.687891\n",
      "Evaluating Epoch 10  45.3% | batch:        34 of        75\t|\tloss: 0.233509\n",
      "Evaluating Epoch 10  46.7% | batch:        35 of        75\t|\tloss: 0.150794\n",
      "Evaluating Epoch 10  48.0% | batch:        36 of        75\t|\tloss: 0.173968\n",
      "Evaluating Epoch 10  49.3% | batch:        37 of        75\t|\tloss: 0.160959\n",
      "Evaluating Epoch 10  50.7% | batch:        38 of        75\t|\tloss: 0.208608\n",
      "Evaluating Epoch 10  52.0% | batch:        39 of        75\t|\tloss: 0.180247\n",
      "Evaluating Epoch 10  53.3% | batch:        40 of        75\t|\tloss: 0.322845\n",
      "Evaluating Epoch 10  54.7% | batch:        41 of        75\t|\tloss: 0.16207\n",
      "Evaluating Epoch 10  56.0% | batch:        42 of        75\t|\tloss: 0.151004\n",
      "Evaluating Epoch 10  57.3% | batch:        43 of        75\t|\tloss: 0.160566\n",
      "Evaluating Epoch 10  58.7% | batch:        44 of        75\t|\tloss: 0.1359\n",
      "Evaluating Epoch 10  60.0% | batch:        45 of        75\t|\tloss: 0.208285\n",
      "Evaluating Epoch 10  61.3% | batch:        46 of        75\t|\tloss: 0.240316\n",
      "Evaluating Epoch 10  62.7% | batch:        47 of        75\t|\tloss: 0.820363\n",
      "Evaluating Epoch 10  64.0% | batch:        48 of        75\t|\tloss: 0.234597\n",
      "Evaluating Epoch 10  65.3% | batch:        49 of        75\t|\tloss: 0.173187\n",
      "Evaluating Epoch 10  66.7% | batch:        50 of        75\t|\tloss: 0.240834\n",
      "Evaluating Epoch 10  68.0% | batch:        51 of        75\t|\tloss: 0.222126\n",
      "Evaluating Epoch 10  69.3% | batch:        52 of        75\t|\tloss: 0.899348\n",
      "Evaluating Epoch 10  70.7% | batch:        53 of        75\t|\tloss: 0.13694\n",
      "Evaluating Epoch 10  72.0% | batch:        54 of        75\t|\tloss: 0.324675\n",
      "Evaluating Epoch 10  73.3% | batch:        55 of        75\t|\tloss: 0.89332\n",
      "Evaluating Epoch 10  74.7% | batch:        56 of        75\t|\tloss: 0.174796\n",
      "Evaluating Epoch 10  76.0% | batch:        57 of        75\t|\tloss: 0.214743\n",
      "Evaluating Epoch 10  77.3% | batch:        58 of        75\t|\tloss: 0.228464\n",
      "Evaluating Epoch 10  78.7% | batch:        59 of        75\t|\tloss: 0.252588\n",
      "Evaluating Epoch 10  80.0% | batch:        60 of        75\t|\tloss: 0.155729\n",
      "Evaluating Epoch 10  81.3% | batch:        61 of        75\t|\tloss: 0.157352\n",
      "Evaluating Epoch 10  82.7% | batch:        62 of        75\t|\tloss: 0.464294\n",
      "Evaluating Epoch 10  84.0% | batch:        63 of        75\t|\tloss: 1.95103\n",
      "Evaluating Epoch 10  85.3% | batch:        64 of        75\t|\tloss: 0.314705\n",
      "Evaluating Epoch 10  86.7% | batch:        65 of        75\t|\tloss: 0.21955\n",
      "Evaluating Epoch 10  88.0% | batch:        66 of        75\t|\tloss: 0.171766\n",
      "Evaluating Epoch 10  89.3% | batch:        67 of        75\t|\tloss: 0.207565\n",
      "Evaluating Epoch 10  90.7% | batch:        68 of        75\t|\tloss: 0.232868\n",
      "Evaluating Epoch 10  92.0% | batch:        69 of        75\t|\tloss: 0.152142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:38:12,061 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.1866626739501953 seconds\n",
      "\n",
      "2023-06-22 14:38:12,062 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.225738797869001 seconds\n",
      "2023-06-22 14:38:12,063 | INFO : Avg batch val. time: 0.01634318397158668 seconds\n",
      "2023-06-22 14:38:12,064 | INFO : Avg sample val. time: 0.0005141521803141783 seconds\n",
      "2023-06-22 14:38:12,064 | INFO : Epoch 10 Validation Summary: epoch: 10.000000 | loss: 0.288224 | \n",
      "Training Epoch:   2%|▎         | 10/400 [02:10<1:24:44, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 10  93.3% | batch:        70 of        75\t|\tloss: 0.126027\n",
      "Evaluating Epoch 10  94.7% | batch:        71 of        75\t|\tloss: 0.183043\n",
      "Evaluating Epoch 10  96.0% | batch:        72 of        75\t|\tloss: 0.122077\n",
      "Evaluating Epoch 10  97.3% | batch:        73 of        75\t|\tloss: 0.200899\n",
      "Evaluating Epoch 10  98.7% | batch:        74 of        75\t|\tloss: 0.204311\n",
      "\n",
      "Training Epoch 11   0.0% | batch:         0 of       298\t|\tloss: 0.190821\n",
      "Training Epoch 11   0.3% | batch:         1 of       298\t|\tloss: 0.390651\n",
      "Training Epoch 11   0.7% | batch:         2 of       298\t|\tloss: 0.197235\n",
      "Training Epoch 11   1.0% | batch:         3 of       298\t|\tloss: 0.185638\n",
      "Training Epoch 11   1.3% | batch:         4 of       298\t|\tloss: 0.205213\n",
      "Training Epoch 11   1.7% | batch:         5 of       298\t|\tloss: 0.210049\n",
      "Training Epoch 11   2.0% | batch:         6 of       298\t|\tloss: 0.208708\n",
      "Training Epoch 11   2.3% | batch:         7 of       298\t|\tloss: 0.213721\n",
      "Training Epoch 11   2.7% | batch:         8 of       298\t|\tloss: 0.453071\n",
      "Training Epoch 11   3.0% | batch:         9 of       298\t|\tloss: 1.00998\n",
      "Training Epoch 11   3.4% | batch:        10 of       298\t|\tloss: 0.275063\n",
      "Training Epoch 11   3.7% | batch:        11 of       298\t|\tloss: 0.180228\n",
      "Training Epoch 11   4.0% | batch:        12 of       298\t|\tloss: 0.16559\n",
      "Training Epoch 11   4.4% | batch:        13 of       298\t|\tloss: 0.227018\n",
      "Training Epoch 11   4.7% | batch:        14 of       298\t|\tloss: 0.193342\n",
      "Training Epoch 11   5.0% | batch:        15 of       298\t|\tloss: 0.190548\n",
      "Training Epoch 11   5.4% | batch:        16 of       298\t|\tloss: 0.242525\n",
      "Training Epoch 11   5.7% | batch:        17 of       298\t|\tloss: 0.204588\n",
      "Training Epoch 11   6.0% | batch:        18 of       298\t|\tloss: 0.2157\n",
      "Training Epoch 11   6.4% | batch:        19 of       298\t|\tloss: 0.245475\n",
      "Training Epoch 11   6.7% | batch:        20 of       298\t|\tloss: 0.224246\n",
      "Training Epoch 11   7.0% | batch:        21 of       298\t|\tloss: 0.17484\n",
      "Training Epoch 11   7.4% | batch:        22 of       298\t|\tloss: 0.230112\n",
      "Training Epoch 11   7.7% | batch:        23 of       298\t|\tloss: 0.197211\n",
      "Training Epoch 11   8.1% | batch:        24 of       298\t|\tloss: 0.178167\n",
      "Training Epoch 11   8.4% | batch:        25 of       298\t|\tloss: 0.180921\n",
      "Training Epoch 11   8.7% | batch:        26 of       298\t|\tloss: 0.1921\n",
      "Training Epoch 11   9.1% | batch:        27 of       298\t|\tloss: 0.2171\n",
      "Training Epoch 11   9.4% | batch:        28 of       298\t|\tloss: 4.15433\n",
      "Training Epoch 11   9.7% | batch:        29 of       298\t|\tloss: 0.2066\n",
      "Training Epoch 11  10.1% | batch:        30 of       298\t|\tloss: 0.190702\n",
      "Training Epoch 11  10.4% | batch:        31 of       298\t|\tloss: 0.217123\n",
      "Training Epoch 11  10.7% | batch:        32 of       298\t|\tloss: 0.166784\n",
      "Training Epoch 11  11.1% | batch:        33 of       298\t|\tloss: 0.195747\n",
      "Training Epoch 11  11.4% | batch:        34 of       298\t|\tloss: 0.232788\n",
      "Training Epoch 11  11.7% | batch:        35 of       298\t|\tloss: 0.20543\n",
      "Training Epoch 11  12.1% | batch:        36 of       298\t|\tloss: 0.392878\n",
      "Training Epoch 11  12.4% | batch:        37 of       298\t|\tloss: 0.827995\n",
      "Training Epoch 11  12.8% | batch:        38 of       298\t|\tloss: 0.170632\n",
      "Training Epoch 11  13.1% | batch:        39 of       298\t|\tloss: 0.271998\n",
      "Training Epoch 11  13.4% | batch:        40 of       298\t|\tloss: 0.24701\n",
      "Training Epoch 11  13.8% | batch:        41 of       298\t|\tloss: 0.167655\n",
      "Training Epoch 11  14.1% | batch:        42 of       298\t|\tloss: 0.292564\n",
      "Training Epoch 11  14.4% | batch:        43 of       298\t|\tloss: 0.250459\n",
      "Training Epoch 11  14.8% | batch:        44 of       298\t|\tloss: 0.202597\n",
      "Training Epoch 11  15.1% | batch:        45 of       298\t|\tloss: 0.185244\n",
      "Training Epoch 11  15.4% | batch:        46 of       298\t|\tloss: 0.364535\n",
      "Training Epoch 11  15.8% | batch:        47 of       298\t|\tloss: 0.147068\n",
      "Training Epoch 11  16.1% | batch:        48 of       298\t|\tloss: 0.175501\n",
      "Training Epoch 11  16.4% | batch:        49 of       298\t|\tloss: 0.157888\n",
      "Training Epoch 11  16.8% | batch:        50 of       298\t|\tloss: 0.4921\n",
      "Training Epoch 11  17.1% | batch:        51 of       298\t|\tloss: 0.223589\n",
      "Training Epoch 11  17.4% | batch:        52 of       298\t|\tloss: 0.180342\n",
      "Training Epoch 11  17.8% | batch:        53 of       298\t|\tloss: 0.20611\n",
      "Training Epoch 11  18.1% | batch:        54 of       298\t|\tloss: 0.181261\n",
      "Training Epoch 11  18.5% | batch:        55 of       298\t|\tloss: 0.639101\n",
      "Training Epoch 11  18.8% | batch:        56 of       298\t|\tloss: 0.153739\n",
      "Training Epoch 11  19.1% | batch:        57 of       298\t|\tloss: 0.170891\n",
      "Training Epoch 11  19.5% | batch:        58 of       298\t|\tloss: 0.173733\n",
      "Training Epoch 11  19.8% | batch:        59 of       298\t|\tloss: 0.162479\n",
      "Training Epoch 11  20.1% | batch:        60 of       298\t|\tloss: 0.170257\n",
      "Training Epoch 11  20.5% | batch:        61 of       298\t|\tloss: 0.156114\n",
      "Training Epoch 11  20.8% | batch:        62 of       298\t|\tloss: 0.20263\n",
      "Training Epoch 11  21.1% | batch:        63 of       298\t|\tloss: 0.245502\n",
      "Training Epoch 11  21.5% | batch:        64 of       298\t|\tloss: 0.16383\n",
      "Training Epoch 11  21.8% | batch:        65 of       298\t|\tloss: 0.192024\n",
      "Training Epoch 11  22.1% | batch:        66 of       298\t|\tloss: 0.186212\n",
      "Training Epoch 11  22.5% | batch:        67 of       298\t|\tloss: 3.69405\n",
      "Training Epoch 11  22.8% | batch:        68 of       298\t|\tloss: 0.242074\n",
      "Training Epoch 11  23.2% | batch:        69 of       298\t|\tloss: 0.195666\n",
      "Training Epoch 11  23.5% | batch:        70 of       298\t|\tloss: 0.204029\n",
      "Training Epoch 11  23.8% | batch:        71 of       298\t|\tloss: 0.225208\n",
      "Training Epoch 11  24.2% | batch:        72 of       298\t|\tloss: 0.186857\n",
      "Training Epoch 11  24.5% | batch:        73 of       298\t|\tloss: 0.201377\n",
      "Training Epoch 11  24.8% | batch:        74 of       298\t|\tloss: 0.215277\n",
      "Training Epoch 11  25.2% | batch:        75 of       298\t|\tloss: 0.202479\n",
      "Training Epoch 11  25.5% | batch:        76 of       298\t|\tloss: 0.382799\n",
      "Training Epoch 11  25.8% | batch:        77 of       298\t|\tloss: 0.185036\n",
      "Training Epoch 11  26.2% | batch:        78 of       298\t|\tloss: 0.25738\n",
      "Training Epoch 11  26.5% | batch:        79 of       298\t|\tloss: 0.255687\n",
      "Training Epoch 11  26.8% | batch:        80 of       298\t|\tloss: 0.256207\n",
      "Training Epoch 11  27.2% | batch:        81 of       298\t|\tloss: 0.172764\n",
      "Training Epoch 11  27.5% | batch:        82 of       298\t|\tloss: 0.189209\n",
      "Training Epoch 11  27.9% | batch:        83 of       298\t|\tloss: 0.154982\n",
      "Training Epoch 11  28.2% | batch:        84 of       298\t|\tloss: 0.290869\n",
      "Training Epoch 11  28.5% | batch:        85 of       298\t|\tloss: 0.19644\n",
      "Training Epoch 11  28.9% | batch:        86 of       298\t|\tloss: 6.69388\n",
      "Training Epoch 11  29.2% | batch:        87 of       298\t|\tloss: 0.194335\n",
      "Training Epoch 11  29.5% | batch:        88 of       298\t|\tloss: 0.686769\n",
      "Training Epoch 11  29.9% | batch:        89 of       298\t|\tloss: 0.229794\n",
      "Training Epoch 11  30.2% | batch:        90 of       298\t|\tloss: 0.427042\n",
      "Training Epoch 11  30.5% | batch:        91 of       298\t|\tloss: 0.249312\n",
      "Training Epoch 11  30.9% | batch:        92 of       298\t|\tloss: 0.196361\n",
      "Training Epoch 11  31.2% | batch:        93 of       298\t|\tloss: 0.312256\n",
      "Training Epoch 11  31.5% | batch:        94 of       298\t|\tloss: 0.238665\n",
      "Training Epoch 11  31.9% | batch:        95 of       298\t|\tloss: 0.228766\n",
      "Training Epoch 11  32.2% | batch:        96 of       298\t|\tloss: 0.405112\n",
      "Training Epoch 11  32.6% | batch:        97 of       298\t|\tloss: 0.193188\n",
      "Training Epoch 11  32.9% | batch:        98 of       298\t|\tloss: 0.650221\n",
      "Training Epoch 11  33.2% | batch:        99 of       298\t|\tloss: 0.283268\n",
      "Training Epoch 11  33.6% | batch:       100 of       298\t|\tloss: 0.372095\n",
      "Training Epoch 11  33.9% | batch:       101 of       298\t|\tloss: 0.239283\n",
      "Training Epoch 11  34.2% | batch:       102 of       298\t|\tloss: 0.191251\n",
      "Training Epoch 11  34.6% | batch:       103 of       298\t|\tloss: 0.158989\n",
      "Training Epoch 11  34.9% | batch:       104 of       298\t|\tloss: 0.170248\n",
      "Training Epoch 11  35.2% | batch:       105 of       298\t|\tloss: 0.190456\n",
      "Training Epoch 11  35.6% | batch:       106 of       298\t|\tloss: 0.166496\n",
      "Training Epoch 11  35.9% | batch:       107 of       298\t|\tloss: 0.248209\n",
      "Training Epoch 11  36.2% | batch:       108 of       298\t|\tloss: 0.194102\n",
      "Training Epoch 11  36.6% | batch:       109 of       298\t|\tloss: 0.476728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  36.9% | batch:       110 of       298\t|\tloss: 0.227433\n",
      "Training Epoch 11  37.2% | batch:       111 of       298\t|\tloss: 0.153943\n",
      "Training Epoch 11  37.6% | batch:       112 of       298\t|\tloss: 0.223449\n",
      "Training Epoch 11  37.9% | batch:       113 of       298\t|\tloss: 0.195653\n",
      "Training Epoch 11  38.3% | batch:       114 of       298\t|\tloss: 0.205305\n",
      "Training Epoch 11  38.6% | batch:       115 of       298\t|\tloss: 0.136172\n",
      "Training Epoch 11  38.9% | batch:       116 of       298\t|\tloss: 0.290341\n",
      "Training Epoch 11  39.3% | batch:       117 of       298\t|\tloss: 0.313948\n",
      "Training Epoch 11  39.6% | batch:       118 of       298\t|\tloss: 0.193773\n",
      "Training Epoch 11  39.9% | batch:       119 of       298\t|\tloss: 0.207364\n",
      "Training Epoch 11  40.3% | batch:       120 of       298\t|\tloss: 0.187085\n",
      "Training Epoch 11  40.6% | batch:       121 of       298\t|\tloss: 0.22314\n",
      "Training Epoch 11  40.9% | batch:       122 of       298\t|\tloss: 0.166601\n",
      "Training Epoch 11  41.3% | batch:       123 of       298\t|\tloss: 0.182622\n",
      "Training Epoch 11  41.6% | batch:       124 of       298\t|\tloss: 0.56206\n",
      "Training Epoch 11  41.9% | batch:       125 of       298\t|\tloss: 0.142093\n",
      "Training Epoch 11  42.3% | batch:       126 of       298\t|\tloss: 0.149989\n",
      "Training Epoch 11  42.6% | batch:       127 of       298\t|\tloss: 0.388491\n",
      "Training Epoch 11  43.0% | batch:       128 of       298\t|\tloss: 0.162119\n",
      "Training Epoch 11  43.3% | batch:       129 of       298\t|\tloss: 2.06548\n",
      "Training Epoch 11  43.6% | batch:       130 of       298\t|\tloss: 0.395825\n",
      "Training Epoch 11  44.0% | batch:       131 of       298\t|\tloss: 0.206177\n",
      "Training Epoch 11  44.3% | batch:       132 of       298\t|\tloss: 0.263509\n",
      "Training Epoch 11  44.6% | batch:       133 of       298\t|\tloss: 0.403707\n",
      "Training Epoch 11  45.0% | batch:       134 of       298\t|\tloss: 0.17833\n",
      "Training Epoch 11  45.3% | batch:       135 of       298\t|\tloss: 0.304751\n",
      "Training Epoch 11  45.6% | batch:       136 of       298\t|\tloss: 0.215597\n",
      "Training Epoch 11  46.0% | batch:       137 of       298\t|\tloss: 0.266061\n",
      "Training Epoch 11  46.3% | batch:       138 of       298\t|\tloss: 2.02864\n",
      "Training Epoch 11  46.6% | batch:       139 of       298\t|\tloss: 0.30038\n",
      "Training Epoch 11  47.0% | batch:       140 of       298\t|\tloss: 0.222711\n",
      "Training Epoch 11  47.3% | batch:       141 of       298\t|\tloss: 0.1931\n",
      "Training Epoch 11  47.7% | batch:       142 of       298\t|\tloss: 0.142487\n",
      "Training Epoch 11  48.0% | batch:       143 of       298\t|\tloss: 0.19501\n",
      "Training Epoch 11  48.3% | batch:       144 of       298\t|\tloss: 0.497918\n",
      "Training Epoch 11  48.7% | batch:       145 of       298\t|\tloss: 0.220749\n",
      "Training Epoch 11  49.0% | batch:       146 of       298\t|\tloss: 0.165468\n",
      "Training Epoch 11  49.3% | batch:       147 of       298\t|\tloss: 0.208088\n",
      "Training Epoch 11  49.7% | batch:       148 of       298\t|\tloss: 0.180779\n",
      "Training Epoch 11  50.0% | batch:       149 of       298\t|\tloss: 0.182161\n",
      "Training Epoch 11  50.3% | batch:       150 of       298\t|\tloss: 0.246359\n",
      "Training Epoch 11  50.7% | batch:       151 of       298\t|\tloss: 1.19653\n",
      "Training Epoch 11  51.0% | batch:       152 of       298\t|\tloss: 0.294336\n",
      "Training Epoch 11  51.3% | batch:       153 of       298\t|\tloss: 0.342781\n",
      "Training Epoch 11  51.7% | batch:       154 of       298\t|\tloss: 0.184304\n",
      "Training Epoch 11  52.0% | batch:       155 of       298\t|\tloss: 0.231148\n",
      "Training Epoch 11  52.3% | batch:       156 of       298\t|\tloss: 0.200895\n",
      "Training Epoch 11  52.7% | batch:       157 of       298\t|\tloss: 0.33337\n",
      "Training Epoch 11  53.0% | batch:       158 of       298\t|\tloss: 0.230085\n",
      "Training Epoch 11  53.4% | batch:       159 of       298\t|\tloss: 0.255475\n",
      "Training Epoch 11  53.7% | batch:       160 of       298\t|\tloss: 0.23386\n",
      "Training Epoch 11  54.0% | batch:       161 of       298\t|\tloss: 0.244191\n",
      "Training Epoch 11  54.4% | batch:       162 of       298\t|\tloss: 0.187647\n",
      "Training Epoch 11  54.7% | batch:       163 of       298\t|\tloss: 0.20298\n",
      "Training Epoch 11  55.0% | batch:       164 of       298\t|\tloss: 0.238547\n",
      "Training Epoch 11  55.4% | batch:       165 of       298\t|\tloss: 0.197697\n",
      "Training Epoch 11  55.7% | batch:       166 of       298\t|\tloss: 0.209152\n",
      "Training Epoch 11  56.0% | batch:       167 of       298\t|\tloss: 0.158617\n",
      "Training Epoch 11  56.4% | batch:       168 of       298\t|\tloss: 0.385468\n",
      "Training Epoch 11  56.7% | batch:       169 of       298\t|\tloss: 0.234108\n",
      "Training Epoch 11  57.0% | batch:       170 of       298\t|\tloss: 0.208007\n",
      "Training Epoch 11  57.4% | batch:       171 of       298\t|\tloss: 0.242351\n",
      "Training Epoch 11  57.7% | batch:       172 of       298\t|\tloss: 0.181426\n",
      "Training Epoch 11  58.1% | batch:       173 of       298\t|\tloss: 0.40635\n",
      "Training Epoch 11  58.4% | batch:       174 of       298\t|\tloss: 0.17474\n",
      "Training Epoch 11  58.7% | batch:       175 of       298\t|\tloss: 0.179384\n",
      "Training Epoch 11  59.1% | batch:       176 of       298\t|\tloss: 0.196036\n",
      "Training Epoch 11  59.4% | batch:       177 of       298\t|\tloss: 0.168818\n",
      "Training Epoch 11  59.7% | batch:       178 of       298\t|\tloss: 0.172984\n",
      "Training Epoch 11  60.1% | batch:       179 of       298\t|\tloss: 0.303605\n",
      "Training Epoch 11  60.4% | batch:       180 of       298\t|\tloss: 0.37349\n",
      "Training Epoch 11  60.7% | batch:       181 of       298\t|\tloss: 0.213965\n",
      "Training Epoch 11  61.1% | batch:       182 of       298\t|\tloss: 0.191124\n",
      "Training Epoch 11  61.4% | batch:       183 of       298\t|\tloss: 0.228285\n",
      "Training Epoch 11  61.7% | batch:       184 of       298\t|\tloss: 0.214726\n",
      "Training Epoch 11  62.1% | batch:       185 of       298\t|\tloss: 0.228124\n",
      "Training Epoch 11  62.4% | batch:       186 of       298\t|\tloss: 0.162669\n",
      "Training Epoch 11  62.8% | batch:       187 of       298\t|\tloss: 0.209911\n",
      "Training Epoch 11  63.1% | batch:       188 of       298\t|\tloss: 0.216624\n",
      "Training Epoch 11  63.4% | batch:       189 of       298\t|\tloss: 0.499\n",
      "Training Epoch 11  63.8% | batch:       190 of       298\t|\tloss: 0.524899\n",
      "Training Epoch 11  64.1% | batch:       191 of       298\t|\tloss: 0.80881\n",
      "Training Epoch 11  64.4% | batch:       192 of       298\t|\tloss: 0.343773\n",
      "Training Epoch 11  64.8% | batch:       193 of       298\t|\tloss: 0.1684\n",
      "Training Epoch 11  65.1% | batch:       194 of       298\t|\tloss: 0.344208\n",
      "Training Epoch 11  65.4% | batch:       195 of       298\t|\tloss: 0.203899\n",
      "Training Epoch 11  65.8% | batch:       196 of       298\t|\tloss: 0.199998\n",
      "Training Epoch 11  66.1% | batch:       197 of       298\t|\tloss: 0.182734\n",
      "Training Epoch 11  66.4% | batch:       198 of       298\t|\tloss: 0.57323\n",
      "Training Epoch 11  66.8% | batch:       199 of       298\t|\tloss: 0.295646\n",
      "Training Epoch 11  67.1% | batch:       200 of       298\t|\tloss: 0.203845\n",
      "Training Epoch 11  67.4% | batch:       201 of       298\t|\tloss: 0.290162\n",
      "Training Epoch 11  67.8% | batch:       202 of       298\t|\tloss: 0.221281\n",
      "Training Epoch 11  68.1% | batch:       203 of       298\t|\tloss: 0.454209\n",
      "Training Epoch 11  68.5% | batch:       204 of       298\t|\tloss: 0.312885\n",
      "Training Epoch 11  68.8% | batch:       205 of       298\t|\tloss: 0.162827\n",
      "Training Epoch 11  69.1% | batch:       206 of       298\t|\tloss: 0.35199\n",
      "Training Epoch 11  69.5% | batch:       207 of       298\t|\tloss: 0.209318\n",
      "Training Epoch 11  69.8% | batch:       208 of       298\t|\tloss: 0.195125\n",
      "Training Epoch 11  70.1% | batch:       209 of       298\t|\tloss: 0.200084\n",
      "Training Epoch 11  70.5% | batch:       210 of       298\t|\tloss: 0.208241\n",
      "Training Epoch 11  70.8% | batch:       211 of       298\t|\tloss: 0.185168\n",
      "Training Epoch 11  71.1% | batch:       212 of       298\t|\tloss: 0.212654\n",
      "Training Epoch 11  71.5% | batch:       213 of       298\t|\tloss: 0.148165\n",
      "Training Epoch 11  71.8% | batch:       214 of       298\t|\tloss: 0.270016\n",
      "Training Epoch 11  72.1% | batch:       215 of       298\t|\tloss: 0.190681\n",
      "Training Epoch 11  72.5% | batch:       216 of       298\t|\tloss: 0.194967\n",
      "Training Epoch 11  72.8% | batch:       217 of       298\t|\tloss: 0.315482\n",
      "Training Epoch 11  73.2% | batch:       218 of       298\t|\tloss: 2.23478\n",
      "Training Epoch 11  73.5% | batch:       219 of       298\t|\tloss: 0.240124\n",
      "Training Epoch 11  73.8% | batch:       220 of       298\t|\tloss: 0.198787\n",
      "Training Epoch 11  74.2% | batch:       221 of       298\t|\tloss: 0.309201\n",
      "Training Epoch 11  74.5% | batch:       222 of       298\t|\tloss: 0.227581\n",
      "Training Epoch 11  74.8% | batch:       223 of       298\t|\tloss: 0.637083\n",
      "Training Epoch 11  75.2% | batch:       224 of       298\t|\tloss: 0.198039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  75.5% | batch:       225 of       298\t|\tloss: 0.283784\n",
      "Training Epoch 11  75.8% | batch:       226 of       298\t|\tloss: 0.432467\n",
      "Training Epoch 11  76.2% | batch:       227 of       298\t|\tloss: 0.180473\n",
      "Training Epoch 11  76.5% | batch:       228 of       298\t|\tloss: 0.263147\n",
      "Training Epoch 11  76.8% | batch:       229 of       298\t|\tloss: 0.178884\n",
      "Training Epoch 11  77.2% | batch:       230 of       298\t|\tloss: 0.175245\n",
      "Training Epoch 11  77.5% | batch:       231 of       298\t|\tloss: 0.176286\n",
      "Training Epoch 11  77.9% | batch:       232 of       298\t|\tloss: 0.184351\n",
      "Training Epoch 11  78.2% | batch:       233 of       298\t|\tloss: 0.24975\n",
      "Training Epoch 11  78.5% | batch:       234 of       298\t|\tloss: 0.381122\n",
      "Training Epoch 11  78.9% | batch:       235 of       298\t|\tloss: 0.323352\n",
      "Training Epoch 11  79.2% | batch:       236 of       298\t|\tloss: 0.17772\n",
      "Training Epoch 11  79.5% | batch:       237 of       298\t|\tloss: 0.521201\n",
      "Training Epoch 11  79.9% | batch:       238 of       298\t|\tloss: 0.227917\n",
      "Training Epoch 11  80.2% | batch:       239 of       298\t|\tloss: 0.195164\n",
      "Training Epoch 11  80.5% | batch:       240 of       298\t|\tloss: 0.211012\n",
      "Training Epoch 11  80.9% | batch:       241 of       298\t|\tloss: 0.181285\n",
      "Training Epoch 11  81.2% | batch:       242 of       298\t|\tloss: 0.1444\n",
      "Training Epoch 11  81.5% | batch:       243 of       298\t|\tloss: 0.217489\n",
      "Training Epoch 11  81.9% | batch:       244 of       298\t|\tloss: 0.227628\n",
      "Training Epoch 11  82.2% | batch:       245 of       298\t|\tloss: 0.222223\n",
      "Training Epoch 11  82.6% | batch:       246 of       298\t|\tloss: 0.271566\n",
      "Training Epoch 11  82.9% | batch:       247 of       298\t|\tloss: 0.742976\n",
      "Training Epoch 11  83.2% | batch:       248 of       298\t|\tloss: 0.170381\n",
      "Training Epoch 11  83.6% | batch:       249 of       298\t|\tloss: 0.159592\n",
      "Training Epoch 11  83.9% | batch:       250 of       298\t|\tloss: 0.211098\n",
      "Training Epoch 11  84.2% | batch:       251 of       298\t|\tloss: 0.34531\n",
      "Training Epoch 11  84.6% | batch:       252 of       298\t|\tloss: 0.212481\n",
      "Training Epoch 11  84.9% | batch:       253 of       298\t|\tloss: 0.169623\n",
      "Training Epoch 11  85.2% | batch:       254 of       298\t|\tloss: 0.319972\n",
      "Training Epoch 11  85.6% | batch:       255 of       298\t|\tloss: 0.274427\n",
      "Training Epoch 11  85.9% | batch:       256 of       298\t|\tloss: 0.179589\n",
      "Training Epoch 11  86.2% | batch:       257 of       298\t|\tloss: 0.223961\n",
      "Training Epoch 11  86.6% | batch:       258 of       298\t|\tloss: 0.174498\n",
      "Training Epoch 11  86.9% | batch:       259 of       298\t|\tloss: 0.191537\n",
      "Training Epoch 11  87.2% | batch:       260 of       298\t|\tloss: 0.226489\n",
      "Training Epoch 11  87.6% | batch:       261 of       298\t|\tloss: 0.198494\n",
      "Training Epoch 11  87.9% | batch:       262 of       298\t|\tloss: 0.157741\n",
      "Training Epoch 11  88.3% | batch:       263 of       298\t|\tloss: 0.231927\n",
      "Training Epoch 11  88.6% | batch:       264 of       298\t|\tloss: 0.893951\n",
      "Training Epoch 11  88.9% | batch:       265 of       298\t|\tloss: 0.174683\n",
      "Training Epoch 11  89.3% | batch:       266 of       298\t|\tloss: 0.243562\n",
      "Training Epoch 11  89.6% | batch:       267 of       298\t|\tloss: 0.254391\n",
      "Training Epoch 11  89.9% | batch:       268 of       298\t|\tloss: 0.243492\n",
      "Training Epoch 11  90.3% | batch:       269 of       298\t|\tloss: 0.213114\n",
      "Training Epoch 11  90.6% | batch:       270 of       298\t|\tloss: 0.158527\n",
      "Training Epoch 11  90.9% | batch:       271 of       298\t|\tloss: 0.12137\n",
      "Training Epoch 11  91.3% | batch:       272 of       298\t|\tloss: 0.189438\n",
      "Training Epoch 11  91.6% | batch:       273 of       298\t|\tloss: 0.225255\n",
      "Training Epoch 11  91.9% | batch:       274 of       298\t|\tloss: 0.206697\n",
      "Training Epoch 11  92.3% | batch:       275 of       298\t|\tloss: 0.192594\n",
      "Training Epoch 11  92.6% | batch:       276 of       298\t|\tloss: 0.161908\n",
      "Training Epoch 11  93.0% | batch:       277 of       298\t|\tloss: 0.1769\n",
      "Training Epoch 11  93.3% | batch:       278 of       298\t|\tloss: 0.397519\n",
      "Training Epoch 11  93.6% | batch:       279 of       298\t|\tloss: 0.211323\n",
      "Training Epoch 11  94.0% | batch:       280 of       298\t|\tloss: 0.253221\n",
      "Training Epoch 11  94.3% | batch:       281 of       298\t|\tloss: 0.215889\n",
      "Training Epoch 11  94.6% | batch:       282 of       298\t|\tloss: 0.297423\n",
      "Training Epoch 11  95.0% | batch:       283 of       298\t|\tloss: 0.152533\n",
      "Training Epoch 11  95.3% | batch:       284 of       298\t|\tloss: 0.193265\n",
      "Training Epoch 11  95.6% | batch:       285 of       298\t|\tloss: 0.220576\n",
      "Training Epoch 11  96.0% | batch:       286 of       298\t|\tloss: 0.178977\n",
      "Training Epoch 11  96.3% | batch:       287 of       298\t|\tloss: 0.213049\n",
      "Training Epoch 11  96.6% | batch:       288 of       298\t|\tloss: 0.380448\n",
      "Training Epoch 11  97.0% | batch:       289 of       298\t|\tloss: 0.148763\n",
      "Training Epoch 11  97.3% | batch:       290 of       298\t|\tloss: 0.232089\n",
      "Training Epoch 11  97.7% | batch:       291 of       298\t|\tloss: 0.185019\n",
      "Training Epoch 11  98.0% | batch:       292 of       298\t|\tloss: 0.197662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:38:24,323 | INFO : Epoch 11 Training Summary: epoch: 11.000000 | loss: 0.320425 | \n",
      "2023-06-22 14:38:24,324 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.23210096359253 seconds\n",
      "\n",
      "2023-06-22 14:38:24,325 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.275036790154196 seconds\n",
      "2023-06-22 14:38:24,326 | INFO : Avg batch train. time: 0.041191398624678514 seconds\n",
      "2023-06-22 14:38:24,327 | INFO : Avg sample train. time: 0.0012875012366429827 seconds\n",
      "Training Epoch:   3%|▎         | 11/400 [02:22<1:22:58, 12.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 11  98.3% | batch:       293 of       298\t|\tloss: 0.233226\n",
      "Training Epoch 11  98.7% | batch:       294 of       298\t|\tloss: 0.190881\n",
      "Training Epoch 11  99.0% | batch:       295 of       298\t|\tloss: 0.202362\n",
      "Training Epoch 11  99.3% | batch:       296 of       298\t|\tloss: 0.189186\n",
      "Training Epoch 11  99.7% | batch:       297 of       298\t|\tloss: 0.25468\n",
      "\n",
      "Training Epoch 12   0.0% | batch:         0 of       298\t|\tloss: 0.230975\n",
      "Training Epoch 12   0.3% | batch:         1 of       298\t|\tloss: 0.280066\n",
      "Training Epoch 12   0.7% | batch:         2 of       298\t|\tloss: 0.236435\n",
      "Training Epoch 12   1.0% | batch:         3 of       298\t|\tloss: 0.417578\n",
      "Training Epoch 12   1.3% | batch:         4 of       298\t|\tloss: 0.199417\n",
      "Training Epoch 12   1.7% | batch:         5 of       298\t|\tloss: 0.167222\n",
      "Training Epoch 12   2.0% | batch:         6 of       298\t|\tloss: 0.168877\n",
      "Training Epoch 12   2.3% | batch:         7 of       298\t|\tloss: 0.214877\n",
      "Training Epoch 12   2.7% | batch:         8 of       298\t|\tloss: 0.329523\n",
      "Training Epoch 12   3.0% | batch:         9 of       298\t|\tloss: 0.204756\n",
      "Training Epoch 12   3.4% | batch:        10 of       298\t|\tloss: 0.204614\n",
      "Training Epoch 12   3.7% | batch:        11 of       298\t|\tloss: 0.209744\n",
      "Training Epoch 12   4.0% | batch:        12 of       298\t|\tloss: 0.210869\n",
      "Training Epoch 12   4.4% | batch:        13 of       298\t|\tloss: 0.291564\n",
      "Training Epoch 12   4.7% | batch:        14 of       298\t|\tloss: 0.280841\n",
      "Training Epoch 12   5.0% | batch:        15 of       298\t|\tloss: 0.278304\n",
      "Training Epoch 12   5.4% | batch:        16 of       298\t|\tloss: 0.249133\n",
      "Training Epoch 12   5.7% | batch:        17 of       298\t|\tloss: 0.142723\n",
      "Training Epoch 12   6.0% | batch:        18 of       298\t|\tloss: 0.206214\n",
      "Training Epoch 12   6.4% | batch:        19 of       298\t|\tloss: 0.233463\n",
      "Training Epoch 12   6.7% | batch:        20 of       298\t|\tloss: 0.999525\n",
      "Training Epoch 12   7.0% | batch:        21 of       298\t|\tloss: 0.299232\n",
      "Training Epoch 12   7.4% | batch:        22 of       298\t|\tloss: 0.216912\n",
      "Training Epoch 12   7.7% | batch:        23 of       298\t|\tloss: 0.226926\n",
      "Training Epoch 12   8.1% | batch:        24 of       298\t|\tloss: 0.186571\n",
      "Training Epoch 12   8.4% | batch:        25 of       298\t|\tloss: 0.168889\n",
      "Training Epoch 12   8.7% | batch:        26 of       298\t|\tloss: 0.402414\n",
      "Training Epoch 12   9.1% | batch:        27 of       298\t|\tloss: 0.139019\n",
      "Training Epoch 12   9.4% | batch:        28 of       298\t|\tloss: 0.646511\n",
      "Training Epoch 12   9.7% | batch:        29 of       298\t|\tloss: 0.215132\n",
      "Training Epoch 12  10.1% | batch:        30 of       298\t|\tloss: 0.770554\n",
      "Training Epoch 12  10.4% | batch:        31 of       298\t|\tloss: 0.346813\n",
      "Training Epoch 12  10.7% | batch:        32 of       298\t|\tloss: 1.8241\n",
      "Training Epoch 12  11.1% | batch:        33 of       298\t|\tloss: 0.375486\n",
      "Training Epoch 12  11.4% | batch:        34 of       298\t|\tloss: 0.405431\n",
      "Training Epoch 12  11.7% | batch:        35 of       298\t|\tloss: 0.17867\n",
      "Training Epoch 12  12.1% | batch:        36 of       298\t|\tloss: 0.203379\n",
      "Training Epoch 12  12.4% | batch:        37 of       298\t|\tloss: 0.216849\n",
      "Training Epoch 12  12.8% | batch:        38 of       298\t|\tloss: 0.14675\n",
      "Training Epoch 12  13.1% | batch:        39 of       298\t|\tloss: 0.158811\n",
      "Training Epoch 12  13.4% | batch:        40 of       298\t|\tloss: 0.174676\n",
      "Training Epoch 12  13.8% | batch:        41 of       298\t|\tloss: 0.293786\n",
      "Training Epoch 12  14.1% | batch:        42 of       298\t|\tloss: 0.250155\n",
      "Training Epoch 12  14.4% | batch:        43 of       298\t|\tloss: 0.250738\n",
      "Training Epoch 12  14.8% | batch:        44 of       298\t|\tloss: 0.17708\n",
      "Training Epoch 12  15.1% | batch:        45 of       298\t|\tloss: 1.5099\n",
      "Training Epoch 12  15.4% | batch:        46 of       298\t|\tloss: 0.711058\n",
      "Training Epoch 12  15.8% | batch:        47 of       298\t|\tloss: 0.12382\n",
      "Training Epoch 12  16.1% | batch:        48 of       298\t|\tloss: 0.240202\n",
      "Training Epoch 12  16.4% | batch:        49 of       298\t|\tloss: 0.150328\n",
      "Training Epoch 12  16.8% | batch:        50 of       298\t|\tloss: 0.219938\n",
      "Training Epoch 12  17.1% | batch:        51 of       298\t|\tloss: 0.594836\n",
      "Training Epoch 12  17.4% | batch:        52 of       298\t|\tloss: 0.235337\n",
      "Training Epoch 12  17.8% | batch:        53 of       298\t|\tloss: 0.471727\n",
      "Training Epoch 12  18.1% | batch:        54 of       298\t|\tloss: 0.322026\n",
      "Training Epoch 12  18.5% | batch:        55 of       298\t|\tloss: 0.19519\n",
      "Training Epoch 12  18.8% | batch:        56 of       298\t|\tloss: 2.02782\n",
      "Training Epoch 12  19.1% | batch:        57 of       298\t|\tloss: 0.365463\n",
      "Training Epoch 12  19.5% | batch:        58 of       298\t|\tloss: 0.230817\n",
      "Training Epoch 12  19.8% | batch:        59 of       298\t|\tloss: 0.244552\n",
      "Training Epoch 12  20.1% | batch:        60 of       298\t|\tloss: 0.185505\n",
      "Training Epoch 12  20.5% | batch:        61 of       298\t|\tloss: 0.237366\n",
      "Training Epoch 12  20.8% | batch:        62 of       298\t|\tloss: 0.181988\n",
      "Training Epoch 12  21.1% | batch:        63 of       298\t|\tloss: 0.189698\n",
      "Training Epoch 12  21.5% | batch:        64 of       298\t|\tloss: 0.211398\n",
      "Training Epoch 12  21.8% | batch:        65 of       298\t|\tloss: 0.210807\n",
      "Training Epoch 12  22.1% | batch:        66 of       298\t|\tloss: 0.194199\n",
      "Training Epoch 12  22.5% | batch:        67 of       298\t|\tloss: 0.443919\n",
      "Training Epoch 12  22.8% | batch:        68 of       298\t|\tloss: 0.243083\n",
      "Training Epoch 12  23.2% | batch:        69 of       298\t|\tloss: 0.17715\n",
      "Training Epoch 12  23.5% | batch:        70 of       298\t|\tloss: 0.502597\n",
      "Training Epoch 12  23.8% | batch:        71 of       298\t|\tloss: 0.723936\n",
      "Training Epoch 12  24.2% | batch:        72 of       298\t|\tloss: 0.255349\n",
      "Training Epoch 12  24.5% | batch:        73 of       298\t|\tloss: 0.202058\n",
      "Training Epoch 12  24.8% | batch:        74 of       298\t|\tloss: 0.2966\n",
      "Training Epoch 12  25.2% | batch:        75 of       298\t|\tloss: 0.246085\n",
      "Training Epoch 12  25.5% | batch:        76 of       298\t|\tloss: 0.191546\n",
      "Training Epoch 12  25.8% | batch:        77 of       298\t|\tloss: 0.394727\n",
      "Training Epoch 12  26.2% | batch:        78 of       298\t|\tloss: 0.307987\n",
      "Training Epoch 12  26.5% | batch:        79 of       298\t|\tloss: 0.19507\n",
      "Training Epoch 12  26.8% | batch:        80 of       298\t|\tloss: 0.154353\n",
      "Training Epoch 12  27.2% | batch:        81 of       298\t|\tloss: 0.20499\n",
      "Training Epoch 12  27.5% | batch:        82 of       298\t|\tloss: 0.258159\n",
      "Training Epoch 12  27.9% | batch:        83 of       298\t|\tloss: 0.218716\n",
      "Training Epoch 12  28.2% | batch:        84 of       298\t|\tloss: 0.184553\n",
      "Training Epoch 12  28.5% | batch:        85 of       298\t|\tloss: 0.351385\n",
      "Training Epoch 12  28.9% | batch:        86 of       298\t|\tloss: 0.188867\n",
      "Training Epoch 12  29.2% | batch:        87 of       298\t|\tloss: 0.196972\n",
      "Training Epoch 12  29.5% | batch:        88 of       298\t|\tloss: 0.224522\n",
      "Training Epoch 12  29.9% | batch:        89 of       298\t|\tloss: 0.323949\n",
      "Training Epoch 12  30.2% | batch:        90 of       298\t|\tloss: 0.267062\n",
      "Training Epoch 12  30.5% | batch:        91 of       298\t|\tloss: 0.227452\n",
      "Training Epoch 12  30.9% | batch:        92 of       298\t|\tloss: 0.206341\n",
      "Training Epoch 12  31.2% | batch:        93 of       298\t|\tloss: 0.212333\n",
      "Training Epoch 12  31.5% | batch:        94 of       298\t|\tloss: 0.327037\n",
      "Training Epoch 12  31.9% | batch:        95 of       298\t|\tloss: 0.226814\n",
      "Training Epoch 12  32.2% | batch:        96 of       298\t|\tloss: 0.178113\n",
      "Training Epoch 12  32.6% | batch:        97 of       298\t|\tloss: 0.145498\n",
      "Training Epoch 12  32.9% | batch:        98 of       298\t|\tloss: 0.237451\n",
      "Training Epoch 12  33.2% | batch:        99 of       298\t|\tloss: 1.14327\n",
      "Training Epoch 12  33.6% | batch:       100 of       298\t|\tloss: 0.324866\n",
      "Training Epoch 12  33.9% | batch:       101 of       298\t|\tloss: 0.498083\n",
      "Training Epoch 12  34.2% | batch:       102 of       298\t|\tloss: 0.298633\n",
      "Training Epoch 12  34.6% | batch:       103 of       298\t|\tloss: 0.195895\n",
      "Training Epoch 12  34.9% | batch:       104 of       298\t|\tloss: 0.517197\n",
      "Training Epoch 12  35.2% | batch:       105 of       298\t|\tloss: 0.200852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  35.6% | batch:       106 of       298\t|\tloss: 0.19739\n",
      "Training Epoch 12  35.9% | batch:       107 of       298\t|\tloss: 0.229162\n",
      "Training Epoch 12  36.2% | batch:       108 of       298\t|\tloss: 0.14183\n",
      "Training Epoch 12  36.6% | batch:       109 of       298\t|\tloss: 0.177289\n",
      "Training Epoch 12  36.9% | batch:       110 of       298\t|\tloss: 0.212801\n",
      "Training Epoch 12  37.2% | batch:       111 of       298\t|\tloss: 0.173869\n",
      "Training Epoch 12  37.6% | batch:       112 of       298\t|\tloss: 0.166407\n",
      "Training Epoch 12  37.9% | batch:       113 of       298\t|\tloss: 0.277556\n",
      "Training Epoch 12  38.3% | batch:       114 of       298\t|\tloss: 0.321158\n",
      "Training Epoch 12  38.6% | batch:       115 of       298\t|\tloss: 0.235617\n",
      "Training Epoch 12  38.9% | batch:       116 of       298\t|\tloss: 0.230623\n",
      "Training Epoch 12  39.3% | batch:       117 of       298\t|\tloss: 0.180508\n",
      "Training Epoch 12  39.6% | batch:       118 of       298\t|\tloss: 0.184356\n",
      "Training Epoch 12  39.9% | batch:       119 of       298\t|\tloss: 0.228009\n",
      "Training Epoch 12  40.3% | batch:       120 of       298\t|\tloss: 0.264607\n",
      "Training Epoch 12  40.6% | batch:       121 of       298\t|\tloss: 0.270735\n",
      "Training Epoch 12  40.9% | batch:       122 of       298\t|\tloss: 0.169537\n",
      "Training Epoch 12  41.3% | batch:       123 of       298\t|\tloss: 0.224529\n",
      "Training Epoch 12  41.6% | batch:       124 of       298\t|\tloss: 0.521026\n",
      "Training Epoch 12  41.9% | batch:       125 of       298\t|\tloss: 0.619914\n",
      "Training Epoch 12  42.3% | batch:       126 of       298\t|\tloss: 0.731391\n",
      "Training Epoch 12  42.6% | batch:       127 of       298\t|\tloss: 0.187763\n",
      "Training Epoch 12  43.0% | batch:       128 of       298\t|\tloss: 0.247977\n",
      "Training Epoch 12  43.3% | batch:       129 of       298\t|\tloss: 0.19491\n",
      "Training Epoch 12  43.6% | batch:       130 of       298\t|\tloss: 0.322956\n",
      "Training Epoch 12  44.0% | batch:       131 of       298\t|\tloss: 0.174527\n",
      "Training Epoch 12  44.3% | batch:       132 of       298\t|\tloss: 0.170056\n",
      "Training Epoch 12  44.6% | batch:       133 of       298\t|\tloss: 0.317863\n",
      "Training Epoch 12  45.0% | batch:       134 of       298\t|\tloss: 0.195633\n",
      "Training Epoch 12  45.3% | batch:       135 of       298\t|\tloss: 0.180551\n",
      "Training Epoch 12  45.6% | batch:       136 of       298\t|\tloss: 0.23513\n",
      "Training Epoch 12  46.0% | batch:       137 of       298\t|\tloss: 0.157076\n",
      "Training Epoch 12  46.3% | batch:       138 of       298\t|\tloss: 0.244032\n",
      "Training Epoch 12  46.6% | batch:       139 of       298\t|\tloss: 0.163636\n",
      "Training Epoch 12  47.0% | batch:       140 of       298\t|\tloss: 0.152663\n",
      "Training Epoch 12  47.3% | batch:       141 of       298\t|\tloss: 0.185607\n",
      "Training Epoch 12  47.7% | batch:       142 of       298\t|\tloss: 0.216581\n",
      "Training Epoch 12  48.0% | batch:       143 of       298\t|\tloss: 0.21798\n",
      "Training Epoch 12  48.3% | batch:       144 of       298\t|\tloss: 0.638224\n",
      "Training Epoch 12  48.7% | batch:       145 of       298\t|\tloss: 0.338734\n",
      "Training Epoch 12  49.0% | batch:       146 of       298\t|\tloss: 0.463472\n",
      "Training Epoch 12  49.3% | batch:       147 of       298\t|\tloss: 0.310265\n",
      "Training Epoch 12  49.7% | batch:       148 of       298\t|\tloss: 0.168397\n",
      "Training Epoch 12  50.0% | batch:       149 of       298\t|\tloss: 0.194262\n",
      "Training Epoch 12  50.3% | batch:       150 of       298\t|\tloss: 0.168392\n",
      "Training Epoch 12  50.7% | batch:       151 of       298\t|\tloss: 0.219162\n",
      "Training Epoch 12  51.0% | batch:       152 of       298\t|\tloss: 0.21458\n",
      "Training Epoch 12  51.3% | batch:       153 of       298\t|\tloss: 0.183193\n",
      "Training Epoch 12  51.7% | batch:       154 of       298\t|\tloss: 0.192612\n",
      "Training Epoch 12  52.0% | batch:       155 of       298\t|\tloss: 0.376522\n",
      "Training Epoch 12  52.3% | batch:       156 of       298\t|\tloss: 0.136638\n",
      "Training Epoch 12  52.7% | batch:       157 of       298\t|\tloss: 0.196937\n",
      "Training Epoch 12  53.0% | batch:       158 of       298\t|\tloss: 0.233422\n",
      "Training Epoch 12  53.4% | batch:       159 of       298\t|\tloss: 0.250171\n",
      "Training Epoch 12  53.7% | batch:       160 of       298\t|\tloss: 0.184487\n",
      "Training Epoch 12  54.0% | batch:       161 of       298\t|\tloss: 0.239401\n",
      "Training Epoch 12  54.4% | batch:       162 of       298\t|\tloss: 0.198286\n",
      "Training Epoch 12  54.7% | batch:       163 of       298\t|\tloss: 0.508826\n",
      "Training Epoch 12  55.0% | batch:       164 of       298\t|\tloss: 0.173677\n",
      "Training Epoch 12  55.4% | batch:       165 of       298\t|\tloss: 0.181772\n",
      "Training Epoch 12  55.7% | batch:       166 of       298\t|\tloss: 0.233977\n",
      "Training Epoch 12  56.0% | batch:       167 of       298\t|\tloss: 0.202468\n",
      "Training Epoch 12  56.4% | batch:       168 of       298\t|\tloss: 0.217386\n",
      "Training Epoch 12  56.7% | batch:       169 of       298\t|\tloss: 0.234543\n",
      "Training Epoch 12  57.0% | batch:       170 of       298\t|\tloss: 0.707088\n",
      "Training Epoch 12  57.4% | batch:       171 of       298\t|\tloss: 0.291388\n",
      "Training Epoch 12  57.7% | batch:       172 of       298\t|\tloss: 0.209318\n",
      "Training Epoch 12  58.1% | batch:       173 of       298\t|\tloss: 0.160638\n",
      "Training Epoch 12  58.4% | batch:       174 of       298\t|\tloss: 0.274175\n",
      "Training Epoch 12  58.7% | batch:       175 of       298\t|\tloss: 0.210542\n",
      "Training Epoch 12  59.1% | batch:       176 of       298\t|\tloss: 0.203825\n",
      "Training Epoch 12  59.4% | batch:       177 of       298\t|\tloss: 0.211928\n",
      "Training Epoch 12  59.7% | batch:       178 of       298\t|\tloss: 0.171318\n",
      "Training Epoch 12  60.1% | batch:       179 of       298\t|\tloss: 0.226575\n",
      "Training Epoch 12  60.4% | batch:       180 of       298\t|\tloss: 0.214495\n",
      "Training Epoch 12  60.7% | batch:       181 of       298\t|\tloss: 0.22476\n",
      "Training Epoch 12  61.1% | batch:       182 of       298\t|\tloss: 0.192045\n",
      "Training Epoch 12  61.4% | batch:       183 of       298\t|\tloss: 0.229914\n",
      "Training Epoch 12  61.7% | batch:       184 of       298\t|\tloss: 0.2042\n",
      "Training Epoch 12  62.1% | batch:       185 of       298\t|\tloss: 1.04537\n",
      "Training Epoch 12  62.4% | batch:       186 of       298\t|\tloss: 0.31898\n",
      "Training Epoch 12  62.8% | batch:       187 of       298\t|\tloss: 0.173169\n",
      "Training Epoch 12  63.1% | batch:       188 of       298\t|\tloss: 0.268234\n",
      "Training Epoch 12  63.4% | batch:       189 of       298\t|\tloss: 0.232946\n",
      "Training Epoch 12  63.8% | batch:       190 of       298\t|\tloss: 0.217337\n",
      "Training Epoch 12  64.1% | batch:       191 of       298\t|\tloss: 0.156231\n",
      "Training Epoch 12  64.4% | batch:       192 of       298\t|\tloss: 0.318989\n",
      "Training Epoch 12  64.8% | batch:       193 of       298\t|\tloss: 1.71523\n",
      "Training Epoch 12  65.1% | batch:       194 of       298\t|\tloss: 0.200065\n",
      "Training Epoch 12  65.4% | batch:       195 of       298\t|\tloss: 0.158891\n",
      "Training Epoch 12  65.8% | batch:       196 of       298\t|\tloss: 0.216009\n",
      "Training Epoch 12  66.1% | batch:       197 of       298\t|\tloss: 0.141347\n",
      "Training Epoch 12  66.4% | batch:       198 of       298\t|\tloss: 0.18577\n",
      "Training Epoch 12  66.8% | batch:       199 of       298\t|\tloss: 0.177801\n",
      "Training Epoch 12  67.1% | batch:       200 of       298\t|\tloss: 0.280888\n",
      "Training Epoch 12  67.4% | batch:       201 of       298\t|\tloss: 0.234524\n",
      "Training Epoch 12  67.8% | batch:       202 of       298\t|\tloss: 0.242075\n",
      "Training Epoch 12  68.1% | batch:       203 of       298\t|\tloss: 0.141101\n",
      "Training Epoch 12  68.5% | batch:       204 of       298\t|\tloss: 0.208481\n",
      "Training Epoch 12  68.8% | batch:       205 of       298\t|\tloss: 0.14483\n",
      "Training Epoch 12  69.1% | batch:       206 of       298\t|\tloss: 0.189144\n",
      "Training Epoch 12  69.5% | batch:       207 of       298\t|\tloss: 0.334432\n",
      "Training Epoch 12  69.8% | batch:       208 of       298\t|\tloss: 0.315516\n",
      "Training Epoch 12  70.1% | batch:       209 of       298\t|\tloss: 0.375868\n",
      "Training Epoch 12  70.5% | batch:       210 of       298\t|\tloss: 0.166675\n",
      "Training Epoch 12  70.8% | batch:       211 of       298\t|\tloss: 0.20386\n",
      "Training Epoch 12  71.1% | batch:       212 of       298\t|\tloss: 0.220555\n",
      "Training Epoch 12  71.5% | batch:       213 of       298\t|\tloss: 0.158561\n",
      "Training Epoch 12  71.8% | batch:       214 of       298\t|\tloss: 0.244948\n",
      "Training Epoch 12  72.1% | batch:       215 of       298\t|\tloss: 0.28168\n",
      "Training Epoch 12  72.5% | batch:       216 of       298\t|\tloss: 0.155961\n",
      "Training Epoch 12  72.8% | batch:       217 of       298\t|\tloss: 0.159166\n",
      "Training Epoch 12  73.2% | batch:       218 of       298\t|\tloss: 0.291669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  73.5% | batch:       219 of       298\t|\tloss: 0.167584\n",
      "Training Epoch 12  73.8% | batch:       220 of       298\t|\tloss: 0.157603\n",
      "Training Epoch 12  74.2% | batch:       221 of       298\t|\tloss: 0.281774\n",
      "Training Epoch 12  74.5% | batch:       222 of       298\t|\tloss: 0.160298\n",
      "Training Epoch 12  74.8% | batch:       223 of       298\t|\tloss: 0.193477\n",
      "Training Epoch 12  75.2% | batch:       224 of       298\t|\tloss: 0.198611\n",
      "Training Epoch 12  75.5% | batch:       225 of       298\t|\tloss: 0.266436\n",
      "Training Epoch 12  75.8% | batch:       226 of       298\t|\tloss: 0.19191\n",
      "Training Epoch 12  76.2% | batch:       227 of       298\t|\tloss: 0.188342\n",
      "Training Epoch 12  76.5% | batch:       228 of       298\t|\tloss: 0.204717\n",
      "Training Epoch 12  76.8% | batch:       229 of       298\t|\tloss: 0.247647\n",
      "Training Epoch 12  77.2% | batch:       230 of       298\t|\tloss: 0.214623\n",
      "Training Epoch 12  77.5% | batch:       231 of       298\t|\tloss: 0.176039\n",
      "Training Epoch 12  77.9% | batch:       232 of       298\t|\tloss: 0.417999\n",
      "Training Epoch 12  78.2% | batch:       233 of       298\t|\tloss: 0.237404\n",
      "Training Epoch 12  78.5% | batch:       234 of       298\t|\tloss: 0.39498\n",
      "Training Epoch 12  78.9% | batch:       235 of       298\t|\tloss: 0.230802\n",
      "Training Epoch 12  79.2% | batch:       236 of       298\t|\tloss: 0.219193\n",
      "Training Epoch 12  79.5% | batch:       237 of       298\t|\tloss: 0.401537\n",
      "Training Epoch 12  79.9% | batch:       238 of       298\t|\tloss: 0.235559\n",
      "Training Epoch 12  80.2% | batch:       239 of       298\t|\tloss: 0.150489\n",
      "Training Epoch 12  80.5% | batch:       240 of       298\t|\tloss: 0.137698\n",
      "Training Epoch 12  80.9% | batch:       241 of       298\t|\tloss: 0.224728\n",
      "Training Epoch 12  81.2% | batch:       242 of       298\t|\tloss: 0.453986\n",
      "Training Epoch 12  81.5% | batch:       243 of       298\t|\tloss: 0.183456\n",
      "Training Epoch 12  81.9% | batch:       244 of       298\t|\tloss: 0.300882\n",
      "Training Epoch 12  82.2% | batch:       245 of       298\t|\tloss: 0.227891\n",
      "Training Epoch 12  82.6% | batch:       246 of       298\t|\tloss: 0.180192\n",
      "Training Epoch 12  82.9% | batch:       247 of       298\t|\tloss: 0.19772\n",
      "Training Epoch 12  83.2% | batch:       248 of       298\t|\tloss: 0.149464\n",
      "Training Epoch 12  83.6% | batch:       249 of       298\t|\tloss: 0.158188\n",
      "Training Epoch 12  83.9% | batch:       250 of       298\t|\tloss: 0.210452\n",
      "Training Epoch 12  84.2% | batch:       251 of       298\t|\tloss: 0.166061\n",
      "Training Epoch 12  84.6% | batch:       252 of       298\t|\tloss: 0.190885\n",
      "Training Epoch 12  84.9% | batch:       253 of       298\t|\tloss: 0.281935\n",
      "Training Epoch 12  85.2% | batch:       254 of       298\t|\tloss: 2.31646\n",
      "Training Epoch 12  85.6% | batch:       255 of       298\t|\tloss: 0.175239\n",
      "Training Epoch 12  85.9% | batch:       256 of       298\t|\tloss: 0.318264\n",
      "Training Epoch 12  86.2% | batch:       257 of       298\t|\tloss: 0.190239\n",
      "Training Epoch 12  86.6% | batch:       258 of       298\t|\tloss: 0.242586\n",
      "Training Epoch 12  86.9% | batch:       259 of       298\t|\tloss: 0.246586\n",
      "Training Epoch 12  87.2% | batch:       260 of       298\t|\tloss: 0.271\n",
      "Training Epoch 12  87.6% | batch:       261 of       298\t|\tloss: 0.223587\n",
      "Training Epoch 12  87.9% | batch:       262 of       298\t|\tloss: 0.171644\n",
      "Training Epoch 12  88.3% | batch:       263 of       298\t|\tloss: 0.185128\n",
      "Training Epoch 12  88.6% | batch:       264 of       298\t|\tloss: 0.230164\n",
      "Training Epoch 12  88.9% | batch:       265 of       298\t|\tloss: 0.176424\n",
      "Training Epoch 12  89.3% | batch:       266 of       298\t|\tloss: 0.219231\n",
      "Training Epoch 12  89.6% | batch:       267 of       298\t|\tloss: 0.204056\n",
      "Training Epoch 12  89.9% | batch:       268 of       298\t|\tloss: 0.132437\n",
      "Training Epoch 12  90.3% | batch:       269 of       298\t|\tloss: 0.139229\n",
      "Training Epoch 12  90.6% | batch:       270 of       298\t|\tloss: 0.185556\n",
      "Training Epoch 12  90.9% | batch:       271 of       298\t|\tloss: 0.220315\n",
      "Training Epoch 12  91.3% | batch:       272 of       298\t|\tloss: 0.281369\n",
      "Training Epoch 12  91.6% | batch:       273 of       298\t|\tloss: 0.191761\n",
      "Training Epoch 12  91.9% | batch:       274 of       298\t|\tloss: 0.247783\n",
      "Training Epoch 12  92.3% | batch:       275 of       298\t|\tloss: 0.203805\n",
      "Training Epoch 12  92.6% | batch:       276 of       298\t|\tloss: 0.145691\n",
      "Training Epoch 12  93.0% | batch:       277 of       298\t|\tloss: 0.192386\n",
      "Training Epoch 12  93.3% | batch:       278 of       298\t|\tloss: 0.343683\n",
      "Training Epoch 12  93.6% | batch:       279 of       298\t|\tloss: 0.215078\n",
      "Training Epoch 12  94.0% | batch:       280 of       298\t|\tloss: 0.218333\n",
      "Training Epoch 12  94.3% | batch:       281 of       298\t|\tloss: 0.174232\n",
      "Training Epoch 12  94.6% | batch:       282 of       298\t|\tloss: 0.17885\n",
      "Training Epoch 12  95.0% | batch:       283 of       298\t|\tloss: 3.80119\n",
      "Training Epoch 12  95.3% | batch:       284 of       298\t|\tloss: 0.342228\n",
      "Training Epoch 12  95.6% | batch:       285 of       298\t|\tloss: 0.162886\n",
      "Training Epoch 12  96.0% | batch:       286 of       298\t|\tloss: 0.26022\n",
      "Training Epoch 12  96.3% | batch:       287 of       298\t|\tloss: 0.272636\n",
      "Training Epoch 12  96.6% | batch:       288 of       298\t|\tloss: 1.01683\n",
      "Training Epoch 12  97.0% | batch:       289 of       298\t|\tloss: 0.571996\n",
      "Training Epoch 12  97.3% | batch:       290 of       298\t|\tloss: 0.207168\n",
      "Training Epoch 12  97.7% | batch:       291 of       298\t|\tloss: 0.171198\n",
      "Training Epoch 12  98.0% | batch:       292 of       298\t|\tloss: 0.189131\n",
      "Training Epoch 12  98.3% | batch:       293 of       298\t|\tloss: 0.151141\n",
      "Training Epoch 12  98.7% | batch:       294 of       298\t|\tloss: 0.307194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:38:36,937 | INFO : Epoch 12 Training Summary: epoch: 12.000000 | loss: 0.299670 | \n",
      "2023-06-22 14:38:36,938 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.592979907989502 seconds\n",
      "\n",
      "2023-06-22 14:38:36,939 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.301532049973806 seconds\n",
      "2023-06-22 14:38:36,940 | INFO : Avg batch train. time: 0.04128030889252955 seconds\n",
      "2023-06-22 14:38:36,940 | INFO : Avg sample train. time: 0.001290280265363311 seconds\n",
      "2023-06-22 14:38:36,941 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 12  99.0% | batch:       295 of       298\t|\tloss: 0.18809\n",
      "Training Epoch 12  99.3% | batch:       296 of       298\t|\tloss: 0.308542\n",
      "Training Epoch 12  99.7% | batch:       297 of       298\t|\tloss: 0.208229\n",
      "\n",
      "Evaluating Epoch 12   0.0% | batch:         0 of        75\t|\tloss: 0.170041\n",
      "Evaluating Epoch 12   1.3% | batch:         1 of        75\t|\tloss: 0.278595\n",
      "Evaluating Epoch 12   2.7% | batch:         2 of        75\t|\tloss: 0.159068\n",
      "Evaluating Epoch 12   4.0% | batch:         3 of        75\t|\tloss: 0.334611\n",
      "Evaluating Epoch 12   5.3% | batch:         4 of        75\t|\tloss: 0.166447\n",
      "Evaluating Epoch 12   6.7% | batch:         5 of        75\t|\tloss: 0.474977\n",
      "Evaluating Epoch 12   8.0% | batch:         6 of        75\t|\tloss: 0.134769\n",
      "Evaluating Epoch 12   9.3% | batch:         7 of        75\t|\tloss: 0.157703\n",
      "Evaluating Epoch 12  10.7% | batch:         8 of        75\t|\tloss: 0.166243\n",
      "Evaluating Epoch 12  12.0% | batch:         9 of        75\t|\tloss: 0.206473\n",
      "Evaluating Epoch 12  13.3% | batch:        10 of        75\t|\tloss: 0.212119\n",
      "Evaluating Epoch 12  14.7% | batch:        11 of        75\t|\tloss: 0.149064\n",
      "Evaluating Epoch 12  16.0% | batch:        12 of        75\t|\tloss: 0.249268\n",
      "Evaluating Epoch 12  17.3% | batch:        13 of        75\t|\tloss: 0.244331\n",
      "Evaluating Epoch 12  18.7% | batch:        14 of        75\t|\tloss: 0.19422\n",
      "Evaluating Epoch 12  20.0% | batch:        15 of        75\t|\tloss: 2.07808\n",
      "Evaluating Epoch 12  21.3% | batch:        16 of        75\t|\tloss: 0.209123\n",
      "Evaluating Epoch 12  22.7% | batch:        17 of        75\t|\tloss: 0.156755\n",
      "Evaluating Epoch 12  24.0% | batch:        18 of        75\t|\tloss: 0.171136\n",
      "Evaluating Epoch 12  25.3% | batch:        19 of        75\t|\tloss: 0.206698\n",
      "Evaluating Epoch 12  26.7% | batch:        20 of        75\t|\tloss: 0.191031\n",
      "Evaluating Epoch 12  28.0% | batch:        21 of        75\t|\tloss: 0.171367\n",
      "Evaluating Epoch 12  29.3% | batch:        22 of        75\t|\tloss: 0.169756\n",
      "Evaluating Epoch 12  30.7% | batch:        23 of        75\t|\tloss: 1.08752\n",
      "Evaluating Epoch 12  32.0% | batch:        24 of        75\t|\tloss: 0.135232\n",
      "Evaluating Epoch 12  33.3% | batch:        25 of        75\t|\tloss: 1.59009\n",
      "Evaluating Epoch 12  34.7% | batch:        26 of        75\t|\tloss: 0.137087\n",
      "Evaluating Epoch 12  36.0% | batch:        27 of        75\t|\tloss: 0.211144\n",
      "Evaluating Epoch 12  37.3% | batch:        28 of        75\t|\tloss: 0.185734\n",
      "Evaluating Epoch 12  38.7% | batch:        29 of        75\t|\tloss: 0.210334\n",
      "Evaluating Epoch 12  40.0% | batch:        30 of        75\t|\tloss: 0.335257\n",
      "Evaluating Epoch 12  41.3% | batch:        31 of        75\t|\tloss: 0.142511\n",
      "Evaluating Epoch 12  42.7% | batch:        32 of        75\t|\tloss: 0.669173\n",
      "Evaluating Epoch 12  44.0% | batch:        33 of        75\t|\tloss: 0.19456\n",
      "Evaluating Epoch 12  45.3% | batch:        34 of        75\t|\tloss: 0.166768\n",
      "Evaluating Epoch 12  46.7% | batch:        35 of        75\t|\tloss: 0.525776\n",
      "Evaluating Epoch 12  48.0% | batch:        36 of        75\t|\tloss: 0.143996\n",
      "Evaluating Epoch 12  49.3% | batch:        37 of        75\t|\tloss: 0.275903\n",
      "Evaluating Epoch 12  50.7% | batch:        38 of        75\t|\tloss: 0.201773\n",
      "Evaluating Epoch 12  52.0% | batch:        39 of        75\t|\tloss: 0.602794\n",
      "Evaluating Epoch 12  53.3% | batch:        40 of        75\t|\tloss: 0.6029\n",
      "Evaluating Epoch 12  54.7% | batch:        41 of        75\t|\tloss: 0.149921\n",
      "Evaluating Epoch 12  56.0% | batch:        42 of        75\t|\tloss: 0.827683\n",
      "Evaluating Epoch 12  57.3% | batch:        43 of        75\t|\tloss: 0.22255\n",
      "Evaluating Epoch 12  58.7% | batch:        44 of        75\t|\tloss: 0.12635\n",
      "Evaluating Epoch 12  60.0% | batch:        45 of        75\t|\tloss: 0.172502\n",
      "Evaluating Epoch 12  61.3% | batch:        46 of        75\t|\tloss: 0.180225\n",
      "Evaluating Epoch 12  62.7% | batch:        47 of        75\t|\tloss: 0.847339\n",
      "Evaluating Epoch 12  64.0% | batch:        48 of        75\t|\tloss: 0.16435\n",
      "Evaluating Epoch 12  65.3% | batch:        49 of        75\t|\tloss: 0.17991\n",
      "Evaluating Epoch 12  66.7% | batch:        50 of        75\t|\tloss: 0.793952\n",
      "Evaluating Epoch 12  68.0% | batch:        51 of        75\t|\tloss: 0.250344\n",
      "Evaluating Epoch 12  69.3% | batch:        52 of        75\t|\tloss: 0.137921\n",
      "Evaluating Epoch 12  70.7% | batch:        53 of        75\t|\tloss: 0.175711\n",
      "Evaluating Epoch 12  72.0% | batch:        54 of        75\t|\tloss: 0.246009\n",
      "Evaluating Epoch 12  73.3% | batch:        55 of        75\t|\tloss: 0.139323\n",
      "Evaluating Epoch 12  74.7% | batch:        56 of        75\t|\tloss: 0.188766\n",
      "Evaluating Epoch 12  76.0% | batch:        57 of        75\t|\tloss: 0.233983\n",
      "Evaluating Epoch 12  77.3% | batch:        58 of        75\t|\tloss: 0.194975\n",
      "Evaluating Epoch 12  78.7% | batch:        59 of        75\t|\tloss: 7.20855\n",
      "Evaluating Epoch 12  80.0% | batch:        60 of        75\t|\tloss: 0.168733\n",
      "Evaluating Epoch 12  81.3% | batch:        61 of        75\t|\tloss: 0.132107\n",
      "Evaluating Epoch 12  82.7% | batch:        62 of        75\t|\tloss: 0.500345\n",
      "Evaluating Epoch 12  84.0% | batch:        63 of        75\t|\tloss: 1.85162\n",
      "Evaluating Epoch 12  85.3% | batch:        64 of        75\t|\tloss: 0.20315\n",
      "Evaluating Epoch 12  86.7% | batch:        65 of        75\t|\tloss: 0.217332\n",
      "Evaluating Epoch 12  88.0% | batch:        66 of        75\t|\tloss: 0.216939\n",
      "Evaluating Epoch 12  89.3% | batch:        67 of        75\t|\tloss: 0.14713\n",
      "Evaluating Epoch 12  90.7% | batch:        68 of        75\t|\tloss: 0.207346\n",
      "Evaluating Epoch 12  92.0% | batch:        69 of        75\t|\tloss: 0.20561\n",
      "Evaluating Epoch 12  93.3% | batch:        70 of        75\t|\tloss: 0.156766\n",
      "Evaluating Epoch 12  94.7% | batch:        71 of        75\t|\tloss: 0.149912\n",
      "Evaluating Epoch 12  96.0% | batch:        72 of        75\t|\tloss: 0.13419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:38:38,148 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.2060422897338867 seconds\n",
      "\n",
      "2023-06-22 14:38:38,149 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.2232767343521118 seconds\n",
      "2023-06-22 14:38:38,150 | INFO : Avg batch val. time: 0.01631035645802816 seconds\n",
      "2023-06-22 14:38:38,150 | INFO : Avg sample val. time: 0.0005131194355503825 seconds\n",
      "2023-06-22 14:38:38,151 | INFO : Epoch 12 Validation Summary: epoch: 12.000000 | loss: 0.425412 | \n",
      "Training Epoch:   3%|▎         | 12/400 [02:36<1:24:46, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 12  97.3% | batch:        73 of        75\t|\tloss: 0.394946\n",
      "Evaluating Epoch 12  98.7% | batch:        74 of        75\t|\tloss: 0.542281\n",
      "\n",
      "Training Epoch 13   0.0% | batch:         0 of       298\t|\tloss: 0.191238\n",
      "Training Epoch 13   0.3% | batch:         1 of       298\t|\tloss: 0.233242\n",
      "Training Epoch 13   0.7% | batch:         2 of       298\t|\tloss: 0.359509\n",
      "Training Epoch 13   1.0% | batch:         3 of       298\t|\tloss: 0.163149\n",
      "Training Epoch 13   1.3% | batch:         4 of       298\t|\tloss: 0.283244\n",
      "Training Epoch 13   1.7% | batch:         5 of       298\t|\tloss: 0.240411\n",
      "Training Epoch 13   2.0% | batch:         6 of       298\t|\tloss: 0.185399\n",
      "Training Epoch 13   2.3% | batch:         7 of       298\t|\tloss: 0.198212\n",
      "Training Epoch 13   2.7% | batch:         8 of       298\t|\tloss: 0.521111\n",
      "Training Epoch 13   3.0% | batch:         9 of       298\t|\tloss: 0.206528\n",
      "Training Epoch 13   3.4% | batch:        10 of       298\t|\tloss: 0.178718\n",
      "Training Epoch 13   3.7% | batch:        11 of       298\t|\tloss: 0.201106\n",
      "Training Epoch 13   4.0% | batch:        12 of       298\t|\tloss: 0.438559\n",
      "Training Epoch 13   4.4% | batch:        13 of       298\t|\tloss: 0.241718\n",
      "Training Epoch 13   4.7% | batch:        14 of       298\t|\tloss: 0.21424\n",
      "Training Epoch 13   5.0% | batch:        15 of       298\t|\tloss: 0.18174\n",
      "Training Epoch 13   5.4% | batch:        16 of       298\t|\tloss: 0.199542\n",
      "Training Epoch 13   5.7% | batch:        17 of       298\t|\tloss: 0.283163\n",
      "Training Epoch 13   6.0% | batch:        18 of       298\t|\tloss: 0.201415\n",
      "Training Epoch 13   6.4% | batch:        19 of       298\t|\tloss: 0.253356\n",
      "Training Epoch 13   6.7% | batch:        20 of       298\t|\tloss: 0.188683\n",
      "Training Epoch 13   7.0% | batch:        21 of       298\t|\tloss: 0.205715\n",
      "Training Epoch 13   7.4% | batch:        22 of       298\t|\tloss: 0.24076\n",
      "Training Epoch 13   7.7% | batch:        23 of       298\t|\tloss: 0.206412\n",
      "Training Epoch 13   8.1% | batch:        24 of       298\t|\tloss: 0.278115\n",
      "Training Epoch 13   8.4% | batch:        25 of       298\t|\tloss: 0.269899\n",
      "Training Epoch 13   8.7% | batch:        26 of       298\t|\tloss: 0.251414\n",
      "Training Epoch 13   9.1% | batch:        27 of       298\t|\tloss: 0.19873\n",
      "Training Epoch 13   9.4% | batch:        28 of       298\t|\tloss: 0.397207\n",
      "Training Epoch 13   9.7% | batch:        29 of       298\t|\tloss: 0.228336\n",
      "Training Epoch 13  10.1% | batch:        30 of       298\t|\tloss: 0.167222\n",
      "Training Epoch 13  10.4% | batch:        31 of       298\t|\tloss: 0.146509\n",
      "Training Epoch 13  10.7% | batch:        32 of       298\t|\tloss: 0.184342\n",
      "Training Epoch 13  11.1% | batch:        33 of       298\t|\tloss: 0.173492\n",
      "Training Epoch 13  11.4% | batch:        34 of       298\t|\tloss: 0.26932\n",
      "Training Epoch 13  11.7% | batch:        35 of       298\t|\tloss: 0.1312\n",
      "Training Epoch 13  12.1% | batch:        36 of       298\t|\tloss: 0.377883\n",
      "Training Epoch 13  12.4% | batch:        37 of       298\t|\tloss: 0.183838\n",
      "Training Epoch 13  12.8% | batch:        38 of       298\t|\tloss: 0.142253\n",
      "Training Epoch 13  13.1% | batch:        39 of       298\t|\tloss: 0.190403\n",
      "Training Epoch 13  13.4% | batch:        40 of       298\t|\tloss: 0.216428\n",
      "Training Epoch 13  13.8% | batch:        41 of       298\t|\tloss: 0.413009\n",
      "Training Epoch 13  14.1% | batch:        42 of       298\t|\tloss: 0.22904\n",
      "Training Epoch 13  14.4% | batch:        43 of       298\t|\tloss: 0.213858\n",
      "Training Epoch 13  14.8% | batch:        44 of       298\t|\tloss: 0.228324\n",
      "Training Epoch 13  15.1% | batch:        45 of       298\t|\tloss: 0.250254\n",
      "Training Epoch 13  15.4% | batch:        46 of       298\t|\tloss: 1.53944\n",
      "Training Epoch 13  15.8% | batch:        47 of       298\t|\tloss: 0.24258\n",
      "Training Epoch 13  16.1% | batch:        48 of       298\t|\tloss: 0.310232\n",
      "Training Epoch 13  16.4% | batch:        49 of       298\t|\tloss: 0.154116\n",
      "Training Epoch 13  16.8% | batch:        50 of       298\t|\tloss: 0.329011\n",
      "Training Epoch 13  17.1% | batch:        51 of       298\t|\tloss: 0.169051\n",
      "Training Epoch 13  17.4% | batch:        52 of       298\t|\tloss: 0.197224\n",
      "Training Epoch 13  17.8% | batch:        53 of       298\t|\tloss: 0.345853\n",
      "Training Epoch 13  18.1% | batch:        54 of       298\t|\tloss: 0.242484\n",
      "Training Epoch 13  18.5% | batch:        55 of       298\t|\tloss: 0.216368\n",
      "Training Epoch 13  18.8% | batch:        56 of       298\t|\tloss: 0.165612\n",
      "Training Epoch 13  19.1% | batch:        57 of       298\t|\tloss: 0.182627\n",
      "Training Epoch 13  19.5% | batch:        58 of       298\t|\tloss: 0.235776\n",
      "Training Epoch 13  19.8% | batch:        59 of       298\t|\tloss: 0.16937\n",
      "Training Epoch 13  20.1% | batch:        60 of       298\t|\tloss: 0.168119\n",
      "Training Epoch 13  20.5% | batch:        61 of       298\t|\tloss: 0.159113\n",
      "Training Epoch 13  20.8% | batch:        62 of       298\t|\tloss: 0.278645\n",
      "Training Epoch 13  21.1% | batch:        63 of       298\t|\tloss: 0.188177\n",
      "Training Epoch 13  21.5% | batch:        64 of       298\t|\tloss: 0.19531\n",
      "Training Epoch 13  21.8% | batch:        65 of       298\t|\tloss: 0.263955\n",
      "Training Epoch 13  22.1% | batch:        66 of       298\t|\tloss: 0.397229\n",
      "Training Epoch 13  22.5% | batch:        67 of       298\t|\tloss: 0.250346\n",
      "Training Epoch 13  22.8% | batch:        68 of       298\t|\tloss: 0.19863\n",
      "Training Epoch 13  23.2% | batch:        69 of       298\t|\tloss: 0.125172\n",
      "Training Epoch 13  23.5% | batch:        70 of       298\t|\tloss: 0.143455\n",
      "Training Epoch 13  23.8% | batch:        71 of       298\t|\tloss: 0.166609\n",
      "Training Epoch 13  24.2% | batch:        72 of       298\t|\tloss: 0.270252\n",
      "Training Epoch 13  24.5% | batch:        73 of       298\t|\tloss: 0.150714\n",
      "Training Epoch 13  24.8% | batch:        74 of       298\t|\tloss: 0.514537\n",
      "Training Epoch 13  25.2% | batch:        75 of       298\t|\tloss: 0.163918\n",
      "Training Epoch 13  25.5% | batch:        76 of       298\t|\tloss: 0.235411\n",
      "Training Epoch 13  25.8% | batch:        77 of       298\t|\tloss: 0.155337\n",
      "Training Epoch 13  26.2% | batch:        78 of       298\t|\tloss: 0.231915\n",
      "Training Epoch 13  26.5% | batch:        79 of       298\t|\tloss: 0.350292\n",
      "Training Epoch 13  26.8% | batch:        80 of       298\t|\tloss: 0.229318\n",
      "Training Epoch 13  27.2% | batch:        81 of       298\t|\tloss: 0.200615\n",
      "Training Epoch 13  27.5% | batch:        82 of       298\t|\tloss: 0.172769\n",
      "Training Epoch 13  27.9% | batch:        83 of       298\t|\tloss: 0.198043\n",
      "Training Epoch 13  28.2% | batch:        84 of       298\t|\tloss: 0.247253\n",
      "Training Epoch 13  28.5% | batch:        85 of       298\t|\tloss: 0.197788\n",
      "Training Epoch 13  28.9% | batch:        86 of       298\t|\tloss: 0.128435\n",
      "Training Epoch 13  29.2% | batch:        87 of       298\t|\tloss: 0.1679\n",
      "Training Epoch 13  29.5% | batch:        88 of       298\t|\tloss: 0.689701\n",
      "Training Epoch 13  29.9% | batch:        89 of       298\t|\tloss: 0.260896\n",
      "Training Epoch 13  30.2% | batch:        90 of       298\t|\tloss: 0.192433\n",
      "Training Epoch 13  30.5% | batch:        91 of       298\t|\tloss: 0.154148\n",
      "Training Epoch 13  30.9% | batch:        92 of       298\t|\tloss: 0.231456\n",
      "Training Epoch 13  31.2% | batch:        93 of       298\t|\tloss: 0.244061\n",
      "Training Epoch 13  31.5% | batch:        94 of       298\t|\tloss: 0.184733\n",
      "Training Epoch 13  31.9% | batch:        95 of       298\t|\tloss: 0.175302\n",
      "Training Epoch 13  32.2% | batch:        96 of       298\t|\tloss: 0.305985\n",
      "Training Epoch 13  32.6% | batch:        97 of       298\t|\tloss: 0.230721\n",
      "Training Epoch 13  32.9% | batch:        98 of       298\t|\tloss: 0.182802\n",
      "Training Epoch 13  33.2% | batch:        99 of       298\t|\tloss: 0.161574\n",
      "Training Epoch 13  33.6% | batch:       100 of       298\t|\tloss: 0.190964\n",
      "Training Epoch 13  33.9% | batch:       101 of       298\t|\tloss: 0.186802\n",
      "Training Epoch 13  34.2% | batch:       102 of       298\t|\tloss: 0.281253\n",
      "Training Epoch 13  34.6% | batch:       103 of       298\t|\tloss: 0.255392\n",
      "Training Epoch 13  34.9% | batch:       104 of       298\t|\tloss: 0.290738\n",
      "Training Epoch 13  35.2% | batch:       105 of       298\t|\tloss: 0.204169\n",
      "Training Epoch 13  35.6% | batch:       106 of       298\t|\tloss: 0.192582\n",
      "Training Epoch 13  35.9% | batch:       107 of       298\t|\tloss: 0.193094\n",
      "Training Epoch 13  36.2% | batch:       108 of       298\t|\tloss: 0.144166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 13  36.6% | batch:       109 of       298\t|\tloss: 0.480858\n",
      "Training Epoch 13  36.9% | batch:       110 of       298\t|\tloss: 0.524399\n",
      "Training Epoch 13  37.2% | batch:       111 of       298\t|\tloss: 0.149014\n",
      "Training Epoch 13  37.6% | batch:       112 of       298\t|\tloss: 0.24688\n",
      "Training Epoch 13  37.9% | batch:       113 of       298\t|\tloss: 0.235805\n",
      "Training Epoch 13  38.3% | batch:       114 of       298\t|\tloss: 0.219976\n",
      "Training Epoch 13  38.6% | batch:       115 of       298\t|\tloss: 0.23932\n",
      "Training Epoch 13  38.9% | batch:       116 of       298\t|\tloss: 0.214242\n",
      "Training Epoch 13  39.3% | batch:       117 of       298\t|\tloss: 0.247188\n",
      "Training Epoch 13  39.6% | batch:       118 of       298\t|\tloss: 0.178585\n",
      "Training Epoch 13  39.9% | batch:       119 of       298\t|\tloss: 0.32721\n",
      "Training Epoch 13  40.3% | batch:       120 of       298\t|\tloss: 0.655018\n",
      "Training Epoch 13  40.6% | batch:       121 of       298\t|\tloss: 0.229144\n",
      "Training Epoch 13  40.9% | batch:       122 of       298\t|\tloss: 0.364673\n",
      "Training Epoch 13  41.3% | batch:       123 of       298\t|\tloss: 0.19947\n",
      "Training Epoch 13  41.6% | batch:       124 of       298\t|\tloss: 0.199184\n",
      "Training Epoch 13  41.9% | batch:       125 of       298\t|\tloss: 0.196024\n",
      "Training Epoch 13  42.3% | batch:       126 of       298\t|\tloss: 0.198737\n",
      "Training Epoch 13  42.6% | batch:       127 of       298\t|\tloss: 0.210757\n",
      "Training Epoch 13  43.0% | batch:       128 of       298\t|\tloss: 0.184913\n",
      "Training Epoch 13  43.3% | batch:       129 of       298\t|\tloss: 0.184014\n",
      "Training Epoch 13  43.6% | batch:       130 of       298\t|\tloss: 0.300564\n",
      "Training Epoch 13  44.0% | batch:       131 of       298\t|\tloss: 0.143381\n",
      "Training Epoch 13  44.3% | batch:       132 of       298\t|\tloss: 0.30943\n",
      "Training Epoch 13  44.6% | batch:       133 of       298\t|\tloss: 0.283603\n",
      "Training Epoch 13  45.0% | batch:       134 of       298\t|\tloss: 0.302895\n",
      "Training Epoch 13  45.3% | batch:       135 of       298\t|\tloss: 0.194144\n",
      "Training Epoch 13  45.6% | batch:       136 of       298\t|\tloss: 0.209752\n",
      "Training Epoch 13  46.0% | batch:       137 of       298\t|\tloss: 0.348094\n",
      "Training Epoch 13  46.3% | batch:       138 of       298\t|\tloss: 0.252697\n",
      "Training Epoch 13  46.6% | batch:       139 of       298\t|\tloss: 0.443546\n",
      "Training Epoch 13  47.0% | batch:       140 of       298\t|\tloss: 0.235198\n",
      "Training Epoch 13  47.3% | batch:       141 of       298\t|\tloss: 0.25661\n",
      "Training Epoch 13  47.7% | batch:       142 of       298\t|\tloss: 0.171702\n",
      "Training Epoch 13  48.0% | batch:       143 of       298\t|\tloss: 0.246118\n",
      "Training Epoch 13  48.3% | batch:       144 of       298\t|\tloss: 0.25969\n",
      "Training Epoch 13  48.7% | batch:       145 of       298\t|\tloss: 0.148261\n",
      "Training Epoch 13  49.0% | batch:       146 of       298\t|\tloss: 0.172769\n",
      "Training Epoch 13  49.3% | batch:       147 of       298\t|\tloss: 0.17097\n",
      "Training Epoch 13  49.7% | batch:       148 of       298\t|\tloss: 1.81841\n",
      "Training Epoch 13  50.0% | batch:       149 of       298\t|\tloss: 0.433053\n",
      "Training Epoch 13  50.3% | batch:       150 of       298\t|\tloss: 0.138661\n",
      "Training Epoch 13  50.7% | batch:       151 of       298\t|\tloss: 0.215902\n",
      "Training Epoch 13  51.0% | batch:       152 of       298\t|\tloss: 0.167544\n",
      "Training Epoch 13  51.3% | batch:       153 of       298\t|\tloss: 0.242439\n",
      "Training Epoch 13  51.7% | batch:       154 of       298\t|\tloss: 0.231264\n",
      "Training Epoch 13  52.0% | batch:       155 of       298\t|\tloss: 0.143898\n",
      "Training Epoch 13  52.3% | batch:       156 of       298\t|\tloss: 0.548873\n",
      "Training Epoch 13  52.7% | batch:       157 of       298\t|\tloss: 0.184913\n",
      "Training Epoch 13  53.0% | batch:       158 of       298\t|\tloss: 0.192439\n",
      "Training Epoch 13  53.4% | batch:       159 of       298\t|\tloss: 0.165065\n",
      "Training Epoch 13  53.7% | batch:       160 of       298\t|\tloss: 0.250753\n",
      "Training Epoch 13  54.0% | batch:       161 of       298\t|\tloss: 0.234454\n",
      "Training Epoch 13  54.4% | batch:       162 of       298\t|\tloss: 0.216955\n",
      "Training Epoch 13  54.7% | batch:       163 of       298\t|\tloss: 0.41231\n",
      "Training Epoch 13  55.0% | batch:       164 of       298\t|\tloss: 0.203912\n",
      "Training Epoch 13  55.4% | batch:       165 of       298\t|\tloss: 0.307853\n",
      "Training Epoch 13  55.7% | batch:       166 of       298\t|\tloss: 0.166709\n",
      "Training Epoch 13  56.0% | batch:       167 of       298\t|\tloss: 0.194322\n",
      "Training Epoch 13  56.4% | batch:       168 of       298\t|\tloss: 0.179124\n",
      "Training Epoch 13  56.7% | batch:       169 of       298\t|\tloss: 0.203988\n",
      "Training Epoch 13  57.0% | batch:       170 of       298\t|\tloss: 0.219383\n",
      "Training Epoch 13  57.4% | batch:       171 of       298\t|\tloss: 0.188543\n",
      "Training Epoch 13  57.7% | batch:       172 of       298\t|\tloss: 0.264045\n",
      "Training Epoch 13  58.1% | batch:       173 of       298\t|\tloss: 0.115485\n",
      "Training Epoch 13  58.4% | batch:       174 of       298\t|\tloss: 0.22082\n",
      "Training Epoch 13  58.7% | batch:       175 of       298\t|\tloss: 0.175798\n",
      "Training Epoch 13  59.1% | batch:       176 of       298\t|\tloss: 0.182519\n",
      "Training Epoch 13  59.4% | batch:       177 of       298\t|\tloss: 0.222553\n",
      "Training Epoch 13  59.7% | batch:       178 of       298\t|\tloss: 0.192611\n",
      "Training Epoch 13  60.1% | batch:       179 of       298\t|\tloss: 0.237277\n",
      "Training Epoch 13  60.4% | batch:       180 of       298\t|\tloss: 0.282842\n",
      "Training Epoch 13  60.7% | batch:       181 of       298\t|\tloss: 0.191941\n",
      "Training Epoch 13  61.1% | batch:       182 of       298\t|\tloss: 0.211114\n",
      "Training Epoch 13  61.4% | batch:       183 of       298\t|\tloss: 0.165534\n",
      "Training Epoch 13  61.7% | batch:       184 of       298\t|\tloss: 0.197681\n",
      "Training Epoch 13  62.1% | batch:       185 of       298\t|\tloss: 0.205733\n",
      "Training Epoch 13  62.4% | batch:       186 of       298\t|\tloss: 0.144475\n",
      "Training Epoch 13  62.8% | batch:       187 of       298\t|\tloss: 0.408708\n",
      "Training Epoch 13  63.1% | batch:       188 of       298\t|\tloss: 0.184187\n",
      "Training Epoch 13  63.4% | batch:       189 of       298\t|\tloss: 0.200672\n",
      "Training Epoch 13  63.8% | batch:       190 of       298\t|\tloss: 0.151168\n",
      "Training Epoch 13  64.1% | batch:       191 of       298\t|\tloss: 0.353472\n",
      "Training Epoch 13  64.4% | batch:       192 of       298\t|\tloss: 0.96892\n",
      "Training Epoch 13  64.8% | batch:       193 of       298\t|\tloss: 0.196999\n",
      "Training Epoch 13  65.1% | batch:       194 of       298\t|\tloss: 0.350367\n",
      "Training Epoch 13  65.4% | batch:       195 of       298\t|\tloss: 0.151202\n",
      "Training Epoch 13  65.8% | batch:       196 of       298\t|\tloss: 0.551913\n",
      "Training Epoch 13  66.1% | batch:       197 of       298\t|\tloss: 0.226999\n",
      "Training Epoch 13  66.4% | batch:       198 of       298\t|\tloss: 0.224145\n",
      "Training Epoch 13  66.8% | batch:       199 of       298\t|\tloss: 0.242553\n",
      "Training Epoch 13  67.1% | batch:       200 of       298\t|\tloss: 0.25394\n",
      "Training Epoch 13  67.4% | batch:       201 of       298\t|\tloss: 0.230854\n",
      "Training Epoch 13  67.8% | batch:       202 of       298\t|\tloss: 0.847806\n",
      "Training Epoch 13  68.1% | batch:       203 of       298\t|\tloss: 0.250189\n",
      "Training Epoch 13  68.5% | batch:       204 of       298\t|\tloss: 0.160914\n",
      "Training Epoch 13  68.8% | batch:       205 of       298\t|\tloss: 1.66868\n",
      "Training Epoch 13  69.1% | batch:       206 of       298\t|\tloss: 0.160849\n",
      "Training Epoch 13  69.5% | batch:       207 of       298\t|\tloss: 0.187291\n",
      "Training Epoch 13  69.8% | batch:       208 of       298\t|\tloss: 0.168605\n",
      "Training Epoch 13  70.1% | batch:       209 of       298\t|\tloss: 2.26954\n",
      "Training Epoch 13  70.5% | batch:       210 of       298\t|\tloss: 0.233602\n",
      "Training Epoch 13  70.8% | batch:       211 of       298\t|\tloss: 0.186538\n",
      "Training Epoch 13  71.1% | batch:       212 of       298\t|\tloss: 0.270335\n",
      "Training Epoch 13  71.5% | batch:       213 of       298\t|\tloss: 0.262894\n",
      "Training Epoch 13  71.8% | batch:       214 of       298\t|\tloss: 0.242754\n",
      "Training Epoch 13  72.1% | batch:       215 of       298\t|\tloss: 0.155805\n",
      "Training Epoch 13  72.5% | batch:       216 of       298\t|\tloss: 0.233242\n",
      "Training Epoch 13  72.8% | batch:       217 of       298\t|\tloss: 0.28454\n",
      "Training Epoch 13  73.2% | batch:       218 of       298\t|\tloss: 0.23986\n",
      "Training Epoch 13  73.5% | batch:       219 of       298\t|\tloss: 0.221542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 13  73.8% | batch:       220 of       298\t|\tloss: 0.191892\n",
      "Training Epoch 13  74.2% | batch:       221 of       298\t|\tloss: 0.687809\n",
      "Training Epoch 13  74.5% | batch:       222 of       298\t|\tloss: 0.332306\n",
      "Training Epoch 13  74.8% | batch:       223 of       298\t|\tloss: 0.238418\n",
      "Training Epoch 13  75.2% | batch:       224 of       298\t|\tloss: 0.198684\n",
      "Training Epoch 13  75.5% | batch:       225 of       298\t|\tloss: 0.202377\n",
      "Training Epoch 13  75.8% | batch:       226 of       298\t|\tloss: 0.162404\n",
      "Training Epoch 13  76.2% | batch:       227 of       298\t|\tloss: 0.220664\n",
      "Training Epoch 13  76.5% | batch:       228 of       298\t|\tloss: 0.210166\n",
      "Training Epoch 13  76.8% | batch:       229 of       298\t|\tloss: 0.203992\n",
      "Training Epoch 13  77.2% | batch:       230 of       298\t|\tloss: 0.41802\n",
      "Training Epoch 13  77.5% | batch:       231 of       298\t|\tloss: 0.175264\n",
      "Training Epoch 13  77.9% | batch:       232 of       298\t|\tloss: 0.224231\n",
      "Training Epoch 13  78.2% | batch:       233 of       298\t|\tloss: 0.143069\n",
      "Training Epoch 13  78.5% | batch:       234 of       298\t|\tloss: 0.175866\n",
      "Training Epoch 13  78.9% | batch:       235 of       298\t|\tloss: 0.167159\n",
      "Training Epoch 13  79.2% | batch:       236 of       298\t|\tloss: 0.176488\n",
      "Training Epoch 13  79.5% | batch:       237 of       298\t|\tloss: 0.369861\n",
      "Training Epoch 13  79.9% | batch:       238 of       298\t|\tloss: 0.263536\n",
      "Training Epoch 13  80.2% | batch:       239 of       298\t|\tloss: 0.169046\n",
      "Training Epoch 13  80.5% | batch:       240 of       298\t|\tloss: 0.301123\n",
      "Training Epoch 13  80.9% | batch:       241 of       298\t|\tloss: 0.22647\n",
      "Training Epoch 13  81.2% | batch:       242 of       298\t|\tloss: 0.232588\n",
      "Training Epoch 13  81.5% | batch:       243 of       298\t|\tloss: 0.200507\n",
      "Training Epoch 13  81.9% | batch:       244 of       298\t|\tloss: 0.222411\n",
      "Training Epoch 13  82.2% | batch:       245 of       298\t|\tloss: 0.187342\n",
      "Training Epoch 13  82.6% | batch:       246 of       298\t|\tloss: 0.188954\n",
      "Training Epoch 13  82.9% | batch:       247 of       298\t|\tloss: 0.23891\n",
      "Training Epoch 13  83.2% | batch:       248 of       298\t|\tloss: 0.172833\n",
      "Training Epoch 13  83.6% | batch:       249 of       298\t|\tloss: 0.180506\n",
      "Training Epoch 13  83.9% | batch:       250 of       298\t|\tloss: 0.224948\n",
      "Training Epoch 13  84.2% | batch:       251 of       298\t|\tloss: 0.175481\n",
      "Training Epoch 13  84.6% | batch:       252 of       298\t|\tloss: 0.156793\n",
      "Training Epoch 13  84.9% | batch:       253 of       298\t|\tloss: 0.127059\n",
      "Training Epoch 13  85.2% | batch:       254 of       298\t|\tloss: 0.172776\n",
      "Training Epoch 13  85.6% | batch:       255 of       298\t|\tloss: 0.293918\n",
      "Training Epoch 13  85.9% | batch:       256 of       298\t|\tloss: 0.246393\n",
      "Training Epoch 13  86.2% | batch:       257 of       298\t|\tloss: 0.185681\n",
      "Training Epoch 13  86.6% | batch:       258 of       298\t|\tloss: 0.203694\n",
      "Training Epoch 13  86.9% | batch:       259 of       298\t|\tloss: 0.276133\n",
      "Training Epoch 13  87.2% | batch:       260 of       298\t|\tloss: 0.182792\n",
      "Training Epoch 13  87.6% | batch:       261 of       298\t|\tloss: 2.20216\n",
      "Training Epoch 13  87.9% | batch:       262 of       298\t|\tloss: 0.255923\n",
      "Training Epoch 13  88.3% | batch:       263 of       298\t|\tloss: 0.177869\n",
      "Training Epoch 13  88.6% | batch:       264 of       298\t|\tloss: 0.52655\n",
      "Training Epoch 13  88.9% | batch:       265 of       298\t|\tloss: 0.216045\n",
      "Training Epoch 13  89.3% | batch:       266 of       298\t|\tloss: 0.203689\n",
      "Training Epoch 13  89.6% | batch:       267 of       298\t|\tloss: 0.23704\n",
      "Training Epoch 13  89.9% | batch:       268 of       298\t|\tloss: 0.116432\n",
      "Training Epoch 13  90.3% | batch:       269 of       298\t|\tloss: 0.22366\n",
      "Training Epoch 13  90.6% | batch:       270 of       298\t|\tloss: 0.151087\n",
      "Training Epoch 13  90.9% | batch:       271 of       298\t|\tloss: 0.2841\n",
      "Training Epoch 13  91.3% | batch:       272 of       298\t|\tloss: 0.206463\n",
      "Training Epoch 13  91.6% | batch:       273 of       298\t|\tloss: 0.159649\n",
      "Training Epoch 13  91.9% | batch:       274 of       298\t|\tloss: 0.159593\n",
      "Training Epoch 13  92.3% | batch:       275 of       298\t|\tloss: 0.239924\n",
      "Training Epoch 13  92.6% | batch:       276 of       298\t|\tloss: 0.217049\n",
      "Training Epoch 13  93.0% | batch:       277 of       298\t|\tloss: 0.55619\n",
      "Training Epoch 13  93.3% | batch:       278 of       298\t|\tloss: 0.226886\n",
      "Training Epoch 13  93.6% | batch:       279 of       298\t|\tloss: 0.187207\n",
      "Training Epoch 13  94.0% | batch:       280 of       298\t|\tloss: 0.20107\n",
      "Training Epoch 13  94.3% | batch:       281 of       298\t|\tloss: 0.282939\n",
      "Training Epoch 13  94.6% | batch:       282 of       298\t|\tloss: 0.157491\n",
      "Training Epoch 13  95.0% | batch:       283 of       298\t|\tloss: 0.523765\n",
      "Training Epoch 13  95.3% | batch:       284 of       298\t|\tloss: 1.01894\n",
      "Training Epoch 13  95.6% | batch:       285 of       298\t|\tloss: 0.169097\n",
      "Training Epoch 13  96.0% | batch:       286 of       298\t|\tloss: 0.169376\n",
      "Training Epoch 13  96.3% | batch:       287 of       298\t|\tloss: 0.19928\n",
      "Training Epoch 13  96.6% | batch:       288 of       298\t|\tloss: 0.240662\n",
      "Training Epoch 13  97.0% | batch:       289 of       298\t|\tloss: 0.148544\n",
      "Training Epoch 13  97.3% | batch:       290 of       298\t|\tloss: 0.204917\n",
      "Training Epoch 13  97.7% | batch:       291 of       298\t|\tloss: 0.169969\n",
      "Training Epoch 13  98.0% | batch:       292 of       298\t|\tloss: 0.542499\n",
      "Training Epoch 13  98.3% | batch:       293 of       298\t|\tloss: 0.19599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:38:50,271 | INFO : Epoch 13 Training Summary: epoch: 13.000000 | loss: 0.269732 | \n",
      "2023-06-22 14:38:50,273 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.100887537002563 seconds\n",
      "\n",
      "2023-06-22 14:38:50,274 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.286097856668325 seconds\n",
      "2023-06-22 14:38:50,275 | INFO : Avg batch train. time: 0.04122851629754472 seconds\n",
      "2023-06-22 14:38:50,276 | INFO : Avg sample train. time: 0.0012886614072444226 seconds\n",
      "Training Epoch:   3%|▎         | 13/400 [02:48<1:22:40, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 13  98.7% | batch:       294 of       298\t|\tloss: 0.173892\n",
      "Training Epoch 13  99.0% | batch:       295 of       298\t|\tloss: 0.268044\n",
      "Training Epoch 13  99.3% | batch:       296 of       298\t|\tloss: 0.256925\n",
      "Training Epoch 13  99.7% | batch:       297 of       298\t|\tloss: 0.284239\n",
      "\n",
      "Training Epoch 14   0.0% | batch:         0 of       298\t|\tloss: 0.171345\n",
      "Training Epoch 14   0.3% | batch:         1 of       298\t|\tloss: 0.682506\n",
      "Training Epoch 14   0.7% | batch:         2 of       298\t|\tloss: 0.164687\n",
      "Training Epoch 14   1.0% | batch:         3 of       298\t|\tloss: 0.541261\n",
      "Training Epoch 14   1.3% | batch:         4 of       298\t|\tloss: 0.24068\n",
      "Training Epoch 14   1.7% | batch:         5 of       298\t|\tloss: 0.164771\n",
      "Training Epoch 14   2.0% | batch:         6 of       298\t|\tloss: 0.236048\n",
      "Training Epoch 14   2.3% | batch:         7 of       298\t|\tloss: 0.150848\n",
      "Training Epoch 14   2.7% | batch:         8 of       298\t|\tloss: 0.194115\n",
      "Training Epoch 14   3.0% | batch:         9 of       298\t|\tloss: 0.252324\n",
      "Training Epoch 14   3.4% | batch:        10 of       298\t|\tloss: 0.221752\n",
      "Training Epoch 14   3.7% | batch:        11 of       298\t|\tloss: 0.22305\n",
      "Training Epoch 14   4.0% | batch:        12 of       298\t|\tloss: 0.209365\n",
      "Training Epoch 14   4.4% | batch:        13 of       298\t|\tloss: 0.530388\n",
      "Training Epoch 14   4.7% | batch:        14 of       298\t|\tloss: 0.157005\n",
      "Training Epoch 14   5.0% | batch:        15 of       298\t|\tloss: 0.197905\n",
      "Training Epoch 14   5.4% | batch:        16 of       298\t|\tloss: 0.218393\n",
      "Training Epoch 14   5.7% | batch:        17 of       298\t|\tloss: 0.181897\n",
      "Training Epoch 14   6.0% | batch:        18 of       298\t|\tloss: 0.255766\n",
      "Training Epoch 14   6.4% | batch:        19 of       298\t|\tloss: 0.272374\n",
      "Training Epoch 14   6.7% | batch:        20 of       298\t|\tloss: 0.445564\n",
      "Training Epoch 14   7.0% | batch:        21 of       298\t|\tloss: 0.212744\n",
      "Training Epoch 14   7.4% | batch:        22 of       298\t|\tloss: 0.170845\n",
      "Training Epoch 14   7.7% | batch:        23 of       298\t|\tloss: 1.97457\n",
      "Training Epoch 14   8.1% | batch:        24 of       298\t|\tloss: 0.187208\n",
      "Training Epoch 14   8.4% | batch:        25 of       298\t|\tloss: 1.11245\n",
      "Training Epoch 14   8.7% | batch:        26 of       298\t|\tloss: 0.188596\n",
      "Training Epoch 14   9.1% | batch:        27 of       298\t|\tloss: 0.238569\n",
      "Training Epoch 14   9.4% | batch:        28 of       298\t|\tloss: 0.225843\n",
      "Training Epoch 14   9.7% | batch:        29 of       298\t|\tloss: 0.251566\n",
      "Training Epoch 14  10.1% | batch:        30 of       298\t|\tloss: 0.216606\n",
      "Training Epoch 14  10.4% | batch:        31 of       298\t|\tloss: 0.194227\n",
      "Training Epoch 14  10.7% | batch:        32 of       298\t|\tloss: 0.192474\n",
      "Training Epoch 14  11.1% | batch:        33 of       298\t|\tloss: 0.337206\n",
      "Training Epoch 14  11.4% | batch:        34 of       298\t|\tloss: 0.197681\n",
      "Training Epoch 14  11.7% | batch:        35 of       298\t|\tloss: 0.276302\n",
      "Training Epoch 14  12.1% | batch:        36 of       298\t|\tloss: 0.210941\n",
      "Training Epoch 14  12.4% | batch:        37 of       298\t|\tloss: 0.207855\n",
      "Training Epoch 14  12.8% | batch:        38 of       298\t|\tloss: 0.234538\n",
      "Training Epoch 14  13.1% | batch:        39 of       298\t|\tloss: 1.40481\n",
      "Training Epoch 14  13.4% | batch:        40 of       298\t|\tloss: 0.280695\n",
      "Training Epoch 14  13.8% | batch:        41 of       298\t|\tloss: 0.299078\n",
      "Training Epoch 14  14.1% | batch:        42 of       298\t|\tloss: 0.255325\n",
      "Training Epoch 14  14.4% | batch:        43 of       298\t|\tloss: 0.157098\n",
      "Training Epoch 14  14.8% | batch:        44 of       298\t|\tloss: 0.216008\n",
      "Training Epoch 14  15.1% | batch:        45 of       298\t|\tloss: 0.235341\n",
      "Training Epoch 14  15.4% | batch:        46 of       298\t|\tloss: 0.23137\n",
      "Training Epoch 14  15.8% | batch:        47 of       298\t|\tloss: 0.263816\n",
      "Training Epoch 14  16.1% | batch:        48 of       298\t|\tloss: 0.157666\n",
      "Training Epoch 14  16.4% | batch:        49 of       298\t|\tloss: 0.285114\n",
      "Training Epoch 14  16.8% | batch:        50 of       298\t|\tloss: 0.223561\n",
      "Training Epoch 14  17.1% | batch:        51 of       298\t|\tloss: 0.316744\n",
      "Training Epoch 14  17.4% | batch:        52 of       298\t|\tloss: 0.84742\n",
      "Training Epoch 14  17.8% | batch:        53 of       298\t|\tloss: 0.188571\n",
      "Training Epoch 14  18.1% | batch:        54 of       298\t|\tloss: 0.164293\n",
      "Training Epoch 14  18.5% | batch:        55 of       298\t|\tloss: 0.258221\n",
      "Training Epoch 14  18.8% | batch:        56 of       298\t|\tloss: 0.252628\n",
      "Training Epoch 14  19.1% | batch:        57 of       298\t|\tloss: 0.162354\n",
      "Training Epoch 14  19.5% | batch:        58 of       298\t|\tloss: 0.670931\n",
      "Training Epoch 14  19.8% | batch:        59 of       298\t|\tloss: 0.329558\n",
      "Training Epoch 14  20.1% | batch:        60 of       298\t|\tloss: 0.291925\n",
      "Training Epoch 14  20.5% | batch:        61 of       298\t|\tloss: 0.166224\n",
      "Training Epoch 14  20.8% | batch:        62 of       298\t|\tloss: 0.186775\n",
      "Training Epoch 14  21.1% | batch:        63 of       298\t|\tloss: 0.173373\n",
      "Training Epoch 14  21.5% | batch:        64 of       298\t|\tloss: 0.178352\n",
      "Training Epoch 14  21.8% | batch:        65 of       298\t|\tloss: 0.238489\n",
      "Training Epoch 14  22.1% | batch:        66 of       298\t|\tloss: 0.332594\n",
      "Training Epoch 14  22.5% | batch:        67 of       298\t|\tloss: 0.343411\n",
      "Training Epoch 14  22.8% | batch:        68 of       298\t|\tloss: 0.158465\n",
      "Training Epoch 14  23.2% | batch:        69 of       298\t|\tloss: 0.280592\n",
      "Training Epoch 14  23.5% | batch:        70 of       298\t|\tloss: 0.190863\n",
      "Training Epoch 14  23.8% | batch:        71 of       298\t|\tloss: 0.193569\n",
      "Training Epoch 14  24.2% | batch:        72 of       298\t|\tloss: 0.157298\n",
      "Training Epoch 14  24.5% | batch:        73 of       298\t|\tloss: 0.153167\n",
      "Training Epoch 14  24.8% | batch:        74 of       298\t|\tloss: 0.14749\n",
      "Training Epoch 14  25.2% | batch:        75 of       298\t|\tloss: 0.188234\n",
      "Training Epoch 14  25.5% | batch:        76 of       298\t|\tloss: 0.208233\n",
      "Training Epoch 14  25.8% | batch:        77 of       298\t|\tloss: 0.245928\n",
      "Training Epoch 14  26.2% | batch:        78 of       298\t|\tloss: 1.41745\n",
      "Training Epoch 14  26.5% | batch:        79 of       298\t|\tloss: 0.181186\n",
      "Training Epoch 14  26.8% | batch:        80 of       298\t|\tloss: 0.191075\n",
      "Training Epoch 14  27.2% | batch:        81 of       298\t|\tloss: 0.244334\n",
      "Training Epoch 14  27.5% | batch:        82 of       298\t|\tloss: 0.243119\n",
      "Training Epoch 14  27.9% | batch:        83 of       298\t|\tloss: 0.188286\n",
      "Training Epoch 14  28.2% | batch:        84 of       298\t|\tloss: 0.583961\n",
      "Training Epoch 14  28.5% | batch:        85 of       298\t|\tloss: 0.141579\n",
      "Training Epoch 14  28.9% | batch:        86 of       298\t|\tloss: 0.224129\n",
      "Training Epoch 14  29.2% | batch:        87 of       298\t|\tloss: 0.204837\n",
      "Training Epoch 14  29.5% | batch:        88 of       298\t|\tloss: 0.155796\n",
      "Training Epoch 14  29.9% | batch:        89 of       298\t|\tloss: 0.214502\n",
      "Training Epoch 14  30.2% | batch:        90 of       298\t|\tloss: 0.13854\n",
      "Training Epoch 14  30.5% | batch:        91 of       298\t|\tloss: 0.315016\n",
      "Training Epoch 14  30.9% | batch:        92 of       298\t|\tloss: 0.165204\n",
      "Training Epoch 14  31.2% | batch:        93 of       298\t|\tloss: 0.156556\n",
      "Training Epoch 14  31.5% | batch:        94 of       298\t|\tloss: 0.185441\n",
      "Training Epoch 14  31.9% | batch:        95 of       298\t|\tloss: 0.2102\n",
      "Training Epoch 14  32.2% | batch:        96 of       298\t|\tloss: 0.178546\n",
      "Training Epoch 14  32.6% | batch:        97 of       298\t|\tloss: 0.158931\n",
      "Training Epoch 14  32.9% | batch:        98 of       298\t|\tloss: 0.232493\n",
      "Training Epoch 14  33.2% | batch:        99 of       298\t|\tloss: 0.291487\n",
      "Training Epoch 14  33.6% | batch:       100 of       298\t|\tloss: 0.202883\n",
      "Training Epoch 14  33.9% | batch:       101 of       298\t|\tloss: 0.182085\n",
      "Training Epoch 14  34.2% | batch:       102 of       298\t|\tloss: 0.18105\n",
      "Training Epoch 14  34.6% | batch:       103 of       298\t|\tloss: 0.169938\n",
      "Training Epoch 14  34.9% | batch:       104 of       298\t|\tloss: 0.164955\n",
      "Training Epoch 14  35.2% | batch:       105 of       298\t|\tloss: 0.204751\n",
      "Training Epoch 14  35.6% | batch:       106 of       298\t|\tloss: 0.164104\n",
      "Training Epoch 14  35.9% | batch:       107 of       298\t|\tloss: 0.417515\n",
      "Training Epoch 14  36.2% | batch:       108 of       298\t|\tloss: 0.571081\n",
      "Training Epoch 14  36.6% | batch:       109 of       298\t|\tloss: 0.208807\n",
      "Training Epoch 14  36.9% | batch:       110 of       298\t|\tloss: 0.181887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 14  37.2% | batch:       111 of       298\t|\tloss: 0.192875\n",
      "Training Epoch 14  37.6% | batch:       112 of       298\t|\tloss: 0.203188\n",
      "Training Epoch 14  37.9% | batch:       113 of       298\t|\tloss: 0.152731\n",
      "Training Epoch 14  38.3% | batch:       114 of       298\t|\tloss: 0.188324\n",
      "Training Epoch 14  38.6% | batch:       115 of       298\t|\tloss: 0.181321\n",
      "Training Epoch 14  38.9% | batch:       116 of       298\t|\tloss: 0.488643\n",
      "Training Epoch 14  39.3% | batch:       117 of       298\t|\tloss: 0.265967\n",
      "Training Epoch 14  39.6% | batch:       118 of       298\t|\tloss: 0.343911\n",
      "Training Epoch 14  39.9% | batch:       119 of       298\t|\tloss: 0.275396\n",
      "Training Epoch 14  40.3% | batch:       120 of       298\t|\tloss: 0.366838\n",
      "Training Epoch 14  40.6% | batch:       121 of       298\t|\tloss: 0.173037\n",
      "Training Epoch 14  40.9% | batch:       122 of       298\t|\tloss: 0.207927\n",
      "Training Epoch 14  41.3% | batch:       123 of       298\t|\tloss: 0.147564\n",
      "Training Epoch 14  41.6% | batch:       124 of       298\t|\tloss: 0.198679\n",
      "Training Epoch 14  41.9% | batch:       125 of       298\t|\tloss: 0.188691\n",
      "Training Epoch 14  42.3% | batch:       126 of       298\t|\tloss: 0.207029\n",
      "Training Epoch 14  42.6% | batch:       127 of       298\t|\tloss: 0.235713\n",
      "Training Epoch 14  43.0% | batch:       128 of       298\t|\tloss: 0.332339\n",
      "Training Epoch 14  43.3% | batch:       129 of       298\t|\tloss: 0.163453\n",
      "Training Epoch 14  43.6% | batch:       130 of       298\t|\tloss: 0.243827\n",
      "Training Epoch 14  44.0% | batch:       131 of       298\t|\tloss: 0.255216\n",
      "Training Epoch 14  44.3% | batch:       132 of       298\t|\tloss: 0.162887\n",
      "Training Epoch 14  44.6% | batch:       133 of       298\t|\tloss: 0.24034\n",
      "Training Epoch 14  45.0% | batch:       134 of       298\t|\tloss: 0.163821\n",
      "Training Epoch 14  45.3% | batch:       135 of       298\t|\tloss: 0.286023\n",
      "Training Epoch 14  45.6% | batch:       136 of       298\t|\tloss: 0.26198\n",
      "Training Epoch 14  46.0% | batch:       137 of       298\t|\tloss: 0.546414\n",
      "Training Epoch 14  46.3% | batch:       138 of       298\t|\tloss: 0.176414\n",
      "Training Epoch 14  46.6% | batch:       139 of       298\t|\tloss: 0.166944\n",
      "Training Epoch 14  47.0% | batch:       140 of       298\t|\tloss: 0.213828\n",
      "Training Epoch 14  47.3% | batch:       141 of       298\t|\tloss: 0.217368\n",
      "Training Epoch 14  47.7% | batch:       142 of       298\t|\tloss: 0.174902\n",
      "Training Epoch 14  48.0% | batch:       143 of       298\t|\tloss: 0.204235\n",
      "Training Epoch 14  48.3% | batch:       144 of       298\t|\tloss: 0.208978\n",
      "Training Epoch 14  48.7% | batch:       145 of       298\t|\tloss: 0.222817\n",
      "Training Epoch 14  49.0% | batch:       146 of       298\t|\tloss: 0.232336\n",
      "Training Epoch 14  49.3% | batch:       147 of       298\t|\tloss: 0.366327\n",
      "Training Epoch 14  49.7% | batch:       148 of       298\t|\tloss: 0.425157\n",
      "Training Epoch 14  50.0% | batch:       149 of       298\t|\tloss: 0.198654\n",
      "Training Epoch 14  50.3% | batch:       150 of       298\t|\tloss: 0.17309\n",
      "Training Epoch 14  50.7% | batch:       151 of       298\t|\tloss: 0.170139\n",
      "Training Epoch 14  51.0% | batch:       152 of       298\t|\tloss: 0.147325\n",
      "Training Epoch 14  51.3% | batch:       153 of       298\t|\tloss: 0.765902\n",
      "Training Epoch 14  51.7% | batch:       154 of       298\t|\tloss: 0.135349\n",
      "Training Epoch 14  52.0% | batch:       155 of       298\t|\tloss: 0.265348\n",
      "Training Epoch 14  52.3% | batch:       156 of       298\t|\tloss: 0.375894\n",
      "Training Epoch 14  52.7% | batch:       157 of       298\t|\tloss: 0.155123\n",
      "Training Epoch 14  53.0% | batch:       158 of       298\t|\tloss: 0.224618\n",
      "Training Epoch 14  53.4% | batch:       159 of       298\t|\tloss: 0.225387\n",
      "Training Epoch 14  53.7% | batch:       160 of       298\t|\tloss: 0.155166\n",
      "Training Epoch 14  54.0% | batch:       161 of       298\t|\tloss: 0.169037\n",
      "Training Epoch 14  54.4% | batch:       162 of       298\t|\tloss: 0.226222\n",
      "Training Epoch 14  54.7% | batch:       163 of       298\t|\tloss: 0.352338\n",
      "Training Epoch 14  55.0% | batch:       164 of       298\t|\tloss: 0.148467\n",
      "Training Epoch 14  55.4% | batch:       165 of       298\t|\tloss: 1.15956\n",
      "Training Epoch 14  55.7% | batch:       166 of       298\t|\tloss: 0.254441\n",
      "Training Epoch 14  56.0% | batch:       167 of       298\t|\tloss: 0.14556\n",
      "Training Epoch 14  56.4% | batch:       168 of       298\t|\tloss: 0.183779\n",
      "Training Epoch 14  56.7% | batch:       169 of       298\t|\tloss: 0.160092\n",
      "Training Epoch 14  57.0% | batch:       170 of       298\t|\tloss: 0.291116\n",
      "Training Epoch 14  57.4% | batch:       171 of       298\t|\tloss: 0.167115\n",
      "Training Epoch 14  57.7% | batch:       172 of       298\t|\tloss: 0.280864\n",
      "Training Epoch 14  58.1% | batch:       173 of       298\t|\tloss: 0.159195\n",
      "Training Epoch 14  58.4% | batch:       174 of       298\t|\tloss: 0.268704\n",
      "Training Epoch 14  58.7% | batch:       175 of       298\t|\tloss: 0.151406\n",
      "Training Epoch 14  59.1% | batch:       176 of       298\t|\tloss: 0.306434\n",
      "Training Epoch 14  59.4% | batch:       177 of       298\t|\tloss: 0.150197\n",
      "Training Epoch 14  59.7% | batch:       178 of       298\t|\tloss: 0.147364\n",
      "Training Epoch 14  60.1% | batch:       179 of       298\t|\tloss: 0.177098\n",
      "Training Epoch 14  60.4% | batch:       180 of       298\t|\tloss: 0.214872\n",
      "Training Epoch 14  60.7% | batch:       181 of       298\t|\tloss: 0.179293\n",
      "Training Epoch 14  61.1% | batch:       182 of       298\t|\tloss: 0.442809\n",
      "Training Epoch 14  61.4% | batch:       183 of       298\t|\tloss: 1.42716\n",
      "Training Epoch 14  61.7% | batch:       184 of       298\t|\tloss: 0.203749\n",
      "Training Epoch 14  62.1% | batch:       185 of       298\t|\tloss: 0.183044\n",
      "Training Epoch 14  62.4% | batch:       186 of       298\t|\tloss: 0.564413\n",
      "Training Epoch 14  62.8% | batch:       187 of       298\t|\tloss: 0.151017\n",
      "Training Epoch 14  63.1% | batch:       188 of       298\t|\tloss: 0.171434\n",
      "Training Epoch 14  63.4% | batch:       189 of       298\t|\tloss: 0.200282\n",
      "Training Epoch 14  63.8% | batch:       190 of       298\t|\tloss: 0.172713\n",
      "Training Epoch 14  64.1% | batch:       191 of       298\t|\tloss: 0.159793\n",
      "Training Epoch 14  64.4% | batch:       192 of       298\t|\tloss: 0.595069\n",
      "Training Epoch 14  64.8% | batch:       193 of       298\t|\tloss: 0.205177\n",
      "Training Epoch 14  65.1% | batch:       194 of       298\t|\tloss: 0.157081\n",
      "Training Epoch 14  65.4% | batch:       195 of       298\t|\tloss: 0.182302\n",
      "Training Epoch 14  65.8% | batch:       196 of       298\t|\tloss: 0.223074\n",
      "Training Epoch 14  66.1% | batch:       197 of       298\t|\tloss: 0.320582\n",
      "Training Epoch 14  66.4% | batch:       198 of       298\t|\tloss: 0.226881\n",
      "Training Epoch 14  66.8% | batch:       199 of       298\t|\tloss: 0.398501\n",
      "Training Epoch 14  67.1% | batch:       200 of       298\t|\tloss: 0.160809\n",
      "Training Epoch 14  67.4% | batch:       201 of       298\t|\tloss: 0.393448\n",
      "Training Epoch 14  67.8% | batch:       202 of       298\t|\tloss: 0.240968\n",
      "Training Epoch 14  68.1% | batch:       203 of       298\t|\tloss: 0.166957\n",
      "Training Epoch 14  68.5% | batch:       204 of       298\t|\tloss: 0.263415\n",
      "Training Epoch 14  68.8% | batch:       205 of       298\t|\tloss: 0.355602\n",
      "Training Epoch 14  69.1% | batch:       206 of       298\t|\tloss: 4.16797\n",
      "Training Epoch 14  69.5% | batch:       207 of       298\t|\tloss: 0.187523\n",
      "Training Epoch 14  69.8% | batch:       208 of       298\t|\tloss: 0.535384\n",
      "Training Epoch 14  70.1% | batch:       209 of       298\t|\tloss: 0.214186\n",
      "Training Epoch 14  70.5% | batch:       210 of       298\t|\tloss: 1.22242\n",
      "Training Epoch 14  70.8% | batch:       211 of       298\t|\tloss: 0.53916\n",
      "Training Epoch 14  71.1% | batch:       212 of       298\t|\tloss: 0.210601\n",
      "Training Epoch 14  71.5% | batch:       213 of       298\t|\tloss: 0.205198\n",
      "Training Epoch 14  71.8% | batch:       214 of       298\t|\tloss: 0.237817\n",
      "Training Epoch 14  72.1% | batch:       215 of       298\t|\tloss: 0.30497\n",
      "Training Epoch 14  72.5% | batch:       216 of       298\t|\tloss: 0.210427\n",
      "Training Epoch 14  72.8% | batch:       217 of       298\t|\tloss: 0.533227\n",
      "Training Epoch 14  73.2% | batch:       218 of       298\t|\tloss: 0.225701\n",
      "Training Epoch 14  73.5% | batch:       219 of       298\t|\tloss: 0.21968\n",
      "Training Epoch 14  73.8% | batch:       220 of       298\t|\tloss: 0.197357\n",
      "Training Epoch 14  74.2% | batch:       221 of       298\t|\tloss: 0.178576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 14  74.5% | batch:       222 of       298\t|\tloss: 0.27806\n",
      "Training Epoch 14  74.8% | batch:       223 of       298\t|\tloss: 0.216972\n",
      "Training Epoch 14  75.2% | batch:       224 of       298\t|\tloss: 0.311236\n",
      "Training Epoch 14  75.5% | batch:       225 of       298\t|\tloss: 0.193385\n",
      "Training Epoch 14  75.8% | batch:       226 of       298\t|\tloss: 0.233595\n",
      "Training Epoch 14  76.2% | batch:       227 of       298\t|\tloss: 0.180029\n",
      "Training Epoch 14  76.5% | batch:       228 of       298\t|\tloss: 0.25216\n",
      "Training Epoch 14  76.8% | batch:       229 of       298\t|\tloss: 0.190619\n",
      "Training Epoch 14  77.2% | batch:       230 of       298\t|\tloss: 0.265056\n",
      "Training Epoch 14  77.5% | batch:       231 of       298\t|\tloss: 0.183717\n",
      "Training Epoch 14  77.9% | batch:       232 of       298\t|\tloss: 0.251198\n",
      "Training Epoch 14  78.2% | batch:       233 of       298\t|\tloss: 0.331616\n",
      "Training Epoch 14  78.5% | batch:       234 of       298\t|\tloss: 0.490015\n",
      "Training Epoch 14  78.9% | batch:       235 of       298\t|\tloss: 0.191184\n",
      "Training Epoch 14  79.2% | batch:       236 of       298\t|\tloss: 0.242732\n",
      "Training Epoch 14  79.5% | batch:       237 of       298\t|\tloss: 0.212591\n",
      "Training Epoch 14  79.9% | batch:       238 of       298\t|\tloss: 0.180855\n",
      "Training Epoch 14  80.2% | batch:       239 of       298\t|\tloss: 0.223228\n",
      "Training Epoch 14  80.5% | batch:       240 of       298\t|\tloss: 0.235436\n",
      "Training Epoch 14  80.9% | batch:       241 of       298\t|\tloss: 0.175796\n",
      "Training Epoch 14  81.2% | batch:       242 of       298\t|\tloss: 0.223415\n",
      "Training Epoch 14  81.5% | batch:       243 of       298\t|\tloss: 0.569375\n",
      "Training Epoch 14  81.9% | batch:       244 of       298\t|\tloss: 0.201527\n",
      "Training Epoch 14  82.2% | batch:       245 of       298\t|\tloss: 0.204581\n",
      "Training Epoch 14  82.6% | batch:       246 of       298\t|\tloss: 0.255975\n",
      "Training Epoch 14  82.9% | batch:       247 of       298\t|\tloss: 0.145161\n",
      "Training Epoch 14  83.2% | batch:       248 of       298\t|\tloss: 0.207462\n",
      "Training Epoch 14  83.6% | batch:       249 of       298\t|\tloss: 0.371867\n",
      "Training Epoch 14  83.9% | batch:       250 of       298\t|\tloss: 2.18743\n",
      "Training Epoch 14  84.2% | batch:       251 of       298\t|\tloss: 0.203701\n",
      "Training Epoch 14  84.6% | batch:       252 of       298\t|\tloss: 0.186149\n",
      "Training Epoch 14  84.9% | batch:       253 of       298\t|\tloss: 0.227305\n",
      "Training Epoch 14  85.2% | batch:       254 of       298\t|\tloss: 0.40539\n",
      "Training Epoch 14  85.6% | batch:       255 of       298\t|\tloss: 0.186893\n",
      "Training Epoch 14  85.9% | batch:       256 of       298\t|\tloss: 1.70693\n",
      "Training Epoch 14  86.2% | batch:       257 of       298\t|\tloss: 0.245191\n",
      "Training Epoch 14  86.6% | batch:       258 of       298\t|\tloss: 0.184636\n",
      "Training Epoch 14  86.9% | batch:       259 of       298\t|\tloss: 0.188662\n",
      "Training Epoch 14  87.2% | batch:       260 of       298\t|\tloss: 0.213574\n",
      "Training Epoch 14  87.6% | batch:       261 of       298\t|\tloss: 0.241909\n",
      "Training Epoch 14  87.9% | batch:       262 of       298\t|\tloss: 0.219665\n",
      "Training Epoch 14  88.3% | batch:       263 of       298\t|\tloss: 0.185448\n",
      "Training Epoch 14  88.6% | batch:       264 of       298\t|\tloss: 0.183295\n",
      "Training Epoch 14  88.9% | batch:       265 of       298\t|\tloss: 0.180152\n",
      "Training Epoch 14  89.3% | batch:       266 of       298\t|\tloss: 0.164733\n",
      "Training Epoch 14  89.6% | batch:       267 of       298\t|\tloss: 0.93177\n",
      "Training Epoch 14  89.9% | batch:       268 of       298\t|\tloss: 0.23781\n",
      "Training Epoch 14  90.3% | batch:       269 of       298\t|\tloss: 0.184768\n",
      "Training Epoch 14  90.6% | batch:       270 of       298\t|\tloss: 0.156051\n",
      "Training Epoch 14  90.9% | batch:       271 of       298\t|\tloss: 0.363251\n",
      "Training Epoch 14  91.3% | batch:       272 of       298\t|\tloss: 0.212082\n",
      "Training Epoch 14  91.6% | batch:       273 of       298\t|\tloss: 0.196473\n",
      "Training Epoch 14  91.9% | batch:       274 of       298\t|\tloss: 0.196643\n",
      "Training Epoch 14  92.3% | batch:       275 of       298\t|\tloss: 0.181225\n",
      "Training Epoch 14  92.6% | batch:       276 of       298\t|\tloss: 0.166503\n",
      "Training Epoch 14  93.0% | batch:       277 of       298\t|\tloss: 0.142156\n",
      "Training Epoch 14  93.3% | batch:       278 of       298\t|\tloss: 0.152003\n",
      "Training Epoch 14  93.6% | batch:       279 of       298\t|\tloss: 0.227956\n",
      "Training Epoch 14  94.0% | batch:       280 of       298\t|\tloss: 0.280843\n",
      "Training Epoch 14  94.3% | batch:       281 of       298\t|\tloss: 0.203239\n",
      "Training Epoch 14  94.6% | batch:       282 of       298\t|\tloss: 0.19113\n",
      "Training Epoch 14  95.0% | batch:       283 of       298\t|\tloss: 0.14113\n",
      "Training Epoch 14  95.3% | batch:       284 of       298\t|\tloss: 0.205179\n",
      "Training Epoch 14  95.6% | batch:       285 of       298\t|\tloss: 0.218756\n",
      "Training Epoch 14  96.0% | batch:       286 of       298\t|\tloss: 0.13499\n",
      "Training Epoch 14  96.3% | batch:       287 of       298\t|\tloss: 0.155903\n",
      "Training Epoch 14  96.6% | batch:       288 of       298\t|\tloss: 0.193404\n",
      "Training Epoch 14  97.0% | batch:       289 of       298\t|\tloss: 0.1666\n",
      "Training Epoch 14  97.3% | batch:       290 of       298\t|\tloss: 0.502217\n",
      "Training Epoch 14  97.7% | batch:       291 of       298\t|\tloss: 0.182019\n",
      "Training Epoch 14  98.0% | batch:       292 of       298\t|\tloss: 0.251877\n",
      "Training Epoch 14  98.3% | batch:       293 of       298\t|\tloss: 0.191216\n",
      "Training Epoch 14  98.7% | batch:       294 of       298\t|\tloss: 0.216721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:39:02,450 | INFO : Epoch 14 Training Summary: epoch: 14.000000 | loss: 0.297982 | \n",
      "2023-06-22 14:39:02,452 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.134496688842773 seconds\n",
      "\n",
      "2023-06-22 14:39:02,452 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.275269201823644 seconds\n",
      "2023-06-22 14:39:02,453 | INFO : Avg batch train. time: 0.04119217852960954 seconds\n",
      "2023-06-22 14:39:02,453 | INFO : Avg sample train. time: 0.001287525613784733 seconds\n",
      "2023-06-22 14:39:02,454 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 14  99.0% | batch:       295 of       298\t|\tloss: 0.398719\n",
      "Training Epoch 14  99.3% | batch:       296 of       298\t|\tloss: 0.378947\n",
      "Training Epoch 14  99.7% | batch:       297 of       298\t|\tloss: 0.172578\n",
      "\n",
      "Evaluating Epoch 14   0.0% | batch:         0 of        75\t|\tloss: 0.228403\n",
      "Evaluating Epoch 14   1.3% | batch:         1 of        75\t|\tloss: 0.218066\n",
      "Evaluating Epoch 14   2.7% | batch:         2 of        75\t|\tloss: 0.141804\n",
      "Evaluating Epoch 14   4.0% | batch:         3 of        75\t|\tloss: 0.140044\n",
      "Evaluating Epoch 14   5.3% | batch:         4 of        75\t|\tloss: 1.36802\n",
      "Evaluating Epoch 14   6.7% | batch:         5 of        75\t|\tloss: 0.151313\n",
      "Evaluating Epoch 14   8.0% | batch:         6 of        75\t|\tloss: 0.168725\n",
      "Evaluating Epoch 14   9.3% | batch:         7 of        75\t|\tloss: 0.137639\n",
      "Evaluating Epoch 14  10.7% | batch:         8 of        75\t|\tloss: 0.212806\n",
      "Evaluating Epoch 14  12.0% | batch:         9 of        75\t|\tloss: 0.226685\n",
      "Evaluating Epoch 14  13.3% | batch:        10 of        75\t|\tloss: 0.178462\n",
      "Evaluating Epoch 14  14.7% | batch:        11 of        75\t|\tloss: 0.136135\n",
      "Evaluating Epoch 14  16.0% | batch:        12 of        75\t|\tloss: 0.525235\n",
      "Evaluating Epoch 14  17.3% | batch:        13 of        75\t|\tloss: 0.264082\n",
      "Evaluating Epoch 14  18.7% | batch:        14 of        75\t|\tloss: 0.166241\n",
      "Evaluating Epoch 14  20.0% | batch:        15 of        75\t|\tloss: 2.08074\n",
      "Evaluating Epoch 14  21.3% | batch:        16 of        75\t|\tloss: 0.118053\n",
      "Evaluating Epoch 14  22.7% | batch:        17 of        75\t|\tloss: 0.180274\n",
      "Evaluating Epoch 14  24.0% | batch:        18 of        75\t|\tloss: 0.306329\n",
      "Evaluating Epoch 14  25.3% | batch:        19 of        75\t|\tloss: 0.504959\n",
      "Evaluating Epoch 14  26.7% | batch:        20 of        75\t|\tloss: 0.16677\n",
      "Evaluating Epoch 14  28.0% | batch:        21 of        75\t|\tloss: 0.173908\n",
      "Evaluating Epoch 14  29.3% | batch:        22 of        75\t|\tloss: 0.204676\n",
      "Evaluating Epoch 14  30.7% | batch:        23 of        75\t|\tloss: 0.146894\n",
      "Evaluating Epoch 14  32.0% | batch:        24 of        75\t|\tloss: 0.144071\n",
      "Evaluating Epoch 14  33.3% | batch:        25 of        75\t|\tloss: 0.255479\n",
      "Evaluating Epoch 14  34.7% | batch:        26 of        75\t|\tloss: 0.146705\n",
      "Evaluating Epoch 14  36.0% | batch:        27 of        75\t|\tloss: 0.246256\n",
      "Evaluating Epoch 14  37.3% | batch:        28 of        75\t|\tloss: 0.139938\n",
      "Evaluating Epoch 14  38.7% | batch:        29 of        75\t|\tloss: 0.164682\n",
      "Evaluating Epoch 14  40.0% | batch:        30 of        75\t|\tloss: 0.230059\n",
      "Evaluating Epoch 14  41.3% | batch:        31 of        75\t|\tloss: 0.172521\n",
      "Evaluating Epoch 14  42.7% | batch:        32 of        75\t|\tloss: 0.198813\n",
      "Evaluating Epoch 14  44.0% | batch:        33 of        75\t|\tloss: 0.870055\n",
      "Evaluating Epoch 14  45.3% | batch:        34 of        75\t|\tloss: 0.169072\n",
      "Evaluating Epoch 14  46.7% | batch:        35 of        75\t|\tloss: 0.473153\n",
      "Evaluating Epoch 14  48.0% | batch:        36 of        75\t|\tloss: 0.157116\n",
      "Evaluating Epoch 14  49.3% | batch:        37 of        75\t|\tloss: 1.14894\n",
      "Evaluating Epoch 14  50.7% | batch:        38 of        75\t|\tloss: 0.235211\n",
      "Evaluating Epoch 14  52.0% | batch:        39 of        75\t|\tloss: 0.592664\n",
      "Evaluating Epoch 14  53.3% | batch:        40 of        75\t|\tloss: 0.248102\n",
      "Evaluating Epoch 14  54.7% | batch:        41 of        75\t|\tloss: 0.109889\n",
      "Evaluating Epoch 14  56.0% | batch:        42 of        75\t|\tloss: 0.155186\n",
      "Evaluating Epoch 14  57.3% | batch:        43 of        75\t|\tloss: 0.136363\n",
      "Evaluating Epoch 14  58.7% | batch:        44 of        75\t|\tloss: 0.16707\n",
      "Evaluating Epoch 14  60.0% | batch:        45 of        75\t|\tloss: 0.148423\n",
      "Evaluating Epoch 14  61.3% | batch:        46 of        75\t|\tloss: 0.167587\n",
      "Evaluating Epoch 14  62.7% | batch:        47 of        75\t|\tloss: 0.770614\n",
      "Evaluating Epoch 14  64.0% | batch:        48 of        75\t|\tloss: 0.159461\n",
      "Evaluating Epoch 14  65.3% | batch:        49 of        75\t|\tloss: 0.194311\n",
      "Evaluating Epoch 14  66.7% | batch:        50 of        75\t|\tloss: 0.813286\n",
      "Evaluating Epoch 14  68.0% | batch:        51 of        75\t|\tloss: 0.138965\n",
      "Evaluating Epoch 14  69.3% | batch:        52 of        75\t|\tloss: 0.169\n",
      "Evaluating Epoch 14  70.7% | batch:        53 of        75\t|\tloss: 0.129902\n",
      "Evaluating Epoch 14  72.0% | batch:        54 of        75\t|\tloss: 0.23412\n",
      "Evaluating Epoch 14  73.3% | batch:        55 of        75\t|\tloss: 0.182552\n",
      "Evaluating Epoch 14  74.7% | batch:        56 of        75\t|\tloss: 0.141629\n",
      "Evaluating Epoch 14  76.0% | batch:        57 of        75\t|\tloss: 0.155589\n",
      "Evaluating Epoch 14  77.3% | batch:        58 of        75\t|\tloss: 0.243029\n",
      "Evaluating Epoch 14  78.7% | batch:        59 of        75\t|\tloss: 0.118198\n",
      "Evaluating Epoch 14  80.0% | batch:        60 of        75\t|\tloss: 0.179775\n",
      "Evaluating Epoch 14  81.3% | batch:        61 of        75\t|\tloss: 0.144062\n",
      "Evaluating Epoch 14  82.7% | batch:        62 of        75\t|\tloss: 0.152679\n",
      "Evaluating Epoch 14  84.0% | batch:        63 of        75\t|\tloss: 0.195655\n",
      "Evaluating Epoch 14  85.3% | batch:        64 of        75\t|\tloss: 0.185653\n",
      "Evaluating Epoch 14  86.7% | batch:        65 of        75\t|\tloss: 0.127364\n",
      "Evaluating Epoch 14  88.0% | batch:        66 of        75\t|\tloss: 0.194225\n",
      "Evaluating Epoch 14  89.3% | batch:        67 of        75\t|\tloss: 0.620331\n",
      "Evaluating Epoch 14  90.7% | batch:        68 of        75\t|\tloss: 0.183383\n",
      "Evaluating Epoch 14  92.0% | batch:        69 of        75\t|\tloss: 0.154626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:39:03,664 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 1.2096996307373047 seconds\n",
      "\n",
      "2023-06-22 14:39:03,665 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 1.2217681672837999 seconds\n",
      "2023-06-22 14:39:03,666 | INFO : Avg batch val. time: 0.016290242230450665 seconds\n",
      "2023-06-22 14:39:03,666 | INFO : Avg sample val. time: 0.0005124866473505872 seconds\n",
      "2023-06-22 14:39:03,667 | INFO : Epoch 14 Validation Summary: epoch: 14.000000 | loss: 0.280626 | \n",
      "Training Epoch:   4%|▎         | 14/400 [03:02<1:23:34, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 14  93.3% | batch:        70 of        75\t|\tloss: 0.149805\n",
      "Evaluating Epoch 14  94.7% | batch:        71 of        75\t|\tloss: 0.147547\n",
      "Evaluating Epoch 14  96.0% | batch:        72 of        75\t|\tloss: 0.104209\n",
      "Evaluating Epoch 14  97.3% | batch:        73 of        75\t|\tloss: 0.169496\n",
      "Evaluating Epoch 14  98.7% | batch:        74 of        75\t|\tloss: 0.262936\n",
      "\n",
      "Training Epoch 15   0.0% | batch:         0 of       298\t|\tloss: 0.245627\n",
      "Training Epoch 15   0.3% | batch:         1 of       298\t|\tloss: 0.250631\n",
      "Training Epoch 15   0.7% | batch:         2 of       298\t|\tloss: 0.152307\n",
      "Training Epoch 15   1.0% | batch:         3 of       298\t|\tloss: 0.206389\n",
      "Training Epoch 15   1.3% | batch:         4 of       298\t|\tloss: 0.22924\n",
      "Training Epoch 15   1.7% | batch:         5 of       298\t|\tloss: 0.161285\n",
      "Training Epoch 15   2.0% | batch:         6 of       298\t|\tloss: 0.363861\n",
      "Training Epoch 15   2.3% | batch:         7 of       298\t|\tloss: 0.654482\n",
      "Training Epoch 15   2.7% | batch:         8 of       298\t|\tloss: 0.198475\n",
      "Training Epoch 15   3.0% | batch:         9 of       298\t|\tloss: 0.163558\n",
      "Training Epoch 15   3.4% | batch:        10 of       298\t|\tloss: 0.543596\n",
      "Training Epoch 15   3.7% | batch:        11 of       298\t|\tloss: 0.20683\n",
      "Training Epoch 15   4.0% | batch:        12 of       298\t|\tloss: 0.252411\n",
      "Training Epoch 15   4.4% | batch:        13 of       298\t|\tloss: 0.240561\n",
      "Training Epoch 15   4.7% | batch:        14 of       298\t|\tloss: 0.138999\n",
      "Training Epoch 15   5.0% | batch:        15 of       298\t|\tloss: 0.186185\n",
      "Training Epoch 15   5.4% | batch:        16 of       298\t|\tloss: 0.273116\n",
      "Training Epoch 15   5.7% | batch:        17 of       298\t|\tloss: 0.160405\n",
      "Training Epoch 15   6.0% | batch:        18 of       298\t|\tloss: 0.191133\n",
      "Training Epoch 15   6.4% | batch:        19 of       298\t|\tloss: 0.610989\n",
      "Training Epoch 15   6.7% | batch:        20 of       298\t|\tloss: 0.403913\n",
      "Training Epoch 15   7.0% | batch:        21 of       298\t|\tloss: 0.222735\n",
      "Training Epoch 15   7.4% | batch:        22 of       298\t|\tloss: 0.162991\n",
      "Training Epoch 15   7.7% | batch:        23 of       298\t|\tloss: 0.227869\n",
      "Training Epoch 15   8.1% | batch:        24 of       298\t|\tloss: 0.224656\n",
      "Training Epoch 15   8.4% | batch:        25 of       298\t|\tloss: 0.154098\n",
      "Training Epoch 15   8.7% | batch:        26 of       298\t|\tloss: 0.203039\n",
      "Training Epoch 15   9.1% | batch:        27 of       298\t|\tloss: 0.149705\n",
      "Training Epoch 15   9.4% | batch:        28 of       298\t|\tloss: 0.236028\n",
      "Training Epoch 15   9.7% | batch:        29 of       298\t|\tloss: 0.190425\n",
      "Training Epoch 15  10.1% | batch:        30 of       298\t|\tloss: 0.17205\n",
      "Training Epoch 15  10.4% | batch:        31 of       298\t|\tloss: 0.218194\n",
      "Training Epoch 15  10.7% | batch:        32 of       298\t|\tloss: 0.207239\n",
      "Training Epoch 15  11.1% | batch:        33 of       298\t|\tloss: 0.198139\n",
      "Training Epoch 15  11.4% | batch:        34 of       298\t|\tloss: 0.19045\n",
      "Training Epoch 15  11.7% | batch:        35 of       298\t|\tloss: 0.209797\n",
      "Training Epoch 15  12.1% | batch:        36 of       298\t|\tloss: 0.248875\n",
      "Training Epoch 15  12.4% | batch:        37 of       298\t|\tloss: 0.161302\n",
      "Training Epoch 15  12.8% | batch:        38 of       298\t|\tloss: 1.21333\n",
      "Training Epoch 15  13.1% | batch:        39 of       298\t|\tloss: 0.193647\n",
      "Training Epoch 15  13.4% | batch:        40 of       298\t|\tloss: 0.171416\n",
      "Training Epoch 15  13.8% | batch:        41 of       298\t|\tloss: 0.169607\n",
      "Training Epoch 15  14.1% | batch:        42 of       298\t|\tloss: 0.135366\n",
      "Training Epoch 15  14.4% | batch:        43 of       298\t|\tloss: 0.254627\n",
      "Training Epoch 15  14.8% | batch:        44 of       298\t|\tloss: 0.153254\n",
      "Training Epoch 15  15.1% | batch:        45 of       298\t|\tloss: 0.148252\n",
      "Training Epoch 15  15.4% | batch:        46 of       298\t|\tloss: 0.208231\n",
      "Training Epoch 15  15.8% | batch:        47 of       298\t|\tloss: 0.224777\n",
      "Training Epoch 15  16.1% | batch:        48 of       298\t|\tloss: 1.98582\n",
      "Training Epoch 15  16.4% | batch:        49 of       298\t|\tloss: 0.129732\n",
      "Training Epoch 15  16.8% | batch:        50 of       298\t|\tloss: 0.404373\n",
      "Training Epoch 15  17.1% | batch:        51 of       298\t|\tloss: 0.260219\n",
      "Training Epoch 15  17.4% | batch:        52 of       298\t|\tloss: 0.165962\n",
      "Training Epoch 15  17.8% | batch:        53 of       298\t|\tloss: 0.157332\n",
      "Training Epoch 15  18.1% | batch:        54 of       298\t|\tloss: 0.187567\n",
      "Training Epoch 15  18.5% | batch:        55 of       298\t|\tloss: 0.121791\n",
      "Training Epoch 15  18.8% | batch:        56 of       298\t|\tloss: 0.229919\n",
      "Training Epoch 15  19.1% | batch:        57 of       298\t|\tloss: 0.153876\n",
      "Training Epoch 15  19.5% | batch:        58 of       298\t|\tloss: 0.204759\n",
      "Training Epoch 15  19.8% | batch:        59 of       298\t|\tloss: 0.143913\n",
      "Training Epoch 15  20.1% | batch:        60 of       298\t|\tloss: 0.1329\n",
      "Training Epoch 15  20.5% | batch:        61 of       298\t|\tloss: 0.204969\n",
      "Training Epoch 15  20.8% | batch:        62 of       298\t|\tloss: 0.604232\n",
      "Training Epoch 15  21.1% | batch:        63 of       298\t|\tloss: 0.147468\n",
      "Training Epoch 15  21.5% | batch:        64 of       298\t|\tloss: 0.466764\n",
      "Training Epoch 15  21.8% | batch:        65 of       298\t|\tloss: 0.153516\n",
      "Training Epoch 15  22.1% | batch:        66 of       298\t|\tloss: 0.212242\n",
      "Training Epoch 15  22.5% | batch:        67 of       298\t|\tloss: 0.201319\n",
      "Training Epoch 15  22.8% | batch:        68 of       298\t|\tloss: 0.200667\n",
      "Training Epoch 15  23.2% | batch:        69 of       298\t|\tloss: 0.296477\n",
      "Training Epoch 15  23.5% | batch:        70 of       298\t|\tloss: 0.168645\n",
      "Training Epoch 15  23.8% | batch:        71 of       298\t|\tloss: 0.345356\n",
      "Training Epoch 15  24.2% | batch:        72 of       298\t|\tloss: 0.238883\n",
      "Training Epoch 15  24.5% | batch:        73 of       298\t|\tloss: 0.163498\n",
      "Training Epoch 15  24.8% | batch:        74 of       298\t|\tloss: 0.18533\n",
      "Training Epoch 15  25.2% | batch:        75 of       298\t|\tloss: 0.215808\n",
      "Training Epoch 15  25.5% | batch:        76 of       298\t|\tloss: 0.247468\n",
      "Training Epoch 15  25.8% | batch:        77 of       298\t|\tloss: 0.168843\n",
      "Training Epoch 15  26.2% | batch:        78 of       298\t|\tloss: 0.233514\n",
      "Training Epoch 15  26.5% | batch:        79 of       298\t|\tloss: 0.322511\n",
      "Training Epoch 15  26.8% | batch:        80 of       298\t|\tloss: 0.238631\n",
      "Training Epoch 15  27.2% | batch:        81 of       298\t|\tloss: 0.139604\n",
      "Training Epoch 15  27.5% | batch:        82 of       298\t|\tloss: 0.183658\n",
      "Training Epoch 15  27.9% | batch:        83 of       298\t|\tloss: 0.289573\n",
      "Training Epoch 15  28.2% | batch:        84 of       298\t|\tloss: 3.02293\n",
      "Training Epoch 15  28.5% | batch:        85 of       298\t|\tloss: 0.149201\n",
      "Training Epoch 15  28.9% | batch:        86 of       298\t|\tloss: 0.185836\n",
      "Training Epoch 15  29.2% | batch:        87 of       298\t|\tloss: 0.134585\n",
      "Training Epoch 15  29.5% | batch:        88 of       298\t|\tloss: 0.258228\n",
      "Training Epoch 15  29.9% | batch:        89 of       298\t|\tloss: 0.16646\n",
      "Training Epoch 15  30.2% | batch:        90 of       298\t|\tloss: 0.163231\n",
      "Training Epoch 15  30.5% | batch:        91 of       298\t|\tloss: 0.256672\n",
      "Training Epoch 15  30.9% | batch:        92 of       298\t|\tloss: 0.196827\n",
      "Training Epoch 15  31.2% | batch:        93 of       298\t|\tloss: 0.277441\n",
      "Training Epoch 15  31.5% | batch:        94 of       298\t|\tloss: 0.193381\n",
      "Training Epoch 15  31.9% | batch:        95 of       298\t|\tloss: 0.247475\n",
      "Training Epoch 15  32.2% | batch:        96 of       298\t|\tloss: 0.184858\n",
      "Training Epoch 15  32.6% | batch:        97 of       298\t|\tloss: 0.217031\n",
      "Training Epoch 15  32.9% | batch:        98 of       298\t|\tloss: 0.226836\n",
      "Training Epoch 15  33.2% | batch:        99 of       298\t|\tloss: 0.17817\n",
      "Training Epoch 15  33.6% | batch:       100 of       298\t|\tloss: 0.172186\n",
      "Training Epoch 15  33.9% | batch:       101 of       298\t|\tloss: 0.241976\n",
      "Training Epoch 15  34.2% | batch:       102 of       298\t|\tloss: 0.473753\n",
      "Training Epoch 15  34.6% | batch:       103 of       298\t|\tloss: 0.186744\n",
      "Training Epoch 15  34.9% | batch:       104 of       298\t|\tloss: 0.2271\n",
      "Training Epoch 15  35.2% | batch:       105 of       298\t|\tloss: 0.522744\n",
      "Training Epoch 15  35.6% | batch:       106 of       298\t|\tloss: 0.249079\n",
      "Training Epoch 15  35.9% | batch:       107 of       298\t|\tloss: 0.239278\n",
      "Training Epoch 15  36.2% | batch:       108 of       298\t|\tloss: 0.289155\n",
      "Training Epoch 15  36.6% | batch:       109 of       298\t|\tloss: 0.279611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 15  36.9% | batch:       110 of       298\t|\tloss: 0.168155\n",
      "Training Epoch 15  37.2% | batch:       111 of       298\t|\tloss: 0.186602\n",
      "Training Epoch 15  37.6% | batch:       112 of       298\t|\tloss: 0.174721\n",
      "Training Epoch 15  37.9% | batch:       113 of       298\t|\tloss: 0.25965\n",
      "Training Epoch 15  38.3% | batch:       114 of       298\t|\tloss: 0.173999\n",
      "Training Epoch 15  38.6% | batch:       115 of       298\t|\tloss: 0.253742\n",
      "Training Epoch 15  38.9% | batch:       116 of       298\t|\tloss: 0.158119\n",
      "Training Epoch 15  39.3% | batch:       117 of       298\t|\tloss: 0.15712\n",
      "Training Epoch 15  39.6% | batch:       118 of       298\t|\tloss: 0.167584\n",
      "Training Epoch 15  39.9% | batch:       119 of       298\t|\tloss: 0.201689\n",
      "Training Epoch 15  40.3% | batch:       120 of       298\t|\tloss: 0.270078\n",
      "Training Epoch 15  40.6% | batch:       121 of       298\t|\tloss: 0.278463\n",
      "Training Epoch 15  40.9% | batch:       122 of       298\t|\tloss: 0.231957\n",
      "Training Epoch 15  41.3% | batch:       123 of       298\t|\tloss: 0.130351\n",
      "Training Epoch 15  41.6% | batch:       124 of       298\t|\tloss: 0.209581\n",
      "Training Epoch 15  41.9% | batch:       125 of       298\t|\tloss: 0.155079\n",
      "Training Epoch 15  42.3% | batch:       126 of       298\t|\tloss: 0.228846\n",
      "Training Epoch 15  42.6% | batch:       127 of       298\t|\tloss: 0.239239\n",
      "Training Epoch 15  43.0% | batch:       128 of       298\t|\tloss: 0.249896\n",
      "Training Epoch 15  43.3% | batch:       129 of       298\t|\tloss: 0.185276\n",
      "Training Epoch 15  43.6% | batch:       130 of       298\t|\tloss: 0.16402\n",
      "Training Epoch 15  44.0% | batch:       131 of       298\t|\tloss: 0.16333\n",
      "Training Epoch 15  44.3% | batch:       132 of       298\t|\tloss: 0.215127\n",
      "Training Epoch 15  44.6% | batch:       133 of       298\t|\tloss: 0.129239\n",
      "Training Epoch 15  45.0% | batch:       134 of       298\t|\tloss: 1.24339\n",
      "Training Epoch 15  45.3% | batch:       135 of       298\t|\tloss: 0.178281\n",
      "Training Epoch 15  45.6% | batch:       136 of       298\t|\tloss: 0.213135\n",
      "Training Epoch 15  46.0% | batch:       137 of       298\t|\tloss: 0.293186\n",
      "Training Epoch 15  46.3% | batch:       138 of       298\t|\tloss: 0.360032\n",
      "Training Epoch 15  46.6% | batch:       139 of       298\t|\tloss: 0.20673\n",
      "Training Epoch 15  47.0% | batch:       140 of       298\t|\tloss: 0.243826\n",
      "Training Epoch 15  47.3% | batch:       141 of       298\t|\tloss: 0.208262\n",
      "Training Epoch 15  47.7% | batch:       142 of       298\t|\tloss: 0.178751\n",
      "Training Epoch 15  48.0% | batch:       143 of       298\t|\tloss: 0.321528\n",
      "Training Epoch 15  48.3% | batch:       144 of       298\t|\tloss: 0.229226\n",
      "Training Epoch 15  48.7% | batch:       145 of       298\t|\tloss: 0.366901\n",
      "Training Epoch 15  49.0% | batch:       146 of       298\t|\tloss: 0.206999\n",
      "Training Epoch 15  49.3% | batch:       147 of       298\t|\tloss: 0.150086\n",
      "Training Epoch 15  49.7% | batch:       148 of       298\t|\tloss: 1.2131\n",
      "Training Epoch 15  50.0% | batch:       149 of       298\t|\tloss: 0.15576\n",
      "Training Epoch 15  50.3% | batch:       150 of       298\t|\tloss: 0.140239\n",
      "Training Epoch 15  50.7% | batch:       151 of       298\t|\tloss: 0.214271\n",
      "Training Epoch 15  51.0% | batch:       152 of       298\t|\tloss: 0.212218\n",
      "Training Epoch 15  51.3% | batch:       153 of       298\t|\tloss: 0.165924\n",
      "Training Epoch 15  51.7% | batch:       154 of       298\t|\tloss: 0.181338\n",
      "Training Epoch 15  52.0% | batch:       155 of       298\t|\tloss: 0.260009\n",
      "Training Epoch 15  52.3% | batch:       156 of       298\t|\tloss: 0.257557\n",
      "Training Epoch 15  52.7% | batch:       157 of       298\t|\tloss: 0.219522\n",
      "Training Epoch 15  53.0% | batch:       158 of       298\t|\tloss: 0.438578\n",
      "Training Epoch 15  53.4% | batch:       159 of       298\t|\tloss: 0.165858\n",
      "Training Epoch 15  53.7% | batch:       160 of       298\t|\tloss: 0.223289\n",
      "Training Epoch 15  54.0% | batch:       161 of       298\t|\tloss: 0.460216\n",
      "Training Epoch 15  54.4% | batch:       162 of       298\t|\tloss: 0.170769\n",
      "Training Epoch 15  54.7% | batch:       163 of       298\t|\tloss: 0.907872\n",
      "Training Epoch 15  55.0% | batch:       164 of       298\t|\tloss: 0.188173\n",
      "Training Epoch 15  55.4% | batch:       165 of       298\t|\tloss: 0.325132\n",
      "Training Epoch 15  55.7% | batch:       166 of       298\t|\tloss: 0.15334\n",
      "Training Epoch 15  56.0% | batch:       167 of       298\t|\tloss: 0.247911\n",
      "Training Epoch 15  56.4% | batch:       168 of       298\t|\tloss: 0.629695\n",
      "Training Epoch 15  56.7% | batch:       169 of       298\t|\tloss: 0.171021\n",
      "Training Epoch 15  57.0% | batch:       170 of       298\t|\tloss: 0.169299\n",
      "Training Epoch 15  57.4% | batch:       171 of       298\t|\tloss: 0.196818\n",
      "Training Epoch 15  57.7% | batch:       172 of       298\t|\tloss: 0.211945\n",
      "Training Epoch 15  58.1% | batch:       173 of       298\t|\tloss: 0.172456\n",
      "Training Epoch 15  58.4% | batch:       174 of       298\t|\tloss: 0.195765\n",
      "Training Epoch 15  58.7% | batch:       175 of       298\t|\tloss: 0.306317\n",
      "Training Epoch 15  59.1% | batch:       176 of       298\t|\tloss: 0.145486\n",
      "Training Epoch 15  59.4% | batch:       177 of       298\t|\tloss: 0.259054\n",
      "Training Epoch 15  59.7% | batch:       178 of       298\t|\tloss: 0.164415\n",
      "Training Epoch 15  60.1% | batch:       179 of       298\t|\tloss: 0.192592\n",
      "Training Epoch 15  60.4% | batch:       180 of       298\t|\tloss: 0.240349\n",
      "Training Epoch 15  60.7% | batch:       181 of       298\t|\tloss: 0.247911\n",
      "Training Epoch 15  61.1% | batch:       182 of       298\t|\tloss: 0.778213\n",
      "Training Epoch 15  61.4% | batch:       183 of       298\t|\tloss: 0.287975\n",
      "Training Epoch 15  61.7% | batch:       184 of       298\t|\tloss: 0.205368\n",
      "Training Epoch 15  62.1% | batch:       185 of       298\t|\tloss: 0.232949\n",
      "Training Epoch 15  62.4% | batch:       186 of       298\t|\tloss: 0.221614\n",
      "Training Epoch 15  62.8% | batch:       187 of       298\t|\tloss: 0.187615\n",
      "Training Epoch 15  63.1% | batch:       188 of       298\t|\tloss: 0.312619\n",
      "Training Epoch 15  63.4% | batch:       189 of       298\t|\tloss: 0.18295\n",
      "Training Epoch 15  63.8% | batch:       190 of       298\t|\tloss: 0.23001\n",
      "Training Epoch 15  64.1% | batch:       191 of       298\t|\tloss: 0.156653\n",
      "Training Epoch 15  64.4% | batch:       192 of       298\t|\tloss: 0.202023\n",
      "Training Epoch 15  64.8% | batch:       193 of       298\t|\tloss: 0.183874\n",
      "Training Epoch 15  65.1% | batch:       194 of       298\t|\tloss: 0.183013\n",
      "Training Epoch 15  65.4% | batch:       195 of       298\t|\tloss: 1.77876\n",
      "Training Epoch 15  65.8% | batch:       196 of       298\t|\tloss: 0.183499\n",
      "Training Epoch 15  66.1% | batch:       197 of       298\t|\tloss: 0.172154\n",
      "Training Epoch 15  66.4% | batch:       198 of       298\t|\tloss: 0.19233\n",
      "Training Epoch 15  66.8% | batch:       199 of       298\t|\tloss: 0.324103\n",
      "Training Epoch 15  67.1% | batch:       200 of       298\t|\tloss: 0.300691\n",
      "Training Epoch 15  67.4% | batch:       201 of       298\t|\tloss: 0.174209\n",
      "Training Epoch 15  67.8% | batch:       202 of       298\t|\tloss: 0.227815\n",
      "Training Epoch 15  68.1% | batch:       203 of       298\t|\tloss: 0.391661\n",
      "Training Epoch 15  68.5% | batch:       204 of       298\t|\tloss: 0.159888\n",
      "Training Epoch 15  68.8% | batch:       205 of       298\t|\tloss: 0.141656\n",
      "Training Epoch 15  69.1% | batch:       206 of       298\t|\tloss: 0.179148\n",
      "Training Epoch 15  69.5% | batch:       207 of       298\t|\tloss: 0.213001\n",
      "Training Epoch 15  69.8% | batch:       208 of       298\t|\tloss: 1.30597\n",
      "Training Epoch 15  70.1% | batch:       209 of       298\t|\tloss: 0.366038\n",
      "Training Epoch 15  70.5% | batch:       210 of       298\t|\tloss: 0.290757\n",
      "Training Epoch 15  70.8% | batch:       211 of       298\t|\tloss: 0.190494\n",
      "Training Epoch 15  71.1% | batch:       212 of       298\t|\tloss: 0.323651\n",
      "Training Epoch 15  71.5% | batch:       213 of       298\t|\tloss: 0.213092\n",
      "Training Epoch 15  71.8% | batch:       214 of       298\t|\tloss: 4.04357\n",
      "Training Epoch 15  72.1% | batch:       215 of       298\t|\tloss: 0.237393\n",
      "Training Epoch 15  72.5% | batch:       216 of       298\t|\tloss: 0.166442\n",
      "Training Epoch 15  72.8% | batch:       217 of       298\t|\tloss: 0.353942\n",
      "Training Epoch 15  73.2% | batch:       218 of       298\t|\tloss: 0.16421\n",
      "Training Epoch 15  73.5% | batch:       219 of       298\t|\tloss: 0.19626\n",
      "Training Epoch 15  73.8% | batch:       220 of       298\t|\tloss: 0.17664\n",
      "Training Epoch 15  74.2% | batch:       221 of       298\t|\tloss: 0.229495\n",
      "Training Epoch 15  74.5% | batch:       222 of       298\t|\tloss: 0.237173\n",
      "Training Epoch 15  74.8% | batch:       223 of       298\t|\tloss: 0.152731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 15  75.2% | batch:       224 of       298\t|\tloss: 0.2475\n",
      "Training Epoch 15  75.5% | batch:       225 of       298\t|\tloss: 0.257643\n",
      "Training Epoch 15  75.8% | batch:       226 of       298\t|\tloss: 0.161708\n",
      "Training Epoch 15  76.2% | batch:       227 of       298\t|\tloss: 0.171368\n",
      "Training Epoch 15  76.5% | batch:       228 of       298\t|\tloss: 0.350635\n",
      "Training Epoch 15  76.8% | batch:       229 of       298\t|\tloss: 0.994677\n",
      "Training Epoch 15  77.2% | batch:       230 of       298\t|\tloss: 0.197352\n",
      "Training Epoch 15  77.5% | batch:       231 of       298\t|\tloss: 0.237511\n",
      "Training Epoch 15  77.9% | batch:       232 of       298\t|\tloss: 0.371457\n",
      "Training Epoch 15  78.2% | batch:       233 of       298\t|\tloss: 0.22433\n",
      "Training Epoch 15  78.5% | batch:       234 of       298\t|\tloss: 0.247422\n",
      "Training Epoch 15  78.9% | batch:       235 of       298\t|\tloss: 0.198914\n",
      "Training Epoch 15  79.2% | batch:       236 of       298\t|\tloss: 0.228813\n",
      "Training Epoch 15  79.5% | batch:       237 of       298\t|\tloss: 0.191118\n",
      "Training Epoch 15  79.9% | batch:       238 of       298\t|\tloss: 0.49281\n",
      "Training Epoch 15  80.2% | batch:       239 of       298\t|\tloss: 0.35441\n",
      "Training Epoch 15  80.5% | batch:       240 of       298\t|\tloss: 0.288188\n",
      "Training Epoch 15  80.9% | batch:       241 of       298\t|\tloss: 0.18989\n",
      "Training Epoch 15  81.2% | batch:       242 of       298\t|\tloss: 0.183338\n",
      "Training Epoch 15  81.5% | batch:       243 of       298\t|\tloss: 0.129367\n",
      "Training Epoch 15  81.9% | batch:       244 of       298\t|\tloss: 1.48063\n",
      "Training Epoch 15  82.2% | batch:       245 of       298\t|\tloss: 0.200031\n",
      "Training Epoch 15  82.6% | batch:       246 of       298\t|\tloss: 0.225939\n",
      "Training Epoch 15  82.9% | batch:       247 of       298\t|\tloss: 0.15834\n",
      "Training Epoch 15  83.2% | batch:       248 of       298\t|\tloss: 0.18318\n",
      "Training Epoch 15  83.6% | batch:       249 of       298\t|\tloss: 0.207412\n",
      "Training Epoch 15  83.9% | batch:       250 of       298\t|\tloss: 0.227939\n",
      "Training Epoch 15  84.2% | batch:       251 of       298\t|\tloss: 0.196134\n",
      "Training Epoch 15  84.6% | batch:       252 of       298\t|\tloss: 0.355163\n",
      "Training Epoch 15  84.9% | batch:       253 of       298\t|\tloss: 0.204385\n",
      "Training Epoch 15  85.2% | batch:       254 of       298\t|\tloss: 0.189157\n",
      "Training Epoch 15  85.6% | batch:       255 of       298\t|\tloss: 1.04505\n",
      "Training Epoch 15  85.9% | batch:       256 of       298\t|\tloss: 0.165291\n",
      "Training Epoch 15  86.2% | batch:       257 of       298\t|\tloss: 0.257383\n",
      "Training Epoch 15  86.6% | batch:       258 of       298\t|\tloss: 0.153552\n",
      "Training Epoch 15  86.9% | batch:       259 of       298\t|\tloss: 0.249547\n",
      "Training Epoch 15  87.2% | batch:       260 of       298\t|\tloss: 0.211731\n",
      "Training Epoch 15  87.6% | batch:       261 of       298\t|\tloss: 0.462351\n",
      "Training Epoch 15  87.9% | batch:       262 of       298\t|\tloss: 0.155521\n",
      "Training Epoch 15  88.3% | batch:       263 of       298\t|\tloss: 0.189408\n",
      "Training Epoch 15  88.6% | batch:       264 of       298\t|\tloss: 0.177473\n",
      "Training Epoch 15  88.9% | batch:       265 of       298\t|\tloss: 0.149194\n",
      "Training Epoch 15  89.3% | batch:       266 of       298\t|\tloss: 0.257861\n",
      "Training Epoch 15  89.6% | batch:       267 of       298\t|\tloss: 0.144768\n",
      "Training Epoch 15  89.9% | batch:       268 of       298\t|\tloss: 0.226176\n",
      "Training Epoch 15  90.3% | batch:       269 of       298\t|\tloss: 0.566475\n",
      "Training Epoch 15  90.6% | batch:       270 of       298\t|\tloss: 0.343188\n",
      "Training Epoch 15  90.9% | batch:       271 of       298\t|\tloss: 0.387132\n",
      "Training Epoch 15  91.3% | batch:       272 of       298\t|\tloss: 0.198911\n",
      "Training Epoch 15  91.6% | batch:       273 of       298\t|\tloss: 0.204606\n",
      "Training Epoch 15  91.9% | batch:       274 of       298\t|\tloss: 0.240977\n",
      "Training Epoch 15  92.3% | batch:       275 of       298\t|\tloss: 0.183392\n",
      "Training Epoch 15  92.6% | batch:       276 of       298\t|\tloss: 0.664881\n",
      "Training Epoch 15  93.0% | batch:       277 of       298\t|\tloss: 0.148246\n",
      "Training Epoch 15  93.3% | batch:       278 of       298\t|\tloss: 0.266973\n",
      "Training Epoch 15  93.6% | batch:       279 of       298\t|\tloss: 0.294275\n",
      "Training Epoch 15  94.0% | batch:       280 of       298\t|\tloss: 0.180826\n",
      "Training Epoch 15  94.3% | batch:       281 of       298\t|\tloss: 0.220642\n",
      "Training Epoch 15  94.6% | batch:       282 of       298\t|\tloss: 0.1463\n",
      "Training Epoch 15  95.0% | batch:       283 of       298\t|\tloss: 0.165434\n",
      "Training Epoch 15  95.3% | batch:       284 of       298\t|\tloss: 0.444175\n",
      "Training Epoch 15  95.6% | batch:       285 of       298\t|\tloss: 0.167765\n",
      "Training Epoch 15  96.0% | batch:       286 of       298\t|\tloss: 0.163951\n",
      "Training Epoch 15  96.3% | batch:       287 of       298\t|\tloss: 0.203168\n",
      "Training Epoch 15  96.6% | batch:       288 of       298\t|\tloss: 0.149007\n",
      "Training Epoch 15  97.0% | batch:       289 of       298\t|\tloss: 0.175404\n",
      "Training Epoch 15  97.3% | batch:       290 of       298\t|\tloss: 0.201887\n",
      "Training Epoch 15  97.7% | batch:       291 of       298\t|\tloss: 0.188042\n",
      "Training Epoch 15  98.0% | batch:       292 of       298\t|\tloss: 0.223975\n",
      "Training Epoch 15  98.3% | batch:       293 of       298\t|\tloss: 0.299164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:39:15,862 | INFO : Epoch 15 Training Summary: epoch: 15.000000 | loss: 0.291969 | \n",
      "2023-06-22 14:39:15,863 | INFO : Epoch runtime: 0.0 hours, 0.0 minutes, 12.15755820274353 seconds\n",
      "\n",
      "2023-06-22 14:39:15,865 | INFO : Avg epoch train. time: 0.0 hours, 0.0 minutes, 12.26742180188497 seconds\n",
      "2023-06-22 14:39:15,866 | INFO : Avg batch train. time: 0.04116584497276835 seconds\n",
      "2023-06-22 14:39:15,866 | INFO : Avg sample train. time: 0.0012867025175041923 seconds\n",
      "Training Epoch:   4%|▍         | 15/400 [03:14<1:21:48, 12.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 15  98.7% | batch:       294 of       298\t|\tloss: 0.176126\n",
      "Training Epoch 15  99.0% | batch:       295 of       298\t|\tloss: 0.410197\n",
      "Training Epoch 15  99.3% | batch:       296 of       298\t|\tloss: 0.278632\n",
      "Training Epoch 15  99.7% | batch:       297 of       298\t|\tloss: 0.194292\n",
      "\n",
      "Training Epoch 16   0.0% | batch:         0 of       298\t|\tloss: 0.180317\n",
      "Training Epoch 16   0.3% | batch:         1 of       298\t|\tloss: 0.146001\n",
      "Training Epoch 16   0.7% | batch:         2 of       298\t|\tloss: 0.412351\n",
      "Training Epoch 16   1.0% | batch:         3 of       298\t|\tloss: 0.283904\n",
      "Training Epoch 16   1.3% | batch:         4 of       298\t|\tloss: 0.152299\n",
      "Training Epoch 16   1.7% | batch:         5 of       298\t|\tloss: 0.33559\n",
      "Training Epoch 16   2.0% | batch:         6 of       298\t|\tloss: 0.169765\n",
      "Training Epoch 16   2.3% | batch:         7 of       298\t|\tloss: 0.77213\n",
      "Training Epoch 16   2.7% | batch:         8 of       298\t|\tloss: 0.157927\n",
      "Training Epoch 16   3.0% | batch:         9 of       298\t|\tloss: 0.225791\n",
      "Training Epoch 16   3.4% | batch:        10 of       298\t|\tloss: 0.181823\n",
      "Training Epoch 16   3.7% | batch:        11 of       298\t|\tloss: 1.26168\n",
      "Training Epoch 16   4.0% | batch:        12 of       298\t|\tloss: 0.294748\n",
      "Training Epoch 16   4.4% | batch:        13 of       298\t|\tloss: 0.176589\n",
      "Training Epoch 16   4.7% | batch:        14 of       298\t|\tloss: 0.131609\n",
      "Training Epoch 16   5.0% | batch:        15 of       298\t|\tloss: 0.139774\n",
      "Training Epoch 16   5.4% | batch:        16 of       298\t|\tloss: 0.144774\n",
      "Training Epoch 16   5.7% | batch:        17 of       298\t|\tloss: 0.157686\n",
      "Training Epoch 16   6.0% | batch:        18 of       298\t|\tloss: 0.438714\n",
      "Training Epoch 16   6.4% | batch:        19 of       298\t|\tloss: 0.187864\n",
      "Training Epoch 16   6.7% | batch:        20 of       298\t|\tloss: 0.16721\n",
      "Training Epoch 16   7.0% | batch:        21 of       298\t|\tloss: 0.246972\n",
      "Training Epoch 16   7.4% | batch:        22 of       298\t|\tloss: 0.261438\n",
      "Training Epoch 16   7.7% | batch:        23 of       298\t|\tloss: 0.231155\n",
      "Training Epoch 16   8.1% | batch:        24 of       298\t|\tloss: 0.258746\n",
      "Training Epoch 16   8.4% | batch:        25 of       298\t|\tloss: 0.238538\n",
      "Training Epoch 16   8.7% | batch:        26 of       298\t|\tloss: 0.198846\n",
      "Training Epoch 16   9.1% | batch:        27 of       298\t|\tloss: 0.19699\n",
      "Training Epoch 16   9.4% | batch:        28 of       298\t|\tloss: 0.186447\n",
      "Training Epoch 16   9.7% | batch:        29 of       298\t|\tloss: 0.127203\n",
      "Training Epoch 16  10.1% | batch:        30 of       298\t|\tloss: 0.217684\n",
      "Training Epoch 16  10.4% | batch:        31 of       298\t|\tloss: 0.209119\n",
      "Training Epoch 16  10.7% | batch:        32 of       298\t|\tloss: 0.17088\n",
      "Training Epoch 16  11.1% | batch:        33 of       298\t|\tloss: 0.321831\n",
      "Training Epoch 16  11.4% | batch:        34 of       298\t|\tloss: 0.178116\n",
      "Training Epoch 16  11.7% | batch:        35 of       298\t|\tloss: 0.58408\n",
      "Training Epoch 16  12.1% | batch:        36 of       298\t|\tloss: 0.180532\n",
      "Training Epoch 16  12.4% | batch:        37 of       298\t|\tloss: 0.148591\n",
      "Training Epoch 16  12.8% | batch:        38 of       298\t|\tloss: 0.155569\n",
      "Training Epoch 16  13.1% | batch:        39 of       298\t|\tloss: 0.194213\n",
      "Training Epoch 16  13.4% | batch:        40 of       298\t|\tloss: 0.874148\n",
      "Training Epoch 16  13.8% | batch:        41 of       298\t|\tloss: 0.41425\n",
      "Training Epoch 16  14.1% | batch:        42 of       298\t|\tloss: 0.234015\n",
      "Training Epoch 16  14.4% | batch:        43 of       298\t|\tloss: 0.12007\n",
      "Training Epoch 16  14.8% | batch:        44 of       298\t|\tloss: 0.172376\n",
      "Training Epoch 16  15.1% | batch:        45 of       298\t|\tloss: 0.157002\n",
      "Training Epoch 16  15.4% | batch:        46 of       298\t|\tloss: 0.138589\n",
      "Training Epoch 16  15.8% | batch:        47 of       298\t|\tloss: 0.154297\n",
      "Training Epoch 16  16.1% | batch:        48 of       298\t|\tloss: 0.222474\n",
      "Training Epoch 16  16.4% | batch:        49 of       298\t|\tloss: 0.179277\n",
      "Training Epoch 16  16.8% | batch:        50 of       298\t|\tloss: 0.224615\n",
      "Training Epoch 16  17.1% | batch:        51 of       298\t|\tloss: 0.132449\n",
      "Training Epoch 16  17.4% | batch:        52 of       298\t|\tloss: 0.145695\n",
      "Training Epoch 16  17.8% | batch:        53 of       298\t|\tloss: 0.249096\n",
      "Training Epoch 16  18.1% | batch:        54 of       298\t|\tloss: 0.425156\n",
      "Training Epoch 16  18.5% | batch:        55 of       298\t|\tloss: 2.50665\n",
      "Training Epoch 16  18.8% | batch:        56 of       298\t|\tloss: 0.141156\n",
      "Training Epoch 16  19.1% | batch:        57 of       298\t|\tloss: 0.206896\n",
      "Training Epoch 16  19.5% | batch:        58 of       298\t|\tloss: 0.237108\n",
      "Training Epoch 16  19.8% | batch:        59 of       298\t|\tloss: 0.152334\n",
      "Training Epoch 16  20.1% | batch:        60 of       298\t|\tloss: 1.49259\n",
      "Training Epoch 16  20.5% | batch:        61 of       298\t|\tloss: 0.141814\n",
      "Training Epoch 16  20.8% | batch:        62 of       298\t|\tloss: 0.232656\n",
      "Training Epoch 16  21.1% | batch:        63 of       298\t|\tloss: 0.278916\n",
      "Training Epoch 16  21.5% | batch:        64 of       298\t|\tloss: 0.189498\n",
      "Training Epoch 16  21.8% | batch:        65 of       298\t|\tloss: 0.137312\n",
      "Training Epoch 16  22.1% | batch:        66 of       298\t|\tloss: 0.281292\n",
      "Training Epoch 16  22.5% | batch:        67 of       298\t|\tloss: 0.21605\n",
      "Training Epoch 16  22.8% | batch:        68 of       298\t|\tloss: 0.181796\n",
      "Training Epoch 16  23.2% | batch:        69 of       298\t|\tloss: 0.213711\n",
      "Training Epoch 16  23.5% | batch:        70 of       298\t|\tloss: 0.291487\n",
      "Training Epoch 16  23.8% | batch:        71 of       298\t|\tloss: 0.320373\n",
      "Training Epoch 16  24.2% | batch:        72 of       298\t|\tloss: 0.693753\n",
      "Training Epoch 16  24.5% | batch:        73 of       298\t|\tloss: 0.212495\n",
      "Training Epoch 16  24.8% | batch:        74 of       298\t|\tloss: 0.247047\n",
      "Training Epoch 16  25.2% | batch:        75 of       298\t|\tloss: 0.170268\n",
      "Training Epoch 16  25.5% | batch:        76 of       298\t|\tloss: 0.155277\n",
      "Training Epoch 16  25.8% | batch:        77 of       298\t|\tloss: 0.18289\n",
      "Training Epoch 16  26.2% | batch:        78 of       298\t|\tloss: 0.142982\n",
      "Training Epoch 16  26.5% | batch:        79 of       298\t|\tloss: 0.25959\n",
      "Training Epoch 16  26.8% | batch:        80 of       298\t|\tloss: 0.196022\n",
      "Training Epoch 16  27.2% | batch:        81 of       298\t|\tloss: 0.161978\n",
      "Training Epoch 16  27.5% | batch:        82 of       298\t|\tloss: 0.638118\n",
      "Training Epoch 16  27.9% | batch:        83 of       298\t|\tloss: 0.185828\n",
      "Training Epoch 16  28.2% | batch:        84 of       298\t|\tloss: 0.195478\n",
      "Training Epoch 16  28.5% | batch:        85 of       298\t|\tloss: 1.85189\n",
      "Training Epoch 16  28.9% | batch:        86 of       298\t|\tloss: 0.303407\n",
      "Training Epoch 16  29.2% | batch:        87 of       298\t|\tloss: 0.125062\n",
      "Training Epoch 16  29.5% | batch:        88 of       298\t|\tloss: 0.322036\n",
      "Training Epoch 16  29.9% | batch:        89 of       298\t|\tloss: 0.421619\n",
      "Training Epoch 16  30.2% | batch:        90 of       298\t|\tloss: 0.157569\n",
      "Training Epoch 16  30.5% | batch:        91 of       298\t|\tloss: 0.160873\n",
      "Training Epoch 16  30.9% | batch:        92 of       298\t|\tloss: 0.195966\n",
      "Training Epoch 16  31.2% | batch:        93 of       298\t|\tloss: 0.262573\n",
      "Training Epoch 16  31.5% | batch:        94 of       298\t|\tloss: 0.206084\n",
      "Training Epoch 16  31.9% | batch:        95 of       298\t|\tloss: 0.189398\n",
      "Training Epoch 16  32.2% | batch:        96 of       298\t|\tloss: 0.436807\n",
      "Training Epoch 16  32.6% | batch:        97 of       298\t|\tloss: 0.273749\n",
      "Training Epoch 16  32.9% | batch:        98 of       298\t|\tloss: 0.194016\n",
      "Training Epoch 16  33.2% | batch:        99 of       298\t|\tloss: 1.2136\n",
      "Training Epoch 16  33.6% | batch:       100 of       298\t|\tloss: 0.201189\n",
      "Training Epoch 16  33.9% | batch:       101 of       298\t|\tloss: 0.198653\n",
      "Training Epoch 16  34.2% | batch:       102 of       298\t|\tloss: 0.172327\n",
      "Training Epoch 16  34.6% | batch:       103 of       298\t|\tloss: 0.297845\n",
      "Training Epoch 16  34.9% | batch:       104 of       298\t|\tloss: 0.163013\n",
      "Training Epoch 16  35.2% | batch:       105 of       298\t|\tloss: 0.38444\n",
      "Training Epoch 16  35.6% | batch:       106 of       298\t|\tloss: 0.189497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 16  35.9% | batch:       107 of       298\t|\tloss: 0.187727\n",
      "Training Epoch 16  36.2% | batch:       108 of       298\t|\tloss: 0.188101\n",
      "Training Epoch 16  36.6% | batch:       109 of       298\t|\tloss: 0.236813\n",
      "Training Epoch 16  36.9% | batch:       110 of       298\t|\tloss: 0.361798\n",
      "Training Epoch 16  37.2% | batch:       111 of       298\t|\tloss: 0.232578\n",
      "Training Epoch 16  37.6% | batch:       112 of       298\t|\tloss: 0.146367\n",
      "Training Epoch 16  37.9% | batch:       113 of       298\t|\tloss: 0.372269\n",
      "Training Epoch 16  38.3% | batch:       114 of       298\t|\tloss: 0.212842\n",
      "Training Epoch 16  38.6% | batch:       115 of       298\t|\tloss: 0.353796\n",
      "Training Epoch 16  38.9% | batch:       116 of       298\t|\tloss: 0.319272\n",
      "Training Epoch 16  39.3% | batch:       117 of       298\t|\tloss: 1.69531\n",
      "Training Epoch 16  39.6% | batch:       118 of       298\t|\tloss: 0.22554\n",
      "Training Epoch 16  39.9% | batch:       119 of       298\t|\tloss: 0.185566\n",
      "Training Epoch 16  40.3% | batch:       120 of       298\t|\tloss: 0.179376\n",
      "Training Epoch 16  40.6% | batch:       121 of       298\t|\tloss: 0.265893\n",
      "Training Epoch 16  40.9% | batch:       122 of       298\t|\tloss: 0.167792\n",
      "Training Epoch 16  41.3% | batch:       123 of       298\t|\tloss: 0.214057\n",
      "Training Epoch 16  41.6% | batch:       124 of       298\t|\tloss: 0.154488\n",
      "Training Epoch 16  41.9% | batch:       125 of       298\t|\tloss: 0.338813\n",
      "Training Epoch 16  42.3% | batch:       126 of       298\t|\tloss: 0.135239\n",
      "Training Epoch 16  42.6% | batch:       127 of       298\t|\tloss: 0.115341\n",
      "Training Epoch 16  43.0% | batch:       128 of       298\t|\tloss: 0.182693\n",
      "Training Epoch 16  43.3% | batch:       129 of       298\t|\tloss: 0.186963\n",
      "Training Epoch 16  43.6% | batch:       130 of       298\t|\tloss: 0.154087\n",
      "Training Epoch 16  44.0% | batch:       131 of       298\t|\tloss: 3.10869\n",
      "Training Epoch 16  44.3% | batch:       132 of       298\t|\tloss: 0.210083\n",
      "Training Epoch 16  44.6% | batch:       133 of       298\t|\tloss: 0.139019\n",
      "Training Epoch 16  45.0% | batch:       134 of       298\t|\tloss: 0.236891\n",
      "Training Epoch 16  45.3% | batch:       135 of       298\t|\tloss: 0.129929\n",
      "Training Epoch 16  45.6% | batch:       136 of       298\t|\tloss: 0.158576\n",
      "Training Epoch 16  46.0% | batch:       137 of       298\t|\tloss: 0.266656\n",
      "Training Epoch 16  46.3% | batch:       138 of       298\t|\tloss: 0.204321\n",
      "Training Epoch 16  46.6% | batch:       139 of       298\t|\tloss: 0.287692\n",
      "Training Epoch 16  47.0% | batch:       140 of       298\t|\tloss: 0.157533\n",
      "Training Epoch 16  47.3% | batch:       141 of       298\t|\tloss: 0.160294\n",
      "Training Epoch 16  47.7% | batch:       142 of       298\t|\tloss: 0.209431\n",
      "Training Epoch 16  48.0% | batch:       143 of       298\t|\tloss: 0.306147\n",
      "Training Epoch 16  48.3% | batch:       144 of       298\t|\tloss: 0.291383\n",
      "Training Epoch 16  48.7% | batch:       145 of       298\t|\tloss: 0.304515\n",
      "Training Epoch 16  49.0% | batch:       146 of       298\t|\tloss: 0.158046\n",
      "Training Epoch 16  49.3% | batch:       147 of       298\t|\tloss: 0.218276\n",
      "Training Epoch 16  49.7% | batch:       148 of       298\t|\tloss: 0.254892\n",
      "Training Epoch 16  50.0% | batch:       149 of       298\t|\tloss: 0.147049\n",
      "Training Epoch 16  50.3% | batch:       150 of       298\t|\tloss: 0.250538\n",
      "Training Epoch 16  50.7% | batch:       151 of       298\t|\tloss: 0.156505\n",
      "Training Epoch 16  51.0% | batch:       152 of       298\t|\tloss: 0.331787\n",
      "Training Epoch 16  51.3% | batch:       153 of       298\t|\tloss: 0.182786\n",
      "Training Epoch 16  51.7% | batch:       154 of       298\t|\tloss: 0.452245\n",
      "Training Epoch 16  52.0% | batch:       155 of       298\t|\tloss: 0.200802\n",
      "Training Epoch 16  52.3% | batch:       156 of       298\t|\tloss: 0.16607\n",
      "Training Epoch 16  52.7% | batch:       157 of       298\t|\tloss: 0.210811\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "total_epoch_time = 0\n",
    "\n",
    "for epoch in tqdm(range(start_epoch + 1, config[\"epochs\"] + 1), desc='Training Epoch', leave=False):\n",
    "    mark = epoch if config['save_all'] else 'last'\n",
    "    epoch_start_time = time.time()\n",
    "    aggr_metrics_train = trainer.train_epoch(epoch)  # dictionary of aggregate epoch metrics\n",
    "    epoch_runtime = time.time() - epoch_start_time\n",
    "    print()\n",
    "    print_str = 'Epoch {} Training Summary: '.format(epoch)\n",
    "    for k, v in aggr_metrics_train.items():\n",
    "        tensorboard_writer.add_scalar('{}/train'.format(k), v, epoch)\n",
    "        print_str += '{}: {:8f} | '.format(k, v)\n",
    "    logger.info(print_str)\n",
    "    logger.info(\"Epoch runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(epoch_runtime)))\n",
    "    total_epoch_time += epoch_runtime\n",
    "    avg_epoch_time = total_epoch_time / (epoch - start_epoch)\n",
    "    avg_batch_time = avg_epoch_time / len(train_loader)\n",
    "    avg_sample_time = avg_epoch_time / len(train_dataset)\n",
    "    logger.info(\"Avg epoch train. time: {} hours, {} minutes, {} seconds\".format(*utils.readable_time(avg_epoch_time)))\n",
    "    logger.info(\"Avg batch train. time: {} seconds\".format(avg_batch_time))\n",
    "    logger.info(\"Avg sample train. time: {} seconds\".format(avg_sample_time))\n",
    "\n",
    "    # evaluate if first or last epoch or at specified interval\n",
    "    if (epoch == config[\"epochs\"]) or (epoch == start_epoch + 1) or (epoch % config['val_interval'] == 0):\n",
    "        aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config,\n",
    "                                                                  best_metrics, best_value, epoch)\n",
    "        metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "        metrics.append(list(metrics_values))\n",
    "\n",
    "    utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(mark)), epoch, model, optimizer)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    if epoch == config['lr_step'][lr_step]:\n",
    "        utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(epoch)), epoch, model, optimizer)\n",
    "        lr = lr * config['lr_factor'][lr_step]\n",
    "        if lr_step < len(config['lr_step']) - 1:  # so that this index does not get out of bounds\n",
    "            lr_step += 1\n",
    "        logger.info('Learning rate updated to: ', lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c8c386",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:36:46.881Z"
    }
   },
   "outputs": [],
   "source": [
    "config['load_model'] = './output/_2023-06-22_14-31-22_Nbr/checkpoints/model_best.pth'\n",
    "model, optimizer, start_epoch = utils.load_model(model, config['load_model'], optimizer, config['resume'],\n",
    "                                                config['change_output'],\n",
    "                                                config['lr'],\n",
    "                                                config['lr_step'],\n",
    "                                                config['lr_factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12454bf2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:36:48.117Z"
    }
   },
   "outputs": [],
   "source": [
    "X, targets, target_masks, padding_masks, IDs = next(iter(train_loader))\n",
    "targets = targets.to(device)\n",
    "target_masks = target_masks.to(device)  \n",
    "padding_masks = padding_masks.to(device) \n",
    "\n",
    "predictions = model(X.to(device), padding_masks)  # (batch_size, padded_length, feat_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02f8a8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:36:48.573Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3435b289",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:36:49.401Z"
    }
   },
   "outputs": [],
   "source": [
    "print(target_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e57ea3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:36:49.726Z"
    }
   },
   "outputs": [],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d2abc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:36:50.738Z"
    }
   },
   "outputs": [],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838127f",
   "metadata": {},
   "source": [
    "### STEP 7. Fine-tuning (regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edad3e",
   "metadata": {},
   "source": [
    "<img src=\"./image/TST04.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef6539",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:37:11.408Z"
    }
   },
   "outputs": [],
   "source": [
    "# 실습을 위해 수정해야 할 argumetn들을 설정해줍니다.\n",
    "args_change = EasyDict({\n",
    "    # for dataloader \n",
    "    'output_dir': './output',\n",
    "    'data_dir': './data/BeijingPM25Quality',\n",
    "    'load_model': './output/_2023-06-22_14-31-22_Nbr/checkpoints/model_best.pth',\n",
    "    'name': 'finetuned',\n",
    "    'records_file': 'Regression_records.xls',\n",
    "    'change_output': True,\n",
    "    \n",
    "    # Dataset\n",
    "    'limit_size': None,\n",
    "    'data_class': 'tsra',\n",
    "    'pattern': 'TRAIN',\n",
    "    'val_pattern': 'TEST',\n",
    "    'val_ratio': 0.2,\n",
    "    'epochs': 200,\n",
    "    'lr': 0.001,\n",
    "    'optimizer': 'RAdam',\n",
    "    'batch_size': 128,\n",
    "    'pos_encoding': 'learnable',\n",
    "    'd_model': 128,\n",
    "    'task': 'regression'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25ff37",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:37:13.892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Offical code에서 default로 세팅한 argument들을 불러옵니다.\n",
    "args = Options()\n",
    "args = args.parser.parse_args([])\n",
    "# 수정할 argument들을 업데이트 해줍니다.\n",
    "args.__dict__.update(args_change)\n",
    "args.__dict__\n",
    "# 세팅한 argument들로 configuration diretory를 생성합니다. (+ config 저장 및 불러오기)\n",
    "config = setup(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a9fd0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:37:16.866Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize data generators\n",
    "dataset_class, collate_fn, runner_class = pipeline_factory(config)    \n",
    "\"\"\"\n",
    "dataset_class : (X, mask, index)를 도출하는 데이터셋 생성하는 class\n",
    "collate_fn : DataLoader에서 각각의 데이터 샘플을 어떻게 배치로 결합할지 결정하는 함수\n",
    "runner_class : 학습 및 테스트 과정에 대한 class\n",
    "\"\"\"\n",
    "val_dataset = dataset_class(val_data, val_indices)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=config['batch_size'],\n",
    "                        shuffle=False,\n",
    "                        num_workers=config['num_workers'],\n",
    "                        pin_memory=True,\n",
    "                        collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "train_dataset = dataset_class(my_data, train_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                            batch_size=config['batch_size'],\n",
    "                            shuffle=True,\n",
    "                            num_workers=config['num_workers'],\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=lambda x: collate_fn(x, max_len=model.max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3249b4d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:37:17.596Z"
    }
   },
   "outputs": [],
   "source": [
    "X, targets, padding_masks, IDs = next(iter(train_loader))\n",
    "print(X.shape, targets.shape, target_masks.shape, padding_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f8b5e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:37:18.429Z"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c959c9fa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:37:19.168Z"
    }
   },
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563581b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:37:22.671Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model_factory(config, my_data)\n",
    "\n",
    "# Initialize optimizer\n",
    "# L2 regularization\n",
    "if config['global_reg']:\n",
    "    weight_decay = config['l2_reg']\n",
    "    output_reg = None\n",
    "else:\n",
    "    weight_decay = 0\n",
    "    output_reg = config['l2_reg']\n",
    "\n",
    "optim_class = get_optimizer(config['optimizer'])\n",
    "optimizer = optim_class(model.parameters(), lr=config['lr'], weight_decay=weight_decay)\n",
    "\n",
    "lr_step = 0  # current step index of `lr_step`\n",
    "lr = config['lr']  # current learning step\n",
    "\n",
    "# 학습된 weight을 불러올 때 사용됩니다.\n",
    "if args.load_model:\n",
    "    model, optimizer, start_epoch = utils.load_model(model, config['load_model'], optimizer, config['resume'],\n",
    "                                                    config['change_output'],\n",
    "                                                    config['lr'],\n",
    "                                                    config['lr_step'],\n",
    "                                                    config['lr_factor'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_module = get_loss_module(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77677331",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:37:23.951Z"
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25665b9b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:37:25.243Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = runner_class(model, train_loader, device, loss_module, optimizer, l2_reg=output_reg,\n",
    "                       print_interval=config['print_interval'], console=config['console'])\n",
    "val_evaluator = runner_class(model, val_loader, device, loss_module,\n",
    "                             print_interval=config['print_interval'], console=config['console'])\n",
    "\n",
    "tensorboard_writer = SummaryWriter(config['tensorboard_dir'])\n",
    "\n",
    "best_value = 1e16 if config['key_metric'] in NEG_METRICS else -1e16  # initialize with +inf or -inf depending on key metric\n",
    "metrics = []  # (for validation) list of lists: for each epoch, stores metrics like loss, ...\n",
    "best_metrics = {}\n",
    "\n",
    "# 학습되지 않은 모델로 초기 성능을 확인합니다.\n",
    "aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, \n",
    "                                                      tensorboard_writer, \n",
    "                                                      config, best_metrics,\n",
    "                                                      best_value, \n",
    "                                                      epoch=0)\n",
    "\n",
    "metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "metrics.append(list(metrics_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154c8d2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T14:37:26.714Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "total_epoch_time = 0\n",
    "\n",
    "for epoch in tqdm(range(start_epoch + 1, config[\"epochs\"] + 1), desc='Training Epoch', leave=False):\n",
    "    mark = epoch if config['save_all'] else 'last'\n",
    "    epoch_start_time = time.time()\n",
    "    aggr_metrics_train = trainer.train_epoch(epoch)  # dictionary of aggregate epoch metrics\n",
    "    epoch_runtime = time.time() - epoch_start_time\n",
    "    print()\n",
    "    print_str = 'Epoch {} Training Summary: '.format(epoch)\n",
    "    for k, v in aggr_metrics_train.items():\n",
    "        tensorboard_writer.add_scalar('{}/train'.format(k), v, epoch)\n",
    "        print_str += '{}: {:8f} | '.format(k, v)\n",
    "    logger.info(print_str)\n",
    "    logger.info(\"Epoch runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(epoch_runtime)))\n",
    "    total_epoch_time += epoch_runtime\n",
    "    avg_epoch_time = total_epoch_time / (epoch - start_epoch)\n",
    "    avg_batch_time = avg_epoch_time / len(train_loader)\n",
    "    avg_sample_time = avg_epoch_time / len(train_dataset)\n",
    "    logger.info(\"Avg epoch train. time: {} hours, {} minutes, {} seconds\".format(*utils.readable_time(avg_epoch_time)))\n",
    "    logger.info(\"Avg batch train. time: {} seconds\".format(avg_batch_time))\n",
    "    logger.info(\"Avg sample train. time: {} seconds\".format(avg_sample_time))\n",
    "\n",
    "    # evaluate if first or last epoch or at specified interval\n",
    "    if (epoch == config[\"epochs\"]) or (epoch == start_epoch + 1) or (epoch % config['val_interval'] == 0):\n",
    "        aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config,\n",
    "                                                                  best_metrics, best_value, epoch)\n",
    "        metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "        metrics.append(list(metrics_values))\n",
    "\n",
    "    utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(mark)), epoch, model, optimizer)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    if epoch == config['lr_step'][lr_step]:\n",
    "        utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(epoch)), epoch, model, optimizer)\n",
    "        lr = lr * config['lr_factor'][lr_step]\n",
    "        if lr_step < len(config['lr_step']) - 1:  # so that this index does not get out of bounds\n",
    "            lr_step += 1\n",
    "        logger.info('Learning rate updated to: ', lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5837348a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T05:19:10.501492Z",
     "start_time": "2023-06-22T05:19:10.485914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ./mvts_transformer/output/_2023-06-21_15-34-54_nTP/checkpoints/model_best.pth. Epoch: 16\n"
     ]
    }
   ],
   "source": [
    "config['load_model'] = './output/_2023-06-22_04-54-52_3fA/checkpoints/model_best.pth'\n",
    "model, optimizer, start_epoch = utils.load_model(model, config['load_model'], optimizer, config['resume'],\n",
    "                                                config['change_output'],\n",
    "                                                config['lr'],\n",
    "                                                config['lr_step'],\n",
    "                                                config['lr_factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43a3c2f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T05:19:10.649492Z",
     "start_time": "2023-06-22T05:19:10.584766Z"
    }
   },
   "outputs": [],
   "source": [
    "X, targets, padding_masks, IDs = next(iter(train_loader))\n",
    "targets = targets.to(device)\n",
    "target_masks = target_masks.to(device)\n",
    "padding_masks = padding_masks.to(device)\n",
    "\n",
    "predictions = model(X.to(device), padding_masks)  # (batch_size, padded_length, feat_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "806952b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T05:19:10.733333Z",
     "start_time": "2023-06-22T05:19:10.729820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([558.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e8cd464a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T05:19:10.829385Z",
     "start_time": "2023-06-22T05:19:10.815493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([260.2995], device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d3e68",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03e719",
   "metadata": {},
   "source": [
    "## 명령어로 실행해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01261054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/gzerveas/mvts_transformer.git\n",
    "# %cd mvts_transformer\n",
    "# ! mkdir output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be47262",
   "metadata": {},
   "source": [
    "### Train models from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f34b17",
   "metadata": {},
   "source": [
    "```\n",
    "python src/main.py \n",
    "--output_dir output \n",
    "--comment \"regression from Scratch\" \n",
    "--name $1_fromScratch_Regression \n",
    "--records_file Regression_records.xls \n",
    "--data_dir ../data/BeijingPM25Quality\n",
    "--data_class tsra \n",
    "--pattern TRAIN \n",
    "--val_pattern TEST \n",
    "--epochs 100 \n",
    "--lr 0.001 \n",
    "--optimizer RAdam  \n",
    "--pos_encoding learnable \n",
    "--task regression\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60a01598",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T12:55:49.385661Z",
     "start_time": "2023-06-22T12:55:46.600098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-22 12:55:47,323 | INFO : Loading packages ...\n",
      "/usr/local/bin/python3: Error while finding module specification for 'src.main.py' (AttributeError: module 'src.main' has no attribute '__path__')\n"
     ]
    }
   ],
   "source": [
    "! python3 src/main.py --output_dir output --comment \"regression from Scratch\" --name $1_fromScratch_Regression --records_file Regression_records.xls --data_dir ../data/BeijingPM25Quality --data_class tsra --pattern TRAIN --val_pattern TEST --epochs 100 --lr 0.001 --optimizer RAdam  --pos_encoding learnable --task regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b93f79",
   "metadata": {},
   "source": [
    "### Pre-train models (unsupervised learning through input masking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450d0ef",
   "metadata": {},
   "source": [
    "```\n",
    "python src/main.py \n",
    "--output_dir output \n",
    "--comment \"pretraining through imputation\" \n",
    "--name $1_pretrained \n",
    "--records_file Imputation_records.xls \n",
    "--data_dir ../data/BeijingPM25Quality\n",
    "--data_class tsra \n",
    "--pattern TRAIN \n",
    "--val_ratio 0.2 \n",
    "--epochs 700 \n",
    "--lr 0.001 \n",
    "--optimizer RAdam \n",
    "--batch_size 32 \n",
    "--pos_encoding learnable \n",
    "--d_model 128\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python src/main.py --output_dir output --comment \"pretraining through imputation\" --name $1_pretrained --records_file Imputation_records.xls --data_dir ../data/BeijingPM25Quality--data_class tsra --pattern TRAIN --val_ratio 0.2 --epochs 700 --lr 0.001 --optimizer RAdam --batch_size 32 --pos_encoding learnable --d_model 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f646c8",
   "metadata": {},
   "source": [
    "### Fine-tune pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f7d90",
   "metadata": {},
   "source": [
    "```\n",
    "python src/main.py \n",
    "--output_dir output\n",
    "--comment \"finetune for regression\" \n",
    "--name BeijingPM25Quality_finetuned \n",
    "--records_file Regression_records.xls \n",
    "--data_dir ../data/BeijingPM25Quality\n",
    "--data_class tsra \n",
    "--pattern TRAIN \n",
    "--val_pattern TEST  \n",
    "--epochs 400 \n",
    "--lr 0.001 \n",
    "--optimizer RAdam \n",
    "--pos_encoding learnable \n",
    "--d_model 128 \n",
    "--load_model path/to/BeijingPM25Quality_pretrained/checkpoints/model_best.pth \n",
    "--task regression \n",
    "--change_output \n",
    "--batch_size 128\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python src/main.py --output_dir output --comment \"finetune for regression\" --name BeijingPM25Quality_finetuned --records_file Regression_records.xls --data_dir data/BeijingPM25Quality --data_class tsra --pattern TRAIN --val_pattern TEST --epochs 400 --lr 0.001 --optimizer RAdam --pos_encoding learnable --d_model 128 --load_model path/to/BeijingPM25Quality_pretrained/checkpoints/model_best.pth --task regression --change_output --batch_size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab46a7e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "ko",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ko",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "344px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
